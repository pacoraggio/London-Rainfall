{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23429d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d7cef4",
   "metadata": {},
   "source": [
    "# ERA5 Land downloading data with APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "769d2ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import os\n",
    "\n",
    "def download_era5_precip(lat, lon, years, output_dir=\"data\"):\n",
    "    \"\"\"\n",
    "    Download ERA5 precipitation data for specific coordinates and years\n",
    "    \n",
    "    Args:\n",
    "        lat (float): Latitude\n",
    "        lon (float): Longitude  \n",
    "        years (list): List of years (e.g., [2020, 2021, 2022])\n",
    "        output_dir (str): Directory to save files\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to downloaded ZIP file\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Setup CDS API client\n",
    "    c = cdsapi.Client()\n",
    "    \n",
    "    # Format coordinates for filename (replace dots with underscores)\n",
    "    lat_str = str(lat).replace('.', '_').replace('-', 'neg')\n",
    "    lon_str = str(lon).replace('.', '_').replace('-', 'neg')\n",
    "    \n",
    "    # Create filename with years\n",
    "    years_str = \"_\".join(map(str, sorted(years)))\n",
    "    filename = f\"era5_precip_{lat_str}_{lon_str}_{years_str}.zip\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    # Define small area around the point (±0.1 degrees)\n",
    "    area = [lat + 0.1, lon - 0.1, lat - 0.1, lon + 0.1]  # North, West, South, East\n",
    "    \n",
    "    print(f\"🌍 Downloading ERA5 precipitation data...\")\n",
    "    print(f\"📍 Location: {lat}°N, {lon}°E\")\n",
    "    print(f\"📅 Years: {years}\")\n",
    "    print(f\"📦 Area: {area}\")\n",
    "    print(f\"💾 Output: {filename}\")\n",
    "    \n",
    "    try:\n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-single-levels',\n",
    "            {\n",
    "                'product_type': 'reanalysis',\n",
    "                'variable': 'total_precipitation',\n",
    "                'year': [str(year) for year in years],\n",
    "                'month': [f'{i:02d}' for i in range(1, 13)],\n",
    "                'day': [f'{i:02d}' for i in range(1, 32)],\n",
    "                'time': [f'{i:02d}:00' for i in range(0, 24)],\n",
    "                'area': area,\n",
    "                'format': 'netcdf',\n",
    "            },\n",
    "            filepath\n",
    "        )\n",
    "        print(f\"✅ Download complete: {filename}\")\n",
    "        file_size = os.path.getsize(filepath) / (1024 * 1024)  # Convert to MB\n",
    "        print(f\"📏 File size: {file_size:.2f} MB\")\n",
    "        return filepath\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Download failed: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d5b6e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing download function...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 14:06:27,360 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌍 Downloading ERA5 precipitation data...\n",
      "📍 Location: 51.5074°N, -0.1278°E\n",
      "📅 Years: [2023, 2024]\n",
      "📦 Area: [51.6074, -0.2278, 51.407399999999996, -0.02779999999999999]\n",
      "💾 Output: era5_precip_51_5074_neg0_1278_2023_2024.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 14:06:27,787 INFO Request ID is bfeebeef-2b4f-48dc-8583-0e34092b8d95\n",
      "2025-07-02 14:06:27,871 INFO status has been updated to accepted\n",
      "2025-07-02 14:06:41,540 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd59932c30745439139b575a601e811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "7675b9b99b97dc377c4010f2a0a2eb92.nc:   0%|          | 0.00/1.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Download complete: era5_precip_51_5074_neg0_1278_2023_2024.zip\n",
      "📏 File size: 1.15 MB\n",
      "Success! File saved at: data\\era5_precip_51_5074_neg0_1278_2023_2024.zip\n"
     ]
    }
   ],
   "source": [
    "lat, lon = 51.5074, -0.1278  # London coordinates\n",
    "years = [2023, 2024]  # Start with just one year for testing\n",
    "    \n",
    "print(\"Testing download function...\")\n",
    "result = download_era5_precip(lat, lon, years)\n",
    "    \n",
    "if result:\n",
    "    print(f\"Success! File saved at: {result}\")\n",
    "else:\n",
    "    print(\"Download failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ae3889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_convert_to_csv(zip_filepath, output_dir=None):\n",
    "    \"\"\"\n",
    "    Extract precipitation data from ZIP file and convert to CSV\n",
    "    \n",
    "    Args:\n",
    "        zip_filepath (str): Path to downloaded ZIP file\n",
    "        output_dir (str): Directory for CSV output (default: same as ZIP file)\n",
    "    \n",
    "    Returns:\n",
    "        list: Paths to created CSV files\n",
    "    \"\"\"\n",
    "    import zipfile\n",
    "    import pandas as pd\n",
    "    import xarray as xr\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.dirname(zip_filepath)\n",
    "    \n",
    "    csv_files = []\n",
    "    \n",
    "    print(f\"📦 Processing ZIP file: {os.path.basename(zip_filepath)}\")\n",
    "    \n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
    "            # List files in archive\n",
    "            file_list = zip_ref.namelist()\n",
    "            print(f\"📄 Files in archive: {len(file_list)}\")\n",
    "            for file_name in file_list:\n",
    "                print(f\"  - {file_name}\")\n",
    "            \n",
    "            # Process each NetCDF file\n",
    "            for file_name in file_list:\n",
    "                if file_name.endswith('.nc'):\n",
    "                    print(f\"🔄 Processing: {file_name}\")\n",
    "                    \n",
    "                    # Extract to temporary location\n",
    "                    temp_path = os.path.join(output_dir, file_name)\n",
    "                    zip_ref.extract(file_name, output_dir)\n",
    "                    \n",
    "                    # Open with xarray\n",
    "                    ds = xr.open_dataset(temp_path)\n",
    "                    \n",
    "                    # Get coordinates info for filename\n",
    "                    lat_vals = ds.latitude.values\n",
    "                    lon_vals = ds.longitude.values\n",
    "                    lat_center = float(lat_vals.mean())\n",
    "                    lon_center = float(lon_vals.mean())\n",
    "                    \n",
    "                    # Format coordinates for filename (replace dots with underscores)\n",
    "                    lat_str = f\"{lat_center:.2f}\".replace('.', '_').replace('-', 'neg')\n",
    "                    lon_str = f\"{lon_center:.2f}\".replace('.', '_').replace('-', 'neg')\n",
    "                    \n",
    "                    # Get years from time dimension\n",
    "                    years = sorted(list(set(pd.to_datetime(ds.time.values).year)))\n",
    "                    years_str = \"_\".join(map(str, years))\n",
    "                    \n",
    "                    # Create CSV filename: lat_lon_Y1_Y2\n",
    "                    csv_filename = f\"{lat_str}_{lon_str}_{years_str}.csv\"\n",
    "                    csv_filepath = os.path.join(output_dir, csv_filename)\n",
    "                    \n",
    "                    print(f\"📊 Dataset info:\")\n",
    "                    print(f\"  Variables: {list(ds.data_vars.keys())}\")\n",
    "                    print(f\"  Dimensions: {dict(ds.dims)}\")\n",
    "                    print(f\"  Time range: {pd.to_datetime(ds.time.values).min()} to {pd.to_datetime(ds.time.values).max()}\")\n",
    "                    \n",
    "                    # Convert to DataFrame\n",
    "                    df_list = []\n",
    "                    \n",
    "                    # Extract precipitation data\n",
    "                    precip_data = ds['tp']  # total precipitation variable\n",
    "                    \n",
    "                    print(\"🔄 Converting to DataFrame...\")\n",
    "                    for time_idx, time_val in enumerate(ds.time.values):\n",
    "                        timestamp = pd.to_datetime(time_val)\n",
    "                        \n",
    "                        for lat_idx, lat_val in enumerate(ds.latitude.values):\n",
    "                            for lon_idx, lon_val in enumerate(ds.longitude.values):\n",
    "                                precip_value = float(precip_data.isel(\n",
    "                                    time=time_idx, \n",
    "                                    latitude=lat_idx, \n",
    "                                    longitude=lon_idx\n",
    "                                ).values)\n",
    "                                \n",
    "                                df_list.append({\n",
    "                                    'datetime': timestamp,\n",
    "                                    'latitude': lat_val,\n",
    "                                    'longitude': lon_val,\n",
    "                                    'precipitation_mm': precip_value * 1000  # Convert m to mm\n",
    "                                })\n",
    "                    \n",
    "                    # Create DataFrame and save to CSV\n",
    "                    df = pd.DataFrame(df_list)\n",
    "                    df = df.sort_values(['datetime', 'latitude', 'longitude'])\n",
    "                    df.to_csv(csv_filepath, index=False)\n",
    "                    \n",
    "                    print(f\"✅ Created CSV: {csv_filename}\")\n",
    "                    print(f\"📊 Records: {len(df)}\")\n",
    "                    print(f\"📅 Date range: {df['datetime'].min()} to {df['datetime'].max()}\")\n",
    "                    print(f\"🌧️  Precipitation range: {df['precipitation_mm'].min():.3f} to {df['precipitation_mm'].max():.3f} mm\")\n",
    "                    \n",
    "                    csv_files.append(csv_filepath)\n",
    "                    \n",
    "                    # Clean up temporary NetCDF file\n",
    "                    os.remove(temp_path)\n",
    "                    ds.close()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing ZIP file: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "    \n",
    "    return csv_files\n",
    "\n",
    "# Test both functions\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    lat, lon = 51.5074, -0.1278  # London coordinates\n",
    "    years = [2024]  # Start with just one year for testing\n",
    "    \n",
    "    print(\"Testing download function...\")\n",
    "    zip_file = download_era5_precip(lat, lon, years)\n",
    "    \n",
    "    if zip_file:\n",
    "        print(f\"Success! File saved at: {zip_file}\")\n",
    "        \n",
    "        print(\"\\nTesting extraction function...\")\n",
    "        csv_files = extract_and_convert_to_csv(zip_file)\n",
    "        \n",
    "        if csv_files:\n",
    "            print(f\"🎉 Created {len(csv_files)} CSV file(s)\")\n",
    "            \n",
    "            # Show sample data\n",
    "            import pandas as pd\n",
    "            df = pd.read_csv(csv_files[0])\n",
    "            print(\"\\n📊 Sample data:\")\n",
    "            print(df.head())\n",
    "            print(f\"\\nDataset shape: {df.shape}\")\n",
    "        else:\n",
    "            print(\"CSV conversion failed!\")\n",
    "    else:\n",
    "        print(\"Download failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f3f94f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_convert_to_csv(file_path, output_dir=None):\n",
    "    \"\"\"\n",
    "    Extract precipitation data from downloaded file (ZIP or GRIB) and convert to CSV\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to downloaded file\n",
    "        output_dir (str): Directory for CSV output (default: same as input file)\n",
    "    \n",
    "    Returns:\n",
    "        list: Paths to created CSV files\n",
    "    \"\"\"\n",
    "    import zipfile\n",
    "    import pandas as pd\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.dirname(file_path)\n",
    "    \n",
    "    csv_files = []\n",
    "    \n",
    "    print(f\"📦 Processing file: {os.path.basename(file_path)}\")\n",
    "    \n",
    "    # Check actual file format by reading first bytes\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            first_bytes = f.read(16)\n",
    "            print(f\"🔍 File signature: {first_bytes}\")\n",
    "        \n",
    "        # Check if it's a ZIP file\n",
    "        if first_bytes.startswith(b'PK\\x03\\x04'):\n",
    "            print(\"✅ Detected format: ZIP archive\")\n",
    "            return process_zip_file(file_path, output_dir)\n",
    "        \n",
    "        # Check if it's a GRIB file\n",
    "        elif first_bytes.startswith(b'GRIB'):\n",
    "            print(\"✅ Detected format: GRIB\")\n",
    "            return process_grib_file(file_path, output_dir)\n",
    "        \n",
    "        # Check if it's NetCDF (even with .zip extension)\n",
    "        elif b'CDF' in first_bytes or file_path.endswith('.nc'):\n",
    "            print(\"✅ Detected format: NetCDF\")\n",
    "            return process_netcdf_file(file_path, output_dir)\n",
    "        \n",
    "        else:\n",
    "            print(f\"❌ Unknown file format. First bytes: {first_bytes}\")\n",
    "            # Try to process as NetCDF anyway (sometimes NetCDF files have different signatures)\n",
    "            print(\"🔄 Attempting to process as NetCDF...\")\n",
    "            return process_netcdf_file(file_path, output_dir)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error analyzing file format: {e}\")\n",
    "        return []\n",
    "\n",
    "def process_zip_file(zip_filepath, output_dir):\n",
    "    \"\"\"Process a ZIP file containing NetCDF/GRIB files\"\"\"\n",
    "    import zipfile\n",
    "    \n",
    "    csv_files = []\n",
    "    \n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
    "            file_list = zip_ref.namelist()\n",
    "            print(f\"📄 Files in archive: {len(file_list)}\")\n",
    "            for file_name in file_list:\n",
    "                print(f\"  - {file_name}\")\n",
    "            \n",
    "            # Process each data file\n",
    "            for file_name in file_list:\n",
    "                if file_name.endswith(('.nc', '.grib', '.grib2', '.grb')):\n",
    "                    print(f\"🔄 Extracting and processing: {file_name}\")\n",
    "                    \n",
    "                    # Extract to temporary location\n",
    "                    temp_path = os.path.join(output_dir, file_name)\n",
    "                    zip_ref.extract(file_name, output_dir)\n",
    "                    \n",
    "                    # Process the extracted file\n",
    "                    if file_name.endswith('.nc'):\n",
    "                        csv_result = process_netcdf_file(temp_path, output_dir)\n",
    "                    else:\n",
    "                        csv_result = process_grib_file(temp_path, output_dir)\n",
    "                    \n",
    "                    csv_files.extend(csv_result)\n",
    "                    \n",
    "                    # Clean up temporary file\n",
    "                    os.remove(temp_path)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing ZIP file: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return csv_files\n",
    "\n",
    "def process_grib_file(grib_filepath, output_dir):\n",
    "    \"\"\"Process a GRIB file\"\"\"\n",
    "    csv_files = []\n",
    "    \n",
    "    try:\n",
    "        # Try with pygrib first\n",
    "        try:\n",
    "            import pygrib\n",
    "            print(\"🔄 Processing GRIB with pygrib...\")\n",
    "            \n",
    "            grbs = pygrib.open(grib_filepath)\n",
    "            print(f\"📊 Number of GRIB messages: {grbs.messages}\")\n",
    "            \n",
    "            # Collect all precipitation data\n",
    "            df_list = []\n",
    "            \n",
    "            for grb in grbs:\n",
    "                if 'precipitation' in grb.name.lower() or grb.shortName == 'tp':\n",
    "                    print(f\"📋 Processing: {grb.name}, Date: {grb.validDate}\")\n",
    "                    \n",
    "                    # Get data and coordinates\n",
    "                    data, lats, lons = grb.data()\n",
    "                    \n",
    "                    # Convert to DataFrame format\n",
    "                    for i in range(data.shape[0]):\n",
    "                        for j in range(data.shape[1]):\n",
    "                            if not np.isnan(data[i, j]):\n",
    "                                df_list.append({\n",
    "                                    'datetime': grb.validDate,\n",
    "                                    'latitude': lats[i, j],\n",
    "                                    'longitude': lons[i, j],\n",
    "                                    'precipitation_mm': data[i, j] * 1000  # Convert m to mm\n",
    "                                })\n",
    "            \n",
    "            grbs.close()\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"⚠️ pygrib not available, trying with xarray...\")\n",
    "            import xarray as xr\n",
    "            ds = xr.open_dataset(grib_filepath, engine='cfgrib')\n",
    "            return process_xarray_dataset(ds, output_dir)\n",
    "        \n",
    "        # Create DataFrame and CSV\n",
    "        if df_list:\n",
    "            import pandas as pd\n",
    "            df = pd.DataFrame(df_list)\n",
    "            df = df.sort_values(['datetime', 'latitude', 'longitude'])\n",
    "            \n",
    "            # Create filename\n",
    "            lat_center = df['latitude'].mean()\n",
    "            lon_center = df['longitude'].mean()\n",
    "            years = sorted(list(set(df['datetime'].dt.year)))\n",
    "            \n",
    "            csv_filename = create_csv_filename(lat_center, lon_center, years)\n",
    "            csv_filepath = os.path.join(output_dir, csv_filename)\n",
    "            \n",
    "            df.to_csv(csv_filepath, index=False)\n",
    "            print(f\"✅ Created CSV: {csv_filename}\")\n",
    "            print(f\"📊 Records: {len(df)}\")\n",
    "            \n",
    "            csv_files.append(csv_filepath)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing GRIB file: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return csv_files\n",
    "\n",
    "def process_netcdf_file(nc_filepath, output_dir):\n",
    "    \"\"\"Process a NetCDF file\"\"\"\n",
    "    try:\n",
    "        import xarray as xr\n",
    "        print(\"🔄 Processing NetCDF with xarray...\")\n",
    "        \n",
    "        ds = xr.open_dataset(nc_filepath)\n",
    "        return process_xarray_dataset(ds, output_dir)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing NetCDF file: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "def process_xarray_dataset(ds, output_dir):\n",
    "    \"\"\"Process an xarray dataset (NetCDF or GRIB)\"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    csv_files = []\n",
    "    \n",
    "    try:\n",
    "        print(f\"📊 Dataset info:\")\n",
    "        print(f\"  Variables: {list(ds.data_vars.keys())}\")\n",
    "        print(f\"  Dimensions: {dict(ds.dims)}\")\n",
    "        \n",
    "        # Find precipitation variable\n",
    "        precip_var = None\n",
    "        for var in ds.data_vars:\n",
    "            if 'tp' in var or 'precip' in var.lower() or 'rain' in var.lower():\n",
    "                precip_var = var\n",
    "                break\n",
    "        \n",
    "        if not precip_var:\n",
    "            print(\"❌ No precipitation variable found\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"🌧️ Using precipitation variable: {precip_var}\")\n",
    "        \n",
    "        # Get coordinates and time info\n",
    "        lat_vals = ds.latitude.values if 'latitude' in ds.dims else ds.lat.values\n",
    "        lon_vals = ds.longitude.values if 'longitude' in ds.dims else ds.lon.values\n",
    "        lat_center = float(lat_vals.mean())\n",
    "        lon_center = float(lon_vals.mean())\n",
    "        \n",
    "        # Get years from time dimension\n",
    "        time_vals = pd.to_datetime(ds.time.values)\n",
    "        years = sorted(list(set(time_vals.year)))\n",
    "        \n",
    "        print(f\"📅 Time range: {time_vals.min()} to {time_vals.max()}\")\n",
    "        \n",
    "        # Create CSV filename\n",
    "        csv_filename = create_csv_filename(lat_center, lon_center, years)\n",
    "        csv_filepath = os.path.join(output_dir, csv_filename)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df_list = []\n",
    "        precip_data = ds[precip_var]\n",
    "        \n",
    "        print(\"🔄 Converting to DataFrame...\")\n",
    "        for time_idx, time_val in enumerate(ds.time.values):\n",
    "            timestamp = pd.to_datetime(time_val)\n",
    "            \n",
    "            for lat_idx, lat_val in enumerate(lat_vals):\n",
    "                for lon_idx, lon_val in enumerate(lon_vals):\n",
    "                    precip_value = float(precip_data.isel(\n",
    "                        time=time_idx, \n",
    "                        latitude=lat_idx if 'latitude' in ds.dims else lat_idx, \n",
    "                        longitude=lon_idx if 'longitude' in ds.dims else lon_idx\n",
    "                    ).values)\n",
    "                    \n",
    "                    if not np.isnan(precip_value):\n",
    "                        df_list.append({\n",
    "                            'datetime': timestamp,\n",
    "                            'latitude': lat_val,\n",
    "                            'longitude': lon_val,\n",
    "                            'precipitation_mm': precip_value * 1000  # Convert m to mm\n",
    "                        })\n",
    "        \n",
    "        # Create DataFrame and save to CSV\n",
    "        df = pd.DataFrame(df_list)\n",
    "        df = df.sort_values(['datetime', 'latitude', 'longitude'])\n",
    "        df.to_csv(csv_filepath, index=False)\n",
    "        \n",
    "        print(f\"✅ Created CSV: {csv_filename}\")\n",
    "        print(f\"📊 Records: {len(df)}\")\n",
    "        print(f\"📅 Date range: {df['datetime'].min()} to {df['datetime'].max()}\")\n",
    "        print(f\"🌧️ Precipitation range: {df['precipitation_mm'].min():.3f} to {df['precipitation_mm'].max():.3f} mm\")\n",
    "        \n",
    "        csv_files.append(csv_filepath)\n",
    "        ds.close()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing dataset: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return csv_files\n",
    "\n",
    "def create_csv_filename(lat_center, lon_center, years):\n",
    "    \"\"\"Create standardized CSV filename\"\"\"\n",
    "    # Format coordinates for filename (replace dots with underscores)\n",
    "    lat_str = f\"{lat_center:.2f}\".replace('.', '_').replace('-', 'neg')\n",
    "    lon_str = f\"{lon_center:.2f}\".replace('.', '_').replace('-', 'neg')\n",
    "    years_str = \"_\".join(map(str, years))\n",
    "    \n",
    "    return f\"{lat_str}_{lon_str}_{years_str}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777a8695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60783576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Processing file: era5_precip_51_5074_neg0_1278_2023_2024.zip\n",
      "🔍 File signature: b'\\x89HDF\\r\\n\\x1a\\n\\x02\\x08\\x08\\x00\\x00\\x00\\x00\\x00'\n",
      "❌ Unknown file format. First bytes: b'\\x89HDF\\r\\n\\x1a\\n\\x02\\x08\\x08\\x00\\x00\\x00\\x00\\x00'\n",
      "🔄 Attempting to process as NetCDF...\n",
      "🔄 Processing NetCDF with xarray...\n",
      "📊 Dataset info:\n",
      "  Variables: ['tp']\n",
      "  Dimensions: {'valid_time': 17544, 'latitude': 1, 'longitude': 1}\n",
      "🌧️ Using precipitation variable: tp\n",
      "❌ Error processing dataset: 'Dataset' object has no attribute 'time'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pacor\\AppData\\Local\\Temp\\ipykernel_3512\\2297161511.py:187: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"  Dimensions: {dict(ds.dims)}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pacor\\AppData\\Local\\Temp\\ipykernel_3512\\2297161511.py\", line 209, in process_xarray_dataset\n",
      "    time_vals = pd.to_datetime(ds.time.values)\n",
      "                               ^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\xarray\\core\\common.py\", line 306, in __getattr__\n",
      "    raise AttributeError(\n",
      "AttributeError: 'Dataset' object has no attribute 'time'\n"
     ]
    }
   ],
   "source": [
    "csv_files = extract_and_convert_to_csv(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b402af3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a25866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e8fe5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb59b843",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def download_and_convert(lat, lon, years, output_dir=\"data\"):\n",
    "    \"\"\"\n",
    "    Complete workflow: download ERA5 data and convert to CSV\n",
    "    \n",
    "    Args:\n",
    "        lat (float): Latitude\n",
    "        lon (float): Longitude  \n",
    "        years (list): List of years\n",
    "        output_dir (str): Output directory\n",
    "    \n",
    "    Returns:\n",
    "        list: Paths to created CSV files\n",
    "    \"\"\"\n",
    "    print(\"🚀 Starting ERA5 precipitation download and conversion...\")\n",
    "    \n",
    "    # Step 1: Download data\n",
    "    zip_file = download_era5_precip(lat, lon, years, output_dir)\n",
    "    if not zip_file:\n",
    "        return []\n",
    "    \n",
    "    # Step 2: Convert to CSV\n",
    "    csv_files = extract_and_convert_to_csv(zip_file, output_dir)\n",
    "    \n",
    "    if csv_files:\n",
    "        print(f\"🎉 Complete! Created {len(csv_files)} CSV file(s)\")\n",
    "        for csv_file in csv_files:\n",
    "            print(f\"📄 {os.path.basename(csv_file)}\")\n",
    "    \n",
    "    return csv_files\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: Download data for London coordinates for 2023-2024\n",
    "    lat, lon = 51.5074, -0.1278  # London\n",
    "    years = [2023, 2024]\n",
    "    \n",
    "    csv_files = download_and_convert(lat, lon, years)\n",
    "    \n",
    "    # Display sample data\n",
    "    if csv_files:\n",
    "        print(\"\\n📊 Sample data from first CSV:\")\n",
    "        df = pd.read_csv(csv_files[0])\n",
    "        print(df.head())\n",
    "        print(f\"\\nTotal records: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_convert_to_csv(zip_filepath, output_dir=None):\n",
    "    \"\"\"\n",
    "    Extract precipitation data from ZIP file and convert to CSV\n",
    "    \n",
    "    Args:\n",
    "        zip_filepath (str): Path to downloaded ZIP file\n",
    "        output_dir (str): Directory for CSV output (default: same as ZIP file)\n",
    "    \n",
    "    Returns:\n",
    "        list: Paths to created CSV files\n",
    "    \"\"\"\n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.dirname(zip_filepath)\n",
    "    \n",
    "    csv_files = []\n",
    "    \n",
    "    print(f\"📦 Processing ZIP file: {os.path.basename(zip_filepath)}\")\n",
    "    \n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
    "            # List files in archive\n",
    "            file_list = zip_ref.namelist()\n",
    "            print(f\"📄 Files in archive: {len(file_list)}\")\n",
    "            \n",
    "            # Process each NetCDF file\n",
    "            for file_name in file_list:\n",
    "                if file_name.endswith('.nc'):\n",
    "                    print(f\"🔄 Processing: {file_name}\")\n",
    "                    \n",
    "                    # Extract to temporary location\n",
    "                    temp_path = os.path.join(output_dir, file_name)\n",
    "                    zip_ref.extract(file_name, output_dir)\n",
    "                    \n",
    "                    # Open with xarray\n",
    "                    ds = xr.open_dataset(temp_path)\n",
    "                    \n",
    "                    # Get coordinates info for filename\n",
    "                    lat_vals = ds.latitude.values\n",
    "                    lon_vals = ds.longitude.values\n",
    "                    lat_center = float(lat_vals.mean())\n",
    "                    lon_center = float(lon_vals.mean())\n",
    "                    \n",
    "                    # Get years from time dimension\n",
    "                    years = sorted(list(set(pd.to_datetime(ds.time.values).year)))\n",
    "                    years_str = \"_\".join(map(str, years))\n",
    "                    \n",
    "                    # Create CSV filename\n",
    "                    csv_filename = f\"{lat_center:.2f}_{lon_center:.2f}_{years_str}.csv\"\n",
    "                    csv_filepath = os.path.join(output_dir, csv_filename)\n",
    "                    \n",
    "                    # Convert to DataFrame\n",
    "                    df_list = []\n",
    "                    \n",
    "                    # Extract precipitation data\n",
    "                    precip_data = ds['tp']  # total precipitation variable\n",
    "                    \n",
    "                    for time_idx, time_val in enumerate(ds.time.values):\n",
    "                        timestamp = pd.to_datetime(time_val)\n",
    "                        \n",
    "                        for lat_idx, lat_val in enumerate(ds.latitude.values):\n",
    "                            for lon_idx, lon_val in enumerate(ds.longitude.values):\n",
    "                                precip_value = float(precip_data.isel(\n",
    "                                    time=time_idx, \n",
    "                                    latitude=lat_idx, \n",
    "                                    longitude=lon_idx\n",
    "                                ).values)\n",
    "                                \n",
    "                                df_list.append({\n",
    "                                    'datetime': timestamp,\n",
    "                                    'latitude': lat_val,\n",
    "                                    'longitude': lon_val,\n",
    "                                    'precipitation_mm': precip_value * 1000  # Convert m to mm\n",
    "                                })\n",
    "                    \n",
    "                    # Create DataFrame and save to CSV\n",
    "                    df = pd.DataFrame(df_list)\n",
    "                    df = df.sort_values(['datetime', 'latitude', 'longitude'])\n",
    "                    df.to_csv(csv_filepath, index=False)\n",
    "                    \n",
    "                    print(f\"✅ Created CSV: {csv_filename}\")\n",
    "                    print(f\"📊 Records: {len(df)}\")\n",
    "                    print(f\"📅 Date range: {df['datetime'].min()} to {df['datetime'].max()}\")\n",
    "                    \n",
    "                    csv_files.append(csv_filepath)\n",
    "                    \n",
    "                    # Clean up temporary NetCDF file\n",
    "                    os.remove(temp_path)\n",
    "                    ds.close()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing ZIP file: {e}\")\n",
    "        return []\n",
    "    \n",
    "    return csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55344644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Processing ZIP file: data\n",
      "❌ Error processing ZIP file: [Errno 13] Permission denied: './data'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_and_convert_to_csv('./data', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfd4458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e44021a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06b4c93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76955866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c78dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_file(file_path):\n",
    "    \"\"\"Analyze the downloaded file to determine its format and contents\"\"\"\n",
    "    print(\"🔍 Analyzing downloaded file...\")\n",
    "    \n",
    "    try:\n",
    "        # Read first few bytes to identify file type\n",
    "        with open(file_path, 'rb') as f:\n",
    "            first_bytes = f.read(16)\n",
    "            print(f\"File signature: {first_bytes}\")\n",
    "        \n",
    "        # Check if it's a ZIP file\n",
    "        if first_bytes.startswith(b'PK\\x03\\x04'):\n",
    "            print(\"✅ File format: ZIP archive\")\n",
    "            try:\n",
    "                import zipfile\n",
    "                with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                    file_list = zip_ref.namelist()\n",
    "                    print(f\"📦 Files in archive: {len(file_list)}\")\n",
    "                    for file_name in file_list[:5]:  # Show first 5 files\n",
    "                        file_info = zip_ref.getinfo(file_name)\n",
    "                        size_mb = file_info.file_size / (1024 * 1024)\n",
    "                        print(f\"  📄 {file_name} ({size_mb:.2f} MB)\")\n",
    "                    \n",
    "                    if len(file_list) > 5:\n",
    "                        print(f\"  ... and {len(file_list) - 5} more files\")\n",
    "                    \n",
    "                    # Extract and analyze the first GRIB or NetCDF file\n",
    "                    for file_name in file_list:\n",
    "                        if file_name.endswith(('.grib', '.grib2', '.grb', '.nc')):\n",
    "                            print(f\"🔄 Extracting and analyzing: {file_name}\")\n",
    "                            extract_path = os.path.join(os.path.dirname(file_path), file_name)\n",
    "                            zip_ref.extract(file_name, os.path.dirname(file_path))\n",
    "                            analyze_extracted_file(extract_path)\n",
    "                            break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  Error reading ZIP file: {e}\")\n",
    "        \n",
    "        # Check if it's a GRIB file\n",
    "        elif first_bytes.startswith(b'GRIB'):\n",
    "            print(\"✅ File format: GRIB\")\n",
    "            analyze_grib_file(file_path)\n",
    "        \n",
    "        # Check if it's a NetCDF file\n",
    "        elif file_path.endswith('.nc') or b'CDF' in first_bytes:\n",
    "            print(\"✅ File format: NetCDF\")\n",
    "            analyze_netcdf_file(file_path)\n",
    "        \n",
    "        else:\n",
    "            print(\"❌ Unknown file format. First bytes:\", first_bytes)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error analyzing file: {e}\")\n",
    "    \n",
    "    # Show file info\n",
    "    file_size = os.path.getsize(file_path) / (1024 * 1024)  # Convert to MB\n",
    "    print(f\"📏 File size: {file_size:.2f} MB\")\n",
    "\n",
    "def analyze_grib_file(file_path):\n",
    "    \"\"\"Analyze GRIB file contents\"\"\"\n",
    "    try:\n",
    "        import pygrib\n",
    "        grbs = pygrib.open(file_path)\n",
    "        print(f\"📊 Number of messages: {grbs.messages}\")\n",
    "        \n",
    "        # List first few messages\n",
    "        for i, grb in enumerate(grbs):\n",
    "            if i >= 3:  # Show first 3 messages\n",
    "                break\n",
    "            print(f\"  Message {i+1}: {grb.name}, Level: {grb.level}, Date: {grb.validDate}\")\n",
    "        grbs.close()\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"⚠️  pygrib not installed - cannot analyze GRIB contents\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error reading GRIB file: {e}\")\n",
    "\n",
    "def analyze_netcdf_file(file_path):\n",
    "    \"\"\"Analyze NetCDF file contents\"\"\"\n",
    "    try:\n",
    "        import xarray as xr\n",
    "        ds = xr.open_dataset(file_path)\n",
    "        print(f\"📊 Variables: {list(ds.data_vars.keys())}\")\n",
    "        print(f\"📊 Dimensions: {dict(ds.dims)}\")\n",
    "        ds.close()\n",
    "    except ImportError:\n",
    "        print(\"⚠️  xarray not installed - cannot analyze NetCDF contents\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error reading NetCDF file: {e}\")\n",
    "\n",
    "def analyze_extracted_file(file_path):\n",
    "    \"\"\"Analyze an extracted file\"\"\"\n",
    "    if file_path.endswith(('.grib', '.grib2', '.grb')):\n",
    "        analyze_grib_file(file_path)\n",
    "    elif file_path.endswith('.nc'):\n",
    "        analyze_netcdf_file(file_path)\n",
    "    else:\n",
    "        print(f\"📄 Extracted file: {os.path.basename(file_path)}\")\n",
    "        file_size = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        print(f\"📏 Size: {file_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff038e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4db12a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9830af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e675981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import zipfile\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def download_era5_precipitation_to_dataframe(\n",
    "    area=[51.7, -0.5, 51.3, 0.2],  # London area: [N, W, S, E]\n",
    "    year='2024',\n",
    "    month='01',\n",
    "    days=['01', '02', '03'],  # First 3 days of January by default\n",
    "    output_csv='london_precipitation.csv',\n",
    "    return_format='time_series',  # 'long_format', 'time_series', or 'spatial'\n",
    "    cleanup_files=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete function to download ERA5 precipitation data and return as pandas DataFrame\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    area : list\n",
    "        Bounding box [North, West, South, East] in decimal degrees\n",
    "    year : str\n",
    "        Year to download (e.g., '2024')\n",
    "    month : str  \n",
    "        Month to download (e.g., '01' for January)\n",
    "    days : list\n",
    "        List of days to download (e.g., ['01', '02', '03'])\n",
    "    output_csv : str\n",
    "        Name of the output CSV file\n",
    "    return_format : str\n",
    "        Format of returned DataFrame:\n",
    "        - 'long_format': Every data point as a row\n",
    "        - 'time_series': Time series with spatial average\n",
    "        - 'spatial': Spatial data with time average\n",
    "    cleanup_files : bool\n",
    "        Whether to delete intermediate files after processing\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Precipitation data in the specified format\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🌧️  ERA5 Precipitation Data Processor\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Download data from CDS API\n",
    "    print(f\"📥 Downloading ERA5 data for {year}-{month}, days {days}\")\n",
    "    print(f\"📍 Area: {area} (N, W, S, E)\")\n",
    "    \n",
    "    temp_grib_file = f\"era5_precip_{year}{month}_temp.grib\"\n",
    "    \n",
    "    try:\n",
    "        c = cdsapi.Client()\n",
    "        \n",
    "        request = {\n",
    "            'variable': 'total_precipitation',\n",
    "            'year': year,\n",
    "            'month': month,\n",
    "            'day': days,\n",
    "            'time': [\n",
    "                '00:00', '01:00', '02:00', '03:00', '04:00', '05:00',\n",
    "                '06:00', '07:00', '08:00', '09:00', '10:00', '11:00', \n",
    "                '12:00', '13:00', '14:00', '15:00', '16:00', '17:00',\n",
    "                '18:00', '19:00', '20:00', '21:00', '22:00', '23:00'\n",
    "            ],\n",
    "            'area': area,\n",
    "            'format': 'grib',\n",
    "        }\n",
    "        \n",
    "        c.retrieve('reanalysis-era5-land', request, temp_grib_file)\n",
    "        print(f\"✅ Download complete: {temp_grib_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Download failed: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Step 2: Check if downloaded file is ZIP or GRIB\n",
    "    print(f\"🔍 Analyzing downloaded file...\")\n",
    "    \n",
    "    with open(temp_grib_file, 'rb') as f:\n",
    "        first_bytes = f.read(4)\n",
    "    \n",
    "    if first_bytes == b'PK\\x03':  # ZIP file\n",
    "        print(\"📦 Downloaded file is ZIP format, extracting...\")\n",
    "        grib_file = extract_grib_from_zip(temp_grib_file)\n",
    "        if grib_file is None:\n",
    "            print(\"❌ Failed to extract GRIB from ZIP\")\n",
    "            return None\n",
    "    elif first_bytes == b'GRIB':  # Already GRIB\n",
    "        print(\"📄 Downloaded file is already GRIB format\")\n",
    "        grib_file = temp_grib_file\n",
    "    else:\n",
    "        print(f\"❌ Unknown file format. First bytes: {first_bytes}\")\n",
    "        return None\n",
    "    \n",
    "    # Step 3: Load GRIB data\n",
    "    print(f\"📊 Loading GRIB data from: {grib_file}\")\n",
    "    \n",
    "    try:\n",
    "        # Try multiple methods to load GRIB\n",
    "        ds = None\n",
    "        methods = [\n",
    "            ('cfgrib', {}),\n",
    "            ('cfgrib with error ignore', {'errors': 'ignore'}),\n",
    "            ('cfgrib with filter', {'filter_by_keys': {'paramId': 228}})\n",
    "        ]\n",
    "        \n",
    "        for method_name, backend_kwargs in methods:\n",
    "            try:\n",
    "                print(f\"🔄 Trying {method_name}...\")\n",
    "                ds = xr.open_dataset(grib_file, engine='cfgrib', backend_kwargs=backend_kwargs)\n",
    "                print(f\"✅ Success with {method_name}!\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"❌ {method_name} failed: {str(e)[:100]}...\")\n",
    "                continue\n",
    "        \n",
    "        if ds is None:\n",
    "            print(\"❌ All GRIB loading methods failed\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading GRIB: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Step 4: Convert to DataFrame\n",
    "    print(f\"🔄 Converting to DataFrame (format: {return_format})...\")\n",
    "    \n",
    "    try:\n",
    "        if return_format == 'long_format':\n",
    "            df = convert_to_long_format(ds)\n",
    "        elif return_format == 'time_series':\n",
    "            df = convert_to_time_series(ds)\n",
    "        elif return_format == 'spatial':\n",
    "            df = convert_to_spatial(ds)\n",
    "        else:\n",
    "            print(f\"⚠️  Unknown format '{return_format}', using time_series\")\n",
    "            df = convert_to_time_series(ds)\n",
    "        \n",
    "        print(f\"✅ DataFrame created: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error converting to DataFrame: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Step 5: Save to CSV\n",
    "    print(f\"💾 Saving to CSV: {output_csv}\")\n",
    "    \n",
    "    try:\n",
    "        # Add metadata header\n",
    "        metadata = [\n",
    "            f\"# ERA5 Land Reanalysis - Total Precipitation\",\n",
    "            f\"# Downloaded: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "            f\"# Period: {year}-{month}, days {days}\",\n",
    "            f\"# Area: {area} (North, West, South, East)\",\n",
    "            f\"# Format: {return_format}\",\n",
    "            f\"# Rows: {len(df)}, Columns: {len(df.columns)}\",\n",
    "            f\"# Variables: {', '.join(df.columns)}\",\n",
    "            \"#\"\n",
    "        ]\n",
    "        \n",
    "        with open(output_csv, 'w') as f:\n",
    "            for line in metadata:\n",
    "                f.write(line + '\\n')\n",
    "            df.to_csv(f, index=False)\n",
    "        \n",
    "        file_size = Path(output_csv).stat().st_size / (1024*1024)\n",
    "        print(f\"✅ CSV saved successfully! Size: {file_size:.2f} MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving CSV: {e}\")\n",
    "    \n",
    "    # Step 6: Cleanup intermediate files\n",
    "    if cleanup_files:\n",
    "        print(\"🧹 Cleaning up intermediate files...\")\n",
    "        files_to_remove = [temp_grib_file]\n",
    "        if grib_file != temp_grib_file:\n",
    "            files_to_remove.append(grib_file)\n",
    "        \n",
    "        for file_path in files_to_remove:\n",
    "            try:\n",
    "                if os.path.exists(file_path):\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"   🗑️  Removed: {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️  Could not remove {file_path}: {e}\")\n",
    "    \n",
    "    # Step 7: Show summary\n",
    "    print(f\"\\n📊 Final Dataset Summary:\")\n",
    "    print(f\"   Shape: {df.shape}\")\n",
    "    print(f\"   Columns: {list(df.columns)}\")\n",
    "    print(f\"   Date range: {df['time'].min()} to {df['time'].max()}\" if 'time' in df.columns else \"   No time column\")\n",
    "    print(f\"   Precipitation range: {df.filter(regex='tp|precip').min().min():.6f} to {df.filter(regex='tp|precip').max().max():.6f}\")\n",
    "    \n",
    "    print(f\"\\n🎉 Process complete! Data ready for analysis.\")\n",
    "    \n",
    "    # Close the xarray dataset\n",
    "    ds.close()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_grib_from_zip(zip_file_path):\n",
    "    \"\"\"Extract GRIB file from ZIP archive\"\"\"\n",
    "    try:\n",
    "        extract_dir = Path(zip_file_path).parent / \"extracted\"\n",
    "        extract_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_dir)\n",
    "            \n",
    "            # Find the extracted file\n",
    "            extracted_files = list(extract_dir.glob('*'))\n",
    "            if extracted_files:\n",
    "                return str(extracted_files[0])\n",
    "        \n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting ZIP: {e}\")\n",
    "        return None\n",
    "\n",
    "def convert_to_long_format(ds):\n",
    "    \"\"\"Convert xarray dataset to long format DataFrame\"\"\"\n",
    "    df = ds.to_dataframe().reset_index()\n",
    "    return df.dropna()\n",
    "\n",
    "def convert_to_time_series(ds):\n",
    "    \"\"\"Convert to time series by averaging spatial dimensions\"\"\"\n",
    "    spatial_dims = [dim for dim in ['latitude', 'longitude'] if dim in ds.dims]\n",
    "    if spatial_dims:\n",
    "        ds_avg = ds.mean(dim=spatial_dims)\n",
    "    else:\n",
    "        ds_avg = ds\n",
    "    \n",
    "    df = ds_avg.to_dataframe().reset_index()\n",
    "    return df.dropna()\n",
    "\n",
    "def convert_to_spatial(ds):\n",
    "    \"\"\"Convert to spatial format by averaging time dimension\"\"\"\n",
    "    if 'time' in ds.dims:\n",
    "        ds_avg = ds.mean(dim='time')\n",
    "    else:\n",
    "        ds_avg = ds\n",
    "    \n",
    "    df = ds_avg.to_dataframe().reset_index()\n",
    "    return df.dropna()\n",
    "\n",
    "# Quick preset functions for common use cases\n",
    "def download_london_precipitation_last_week(output_csv='london_last_week.csv'):\n",
    "    \"\"\"Download last week of precipitation data for London\"\"\"\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    # Get last week's dates\n",
    "    today = datetime.now()\n",
    "    week_ago = today - timedelta(days=7)\n",
    "    \n",
    "    days = [(week_ago + timedelta(days=i)).strftime('%d') for i in range(7)]\n",
    "    \n",
    "    return download_era5_precipitation_to_dataframe(\n",
    "        area=[51.7, -0.5, 51.3, 0.2],  # London\n",
    "        year=week_ago.strftime('%Y'),\n",
    "        month=week_ago.strftime('%m'),\n",
    "        days=days,\n",
    "        output_csv=output_csv,\n",
    "        return_format='time_series'\n",
    "    )\n",
    "\n",
    "def download_uk_precipitation_month(year='2024', month='01', output_csv='uk_precipitation.csv'):\n",
    "    \"\"\"Download a full month of precipitation data for the UK\"\"\"\n",
    "    # UK bounding box\n",
    "    uk_area = [60.0, -8.0, 49.0, 2.0]  # [N, W, S, E]\n",
    "    \n",
    "    # All days in month\n",
    "    days = [f\"{i:02d}\" for i in range(1, 32)]  # Will auto-adjust for month length\n",
    "    \n",
    "    return download_era5_precipitation_to_dataframe(\n",
    "        area=uk_area,\n",
    "        year=year,\n",
    "        month=month,\n",
    "        days=days,\n",
    "        output_csv=output_csv,\n",
    "        return_format='time_series'\n",
    "    )\n",
    "\n",
    "# # Example usage and main execution\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"🚀 ERA5 Precipitation Data Processor\")\n",
    "#     print(\"=\" * 50)\n",
    "    \n",
    "#     # Example 1: Download London data for first 3 days of January 2024\n",
    "#     print(\"\\n📊 Example 1: London precipitation (3 days)\")\n",
    "#     df = download_era5_precipitation_to_dataframe(\n",
    "#         area=[51.7, -0.5, 51.3, 0.2],  # London\n",
    "#         year='2024',\n",
    "#         month='01', \n",
    "#         days=['01', '02', '03'],\n",
    "#         output_csv='london_3days.csv',\n",
    "#         return_format='time_series'\n",
    "#     )\n",
    "    \n",
    "#     if df is not None:\n",
    "#         print(f\"✅ Success! DataFrame shape: {df.shape}\")\n",
    "#         print(f\"📊 Sample data:\\n{df.head()}\")\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*50)\n",
    "#     print(\"🎯 Usage Examples:\")\n",
    "#     print(\"1. df = download_era5_precipitation_to_dataframe()\")\n",
    "#     print(\"2. df = download_london_precipitation_last_week()\")\n",
    "#     print(\"3. df = download_uk_precipitation_month('2024', '01')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfc2d381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌧️  ERA5 Precipitation Data Processor\n",
      "==================================================\n",
      "📥 Downloading ERA5 data for 2024-01, days ['01', '02', '03', '04', '05']\n",
      "📍 Area: [51.7, -0.5, 51.3, 0.2] (N, W, S, E)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 13:03:38,281 INFO [2025-06-10T00:00:00] To improve our C3S service, we need to hear from you! Please complete this very short [survey](https://confluence.ecmwf.int/x/E7uBEQ/). Thank you.\n",
      "2025-07-02 13:03:38,283 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-02 13:03:38,706 INFO Request ID is c225dcd2-7d8f-4b9e-afd9-93a88ba7d7de\n",
      "2025-07-02 13:03:38,843 INFO status has been updated to accepted\n",
      "2025-07-02 13:03:47,742 INFO status has been updated to running\n",
      "2025-07-02 13:04:30,017 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97fc3ff140784b1b8f28d52eb89d35ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "75bf13f192a81bbbce5196fb932c9bd8.zip:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Download complete: era5_precip_202401_temp.grib\n",
      "🔍 Analyzing downloaded file...\n",
      "❌ Unknown file format. First bytes: b'PK\\x03\\x04'\n"
     ]
    }
   ],
   "source": [
    "# Custom area and time period\n",
    "df = download_era5_precipitation_to_dataframe(\n",
    "    area=[51.7, -0.5, 51.3, 0.2],  # London bounding box\n",
    "    year='2024',\n",
    "    month='01',\n",
    "    days=['01', '02', '03', '04', '05'],  # First 5 days\n",
    "    output_csv='my_precipitation_data.csv',\n",
    "    return_format='time_series'  # or 'long_format' or 'spatial'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f205a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c516c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91e57007",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b5f6994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced ERA5-Land Precipitation Data Processor\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 14:43:48,423 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-02 14:43:48,624 INFO Request ID is 52d7c0ac-3063-46be-aa64-715bb092a612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ERA5-Land hourly precipitation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 14:43:48,703 INFO status has been updated to accepted\n",
      "2025-07-02 14:43:57,238 INFO status has been updated to running\n",
      "2025-07-02 15:12:10,884 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70d11d58005481784e5c629d3697f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "680d5d694ff0c8393127335063713180.zip:   0%|          | 0.00/193k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded to london_era5land_hourly_precip.grib\n",
      "File size: 0.19 MB\n",
      "\n",
      "Method 1: Attempting to open with cfgrib engine...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Can't create file 'london_era5land_hourly_precip.grib.5b7b6.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 274, in itervalues\n",
      "    yield self.filestream.message_from_file(file, errors=errors)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 341, in message_from_file\n",
      "    return Message.from_file(file, offset, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 105, in from_file\n",
      "    raise EOFError(\"End of file: %r\" % file)\n",
      "EOFError: End of file: <_io.BufferedReader name='london_era5land_hourly_precip.grib'>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 539, in from_indexpath_or_filestream\n",
      "    self = cls.from_fieldset(filestream, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 379, in from_fieldset\n",
      "    return cls.from_fieldset_and_iteritems(fieldset, iteritems, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 392, in from_fieldset_and_iteritems\n",
      "    for field_id, raw_field in iteritems:\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 292, in __iter__\n",
      "    for message in self.itervalues():\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 278, in itervalues\n",
      "    raise EOFError(\"No valid message found: %r\" % self.filestream.path)\n",
      "EOFError: No valid message found: 'london_era5land_hourly_precip.grib'\n",
      "Can't read index file 'london_era5land_hourly_precip.grib.5b7b6.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 548, in from_indexpath_or_filestream\n",
      "    index_mtime = os.path.getmtime(indexpath)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen genericpath>\", line 67, in getmtime\n",
      "FileNotFoundError: [WinError 2] The system cannot find the file specified: 'london_era5land_hourly_precip.grib.5b7b6.idx'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ cfgrib failed: No valid message found: 'london_era5land_hourly_precip.grib'\n",
      "\n",
      "Method 2: Trying cfgrib with error handling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Can't create file 'london_era5land_hourly_precip.grib.5b7b6.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 274, in itervalues\n",
      "    yield self.filestream.message_from_file(file, errors=errors)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 341, in message_from_file\n",
      "    return Message.from_file(file, offset, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 105, in from_file\n",
      "    raise EOFError(\"End of file: %r\" % file)\n",
      "EOFError: End of file: <_io.BufferedReader name='london_era5land_hourly_precip.grib'>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pacor\\AppData\\Local\\Temp\\ipykernel_3512\\1757161118.py\", line 44, in download_and_process_era5_precip\n",
      "    ds = xr.open_dataset(grib_file, engine='cfgrib')\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\xarray\\backends\\api.py\", line 687, in open_dataset\n",
      "    backend_ds = backend.open_dataset(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\xarray_plugin.py\", line 112, in open_dataset\n",
      "    store = CfGribDataStore(\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\xarray_plugin.py\", line 40, in __init__\n",
      "    self.ds = opener(filename, **backend_kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\dataset.py\", line 827, in open_file\n",
      "    index = open_fileindex(stream, indexpath, index_keys, ignore_keys=ignore_keys, filter_by_keys=filter_by_keys)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\dataset.py\", line 806, in open_fileindex\n",
      "    index = messages.FileIndex.from_indexpath_or_filestream(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 565, in from_indexpath_or_filestream\n",
      "    return cls.from_fieldset(filestream, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 379, in from_fieldset\n",
      "    return cls.from_fieldset_and_iteritems(fieldset, iteritems, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 392, in from_fieldset_and_iteritems\n",
      "    for field_id, raw_field in iteritems:\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 292, in __iter__\n",
      "    for message in self.itervalues():\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 278, in itervalues\n",
      "    raise EOFError(\"No valid message found: %r\" % self.filestream.path)\n",
      "EOFError: No valid message found: 'london_era5land_hourly_precip.grib'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 274, in itervalues\n",
      "    yield self.filestream.message_from_file(file, errors=errors)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 341, in message_from_file\n",
      "    return Message.from_file(file, offset, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 105, in from_file\n",
      "    raise EOFError(\"End of file: %r\" % file)\n",
      "EOFError: End of file: <_io.BufferedReader name='london_era5land_hourly_precip.grib'>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 539, in from_indexpath_or_filestream\n",
      "    self = cls.from_fieldset(filestream, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 379, in from_fieldset\n",
      "    return cls.from_fieldset_and_iteritems(fieldset, iteritems, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 392, in from_fieldset_and_iteritems\n",
      "    for field_id, raw_field in iteritems:\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 292, in __iter__\n",
      "    for message in self.itervalues():\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 278, in itervalues\n",
      "    raise EOFError(\"No valid message found: %r\" % self.filestream.path)\n",
      "EOFError: No valid message found: 'london_era5land_hourly_precip.grib'\n",
      "Can't read index file 'london_era5land_hourly_precip.grib.5b7b6.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 274, in itervalues\n",
      "    yield self.filestream.message_from_file(file, errors=errors)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 341, in message_from_file\n",
      "    return Message.from_file(file, offset, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 105, in from_file\n",
      "    raise EOFError(\"End of file: %r\" % file)\n",
      "EOFError: End of file: <_io.BufferedReader name='london_era5land_hourly_precip.grib'>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pacor\\AppData\\Local\\Temp\\ipykernel_3512\\1757161118.py\", line 44, in download_and_process_era5_precip\n",
      "    ds = xr.open_dataset(grib_file, engine='cfgrib')\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\xarray\\backends\\api.py\", line 687, in open_dataset\n",
      "    backend_ds = backend.open_dataset(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\xarray_plugin.py\", line 112, in open_dataset\n",
      "    store = CfGribDataStore(\n",
      "            ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\xarray_plugin.py\", line 40, in __init__\n",
      "    self.ds = opener(filename, **backend_kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\dataset.py\", line 827, in open_file\n",
      "    index = open_fileindex(stream, indexpath, index_keys, ignore_keys=ignore_keys, filter_by_keys=filter_by_keys)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\dataset.py\", line 806, in open_fileindex\n",
      "    index = messages.FileIndex.from_indexpath_or_filestream(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 565, in from_indexpath_or_filestream\n",
      "    return cls.from_fieldset(filestream, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 379, in from_fieldset\n",
      "    return cls.from_fieldset_and_iteritems(fieldset, iteritems, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 392, in from_fieldset_and_iteritems\n",
      "    for field_id, raw_field in iteritems:\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 292, in __iter__\n",
      "    for message in self.itervalues():\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 278, in itervalues\n",
      "    raise EOFError(\"No valid message found: %r\" % self.filestream.path)\n",
      "EOFError: No valid message found: 'london_era5land_hourly_precip.grib'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 548, in from_indexpath_or_filestream\n",
      "    index_mtime = os.path.getmtime(indexpath)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen genericpath>\", line 67, in getmtime\n",
      "FileNotFoundError: [WinError 2] The system cannot find the file specified: 'london_era5land_hourly_precip.grib.5b7b6.idx'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ cfgrib with error handling failed: No valid message found: 'london_era5land_hourly_precip.grib'\n",
      "\n",
      "Method 3: Trying pygrib...\n",
      "Opening london_era5land_hourly_precip.grib with pygrib...\n",
      "Total messages in file: 0\n",
      "Found 0 messages\n",
      "pygrib processing failed: no matches found\n",
      "\n",
      "✓ Processing completed successfully!\n",
      "Dataset available for further processing\n"
     ]
    }
   ],
   "source": [
    "# 14:36 test\n",
    "\n",
    "import cdsapi\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "def download_and_process_era5_precip(output_file=\"london_era5land_hourly_precip\"):\n",
    "    \"\"\"\n",
    "    Enhanced function to download and process ERA5-Land precipitation data with multiple fallback options\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the CDS API client\n",
    "    c = cdsapi.Client()\n",
    "    \n",
    "    # Define the request parameters\n",
    "    request_params = {\n",
    "        'variable': 'total_precipitation',\n",
    "        'year': ['2022'],\n",
    "        'month': ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'],\n",
    "        'day': [f'{i:02d}' for i in range(1, 32)],\n",
    "        'time': [f'{i:02d}:00' for i in range(24)],\n",
    "        'area': [51.6, -0.2, 51.4, 0.1],  # London area: North, West, South, East\n",
    "        'format': 'grib',\n",
    "    }\n",
    "    \n",
    "    grib_file = f\"{output_file}.grib\"\n",
    "    netcdf_file = f\"{output_file}.nc\"\n",
    "    \n",
    "    print(\"Downloading ERA5-Land hourly precipitation data...\")\n",
    "    \n",
    "    try:\n",
    "        # Try downloading as GRIB first\n",
    "        c.retrieve('reanalysis-era5-land', request_params, grib_file)\n",
    "        print(f\"Data downloaded to {grib_file}\")\n",
    "        print(f\"File size: {os.path.getsize(grib_file) / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        # Method 1: Try opening with cfgrib (original approach)\n",
    "        print(\"\\nMethod 1: Attempting to open with cfgrib engine...\")\n",
    "        try:\n",
    "            ds = xr.open_dataset(grib_file, engine='cfgrib')\n",
    "            print(\"✓ Successfully opened with cfgrib\")\n",
    "            return process_dataset(ds, \"cfgrib\")\n",
    "            \n",
    "        except Exception as e1:\n",
    "            print(f\"✗ cfgrib failed: {e1}\")\n",
    "            \n",
    "            # Method 2: Try with error handling options\n",
    "            print(\"\\nMethod 2: Trying cfgrib with error handling...\")\n",
    "            try:\n",
    "                ds = xr.open_dataset(grib_file, engine='cfgrib', \n",
    "                                   backend_kwargs={'errors': 'ignore'})\n",
    "                print(\"✓ Successfully opened with cfgrib (ignoring errors)\")\n",
    "                return process_dataset(ds, \"cfgrib_ignore_errors\")\n",
    "                \n",
    "            except Exception as e2:\n",
    "                print(f\"✗ cfgrib with error handling failed: {e2}\")\n",
    "                \n",
    "                # Method 3: Try pygrib if available\n",
    "                print(\"\\nMethod 3: Trying pygrib...\")\n",
    "                try:\n",
    "                    import pygrib\n",
    "                    return process_with_pygrib(grib_file)\n",
    "                    \n",
    "                except ImportError:\n",
    "                    print(\"✗ pygrib not available. Install with: pip install pygrib\")\n",
    "                except Exception as e3:\n",
    "                    print(f\"✗ pygrib failed: {e3}\")\n",
    "                \n",
    "                # Method 4: Download as NetCDF instead\n",
    "                print(\"\\nMethod 4: Downloading as NetCDF format...\")\n",
    "                try:\n",
    "                    request_params['format'] = 'netcdf'\n",
    "                    c.retrieve('reanalysis-era5-land', request_params, netcdf_file)\n",
    "                    print(f\"NetCDF data downloaded to {netcdf_file}\")\n",
    "                    \n",
    "                    ds = xr.open_dataset(netcdf_file)\n",
    "                    print(\"✓ Successfully opened NetCDF file\")\n",
    "                    return process_dataset(ds, \"netcdf\")\n",
    "                    \n",
    "                except Exception as e4:\n",
    "                    print(f\"✗ NetCDF download/processing failed: {e4}\")\n",
    "                    \n",
    "                    # Method 5: Manual GRIB inspection\n",
    "                    print(\"\\nMethod 5: Manual GRIB file inspection...\")\n",
    "                    return inspect_grib_file(grib_file)\n",
    "    \n",
    "    except Exception as download_error:\n",
    "        print(f\"Download failed: {download_error}\")\n",
    "        return None\n",
    "\n",
    "def process_dataset(ds, method_name):\n",
    "    \"\"\"Process the successfully opened dataset\"\"\"\n",
    "    print(f\"\\n=== Dataset Info ({method_name}) ===\")\n",
    "    print(f\"Variables: {list(ds.variables.keys())}\")\n",
    "    print(f\"Dimensions: {ds.dims}\")\n",
    "    print(f\"Coordinates: {list(ds.coords.keys())}\")\n",
    "    \n",
    "    # Find precipitation variable (different names possible)\n",
    "    precip_vars = [var for var in ds.variables.keys() \n",
    "                   if any(keyword in var.lower() for keyword in ['precip', 'tp', 'rain'])]\n",
    "    \n",
    "    if precip_vars:\n",
    "        precip_var = precip_vars[0]\n",
    "        print(f\"Found precipitation variable: {precip_var}\")\n",
    "        \n",
    "        # Get basic statistics\n",
    "        precip_data = ds[precip_var]\n",
    "        print(f\"Shape: {precip_data.shape}\")\n",
    "        print(f\"Data range: {float(precip_data.min()):.6f} to {float(precip_data.max()):.6f}\")\n",
    "        \n",
    "        # Create time series if possible\n",
    "        if 'time' in precip_data.dims:\n",
    "            ts = precip_data.mean(dim=[d for d in precip_data.dims if d != 'time'])\n",
    "            print(f\"Time series created with {len(ts)} points\")\n",
    "            \n",
    "            # Convert to pandas for easier handling\n",
    "            df = ts.to_dataframe(name='precipitation')\n",
    "            print(f\"Sample data:\\n{df.head()}\")\n",
    "            \n",
    "            return ds, df\n",
    "    else:\n",
    "        print(\"No precipitation variable found in dataset\")\n",
    "        print(\"Available variables:\", list(ds.variables.keys()))\n",
    "    \n",
    "    return ds, None\n",
    "\n",
    "def process_with_pygrib(grib_file):\n",
    "    \"\"\"Process GRIB file using pygrib library\"\"\"\n",
    "    try:\n",
    "        import pygrib\n",
    "        \n",
    "        print(f\"Opening {grib_file} with pygrib...\")\n",
    "        grbs = pygrib.open(grib_file)\n",
    "        \n",
    "        print(f\"Total messages in file: {grbs.messages}\")\n",
    "        \n",
    "        # List all messages\n",
    "        grbs.rewind()\n",
    "        messages = []\n",
    "        for grb in grbs:\n",
    "            messages.append({\n",
    "                'name': grb.name,\n",
    "                'shortName': grb.shortName,\n",
    "                'level': grb.level,\n",
    "                'validDate': grb.validDate,\n",
    "                'shape': grb.values.shape\n",
    "            })\n",
    "        \n",
    "        print(f\"Found {len(messages)} messages\")\n",
    "        if messages:\n",
    "            print(\"Sample messages:\")\n",
    "            for i, msg in enumerate(messages[:3]):\n",
    "                print(f\"  {i+1}: {msg}\")\n",
    "        \n",
    "        # Try to extract precipitation data\n",
    "        grbs.rewind()\n",
    "        precip_messages = grbs.select(shortName='tp')  # total precipitation\n",
    "        \n",
    "        if precip_messages:\n",
    "            print(f\"Found {len(precip_messages)} precipitation messages\")\n",
    "            \n",
    "            # Extract first message as example\n",
    "            first_msg = precip_messages[0]\n",
    "            print(f\"First message: {first_msg.name}\")\n",
    "            print(f\"Valid date: {first_msg.validDate}\")\n",
    "            print(f\"Data shape: {first_msg.values.shape}\")\n",
    "            print(f\"Data range: {first_msg.values.min():.6f} to {first_msg.values.max():.6f}\")\n",
    "            \n",
    "            return precip_messages, None\n",
    "        else:\n",
    "            print(\"No precipitation messages found\")\n",
    "            return messages, None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"pygrib processing failed: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def inspect_grib_file(grib_file):\n",
    "    \"\"\"Manually inspect GRIB file properties\"\"\"\n",
    "    try:\n",
    "        file_size = os.path.getsize(grib_file)\n",
    "        print(f\"\\n=== Manual GRIB File Inspection ===\")\n",
    "        print(f\"File: {grib_file}\")\n",
    "        print(f\"Size: {file_size} bytes ({file_size/(1024*1024):.2f} MB)\")\n",
    "        \n",
    "        # Check if file is too small\n",
    "        if file_size < 1000:\n",
    "            print(\"⚠️  File is very small, likely corrupted or incomplete\")\n",
    "            \n",
    "        # Try to read first few bytes\n",
    "        with open(grib_file, 'rb') as f:\n",
    "            header = f.read(100)\n",
    "            print(f\"File header (first 20 bytes): {header[:20]}\")\n",
    "            \n",
    "            # Check for GRIB magic bytes\n",
    "            if header.startswith(b'GRIB'):\n",
    "                print(\"✓ File starts with GRIB magic bytes\")\n",
    "                \n",
    "                # Try to get GRIB edition\n",
    "                if len(header) >= 8:\n",
    "                    edition = header[7]\n",
    "                    print(f\"GRIB Edition: {edition}\")\n",
    "                    \n",
    "                    if edition not in [1, 2]:\n",
    "                        print(f\"⚠️  Unusual GRIB edition: {edition}\")\n",
    "            else:\n",
    "                print(\"✗ File does not start with GRIB magic bytes\")\n",
    "                print(\"This may not be a valid GRIB file\")\n",
    "        \n",
    "        return None, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"File inspection failed: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def suggest_alternatives():\n",
    "    \"\"\"Suggest alternative approaches\"\"\"\n",
    "    print(\"\\n=== Alternative Solutions ===\")\n",
    "    print(\"1. Update libraries:\")\n",
    "    print(\"   conda update eccodes cfgrib xarray\")\n",
    "    print(\"   # or\")\n",
    "    print(\"   pip install --upgrade eccodes cfgrib xarray pygrib\")\n",
    "    \n",
    "    print(\"\\n2. Try different data format:\")\n",
    "    print(\"   - Download as NetCDF instead of GRIB\")\n",
    "    print(\"   - Use different CDS API parameters\")\n",
    "    \n",
    "    print(\"\\n3. Use different tools:\")\n",
    "    print(\"   - CDO (Climate Data Operators)\")\n",
    "    print(\"   - NCO (NetCDF Operators)\")\n",
    "    print(\"   - GDAL for GRIB reading\")\n",
    "    \n",
    "    print(\"\\n4. Check file integrity:\")\n",
    "    print(\"   - Re-download the file\")\n",
    "    print(\"   - Check CDS API status\")\n",
    "    print(\"   - Verify your CDS API credentials\")\n",
    "\n",
    "# Run the enhanced function\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Enhanced ERA5-Land Precipitation Data Processor\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    result = download_and_process_era5_precip()\n",
    "    \n",
    "    if result is None:\n",
    "        suggest_alternatives()\n",
    "    else:\n",
    "        ds, df = result\n",
    "        print(\"\\n✓ Processing completed successfully!\")\n",
    "        \n",
    "        if df is not None:\n",
    "            print(f\"Final dataframe shape: {df.shape}\")\n",
    "        else:\n",
    "            print(\"Dataset available for further processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075e8666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3875e789",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4cca935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRIB File Diagnosis Tool\n",
      "============================================================\n",
      "Diagnosing GRIB file: london_era5land_hourly_precip.grib\n",
      "==================================================\n",
      "📏 File size: 197,688 bytes (0.19 MB)\n",
      "🔐 File readable: ✅\n",
      "🔍 First 16 bytes: b'PK\\x03\\x04\\x14\\x00\\x00\\x00\\x08\\x00eq\\xe2Z\\xa4\\x95'\n",
      "❌ File does not start with GRIB magic bytes\n",
      "   This might be a corrupted or different file format\n",
      "🧹 No index files found to clean up\n",
      "\n",
      "🔄 Attempting to read london_era5land_hourly_precip.grib with different methods...\n",
      "==================================================\n",
      "🔄 Trying xarray with cfgrib engine...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Can't create file 'london_era5land_hourly_precip.grib.5b7b6.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 274, in itervalues\n",
      "    yield self.filestream.message_from_file(file, errors=errors)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 341, in message_from_file\n",
      "    return Message.from_file(file, offset, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 105, in from_file\n",
      "    raise EOFError(\"End of file: %r\" % file)\n",
      "EOFError: End of file: <_io.BufferedReader name='london_era5land_hourly_precip.grib'>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 539, in from_indexpath_or_filestream\n",
      "    self = cls.from_fieldset(filestream, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 379, in from_fieldset\n",
      "    return cls.from_fieldset_and_iteritems(fieldset, iteritems, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 392, in from_fieldset_and_iteritems\n",
      "    for field_id, raw_field in iteritems:\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 292, in __iter__\n",
      "    for message in self.itervalues():\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 278, in itervalues\n",
      "    raise EOFError(\"No valid message found: %r\" % self.filestream.path)\n",
      "EOFError: No valid message found: 'london_era5land_hourly_precip.grib'\n",
      "Can't read index file 'london_era5land_hourly_precip.grib.5b7b6.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 548, in from_indexpath_or_filestream\n",
      "    index_mtime = os.path.getmtime(indexpath)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen genericpath>\", line 67, in getmtime\n",
      "FileNotFoundError: [WinError 2] The system cannot find the file specified: 'london_era5land_hourly_precip.grib.5b7b6.idx'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ cfgrib failed: No valid message found: 'london_era5land_hourly_precip.grib'...\n",
      "🔄 Trying cfgrib with filter_by_keys...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Can't create file 'london_era5land_hourly_precip.grib.5b7b6.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 274, in itervalues\n",
      "    yield self.filestream.message_from_file(file, errors=errors)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 341, in message_from_file\n",
      "    return Message.from_file(file, offset, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 105, in from_file\n",
      "    raise EOFError(\"End of file: %r\" % file)\n",
      "EOFError: End of file: <_io.BufferedReader name='london_era5land_hourly_precip.grib'>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 539, in from_indexpath_or_filestream\n",
      "    self = cls.from_fieldset(filestream, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 379, in from_fieldset\n",
      "    return cls.from_fieldset_and_iteritems(fieldset, iteritems, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 392, in from_fieldset_and_iteritems\n",
      "    for field_id, raw_field in iteritems:\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 292, in __iter__\n",
      "    for message in self.itervalues():\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 278, in itervalues\n",
      "    raise EOFError(\"No valid message found: %r\" % self.filestream.path)\n",
      "EOFError: No valid message found: 'london_era5land_hourly_precip.grib'\n",
      "Can't read index file 'london_era5land_hourly_precip.grib.5b7b6.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 548, in from_indexpath_or_filestream\n",
      "    index_mtime = os.path.getmtime(indexpath)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen genericpath>\", line 67, in getmtime\n",
      "FileNotFoundError: [WinError 2] The system cannot find the file specified: 'london_era5land_hourly_precip.grib.5b7b6.idx'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ cfgrib with filter failed: No valid message found: 'london_era5land_hourly_precip.grib'...\n",
      "🔄 Trying cfgrib with errors='ignore'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Can't create file 'london_era5land_hourly_precip.grib.5b7b6.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 274, in itervalues\n",
      "    yield self.filestream.message_from_file(file, errors=errors)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 341, in message_from_file\n",
      "    return Message.from_file(file, offset, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 105, in from_file\n",
      "    raise EOFError(\"End of file: %r\" % file)\n",
      "EOFError: End of file: <_io.BufferedReader name='london_era5land_hourly_precip.grib'>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 539, in from_indexpath_or_filestream\n",
      "    self = cls.from_fieldset(filestream, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 379, in from_fieldset\n",
      "    return cls.from_fieldset_and_iteritems(fieldset, iteritems, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 392, in from_fieldset_and_iteritems\n",
      "    for field_id, raw_field in iteritems:\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 292, in __iter__\n",
      "    for message in self.itervalues():\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 278, in itervalues\n",
      "    raise EOFError(\"No valid message found: %r\" % self.filestream.path)\n",
      "EOFError: No valid message found: 'london_era5land_hourly_precip.grib'\n",
      "Can't read index file 'london_era5land_hourly_precip.grib.5b7b6.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\messages.py\", line 548, in from_indexpath_or_filestream\n",
      "    index_mtime = os.path.getmtime(indexpath)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen genericpath>\", line 67, in getmtime\n",
      "FileNotFoundError: [WinError 2] The system cannot find the file specified: 'london_era5land_hourly_precip.grib.5b7b6.idx'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ cfgrib with errors='ignore' failed: No valid message found: 'london_era5land_hourly_precip.grib'...\n",
      "🔄 Trying pygrib...\n",
      "✅ pygrib found 0 messages!\n",
      "\n",
      "✅ Found 1 working method(s)!\n",
      "   - pygrib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "def diagnose_grib_file(grib_file):\n",
    "    \"\"\"Comprehensive diagnosis of GRIB file issues\"\"\"\n",
    "    print(f\"Diagnosing GRIB file: {grib_file}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(grib_file):\n",
    "        print(\"❌ File does not exist!\")\n",
    "        return False\n",
    "    \n",
    "    # Check file size\n",
    "    file_size = os.path.getsize(grib_file)\n",
    "    print(f\"📏 File size: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)\")\n",
    "    \n",
    "    if file_size == 0:\n",
    "        print(\"❌ File is empty!\")\n",
    "        return False\n",
    "    \n",
    "    # Check file permissions\n",
    "    readable = os.access(grib_file, os.R_OK)\n",
    "    print(f\"🔐 File readable: {'✅' if readable else '❌'}\")\n",
    "    \n",
    "    # Try to read first few bytes\n",
    "    try:\n",
    "        with open(grib_file, 'rb') as f:\n",
    "            first_bytes = f.read(16)\n",
    "            print(f\"🔍 First 16 bytes: {first_bytes}\")\n",
    "            \n",
    "            # Check for GRIB magic bytes\n",
    "            if first_bytes.startswith(b'GRIB'):\n",
    "                print(\"✅ File starts with GRIB magic bytes\")\n",
    "            else:\n",
    "                print(\"❌ File does not start with GRIB magic bytes\")\n",
    "                print(\"   This might be a corrupted or different file format\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading file: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Clean up any existing index files\n",
    "    cleanup_index_files(grib_file)\n",
    "    \n",
    "    return True\n",
    "\n",
    "def cleanup_index_files(grib_file):\n",
    "    \"\"\"Remove any existing cfgrib index files that might be corrupted\"\"\"\n",
    "    grib_path = Path(grib_file)\n",
    "    parent_dir = grib_path.parent\n",
    "    base_name = grib_path.name\n",
    "    \n",
    "    # Look for index files\n",
    "    index_patterns = [\n",
    "        f\"{base_name}.*.idx\",\n",
    "        f\".{base_name}.*.idx\"\n",
    "    ]\n",
    "    \n",
    "    removed_files = []\n",
    "    for pattern in index_patterns:\n",
    "        for idx_file in parent_dir.glob(pattern):\n",
    "            try:\n",
    "                idx_file.unlink()\n",
    "                removed_files.append(str(idx_file))\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not remove {idx_file}: {e}\")\n",
    "    \n",
    "    if removed_files:\n",
    "        print(f\"🧹 Cleaned up index files: {removed_files}\")\n",
    "    else:\n",
    "        print(\"🧹 No index files found to clean up\")\n",
    "\n",
    "def try_multiple_grib_readers(grib_file):\n",
    "    \"\"\"Try different methods to read the GRIB file\"\"\"\n",
    "    methods = []\n",
    "    \n",
    "    # Method 1: xarray with cfgrib (default)\n",
    "    try:\n",
    "        print(\"🔄 Trying xarray with cfgrib engine...\")\n",
    "        ds = xr.open_dataset(grib_file, engine='cfgrib')\n",
    "        print(\"✅ Success with cfgrib!\")\n",
    "        methods.append((\"cfgrib\", ds))\n",
    "        ds.close()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ cfgrib failed: {str(e)[:100]}...\")\n",
    "    \n",
    "    # Method 2: xarray with cfgrib and filter_by_keys\n",
    "    try:\n",
    "        print(\"🔄 Trying cfgrib with filter_by_keys...\")\n",
    "        ds = xr.open_dataset(grib_file, engine='cfgrib', \n",
    "                           backend_kwargs={'filter_by_keys': {'paramId': 228}})\n",
    "        print(\"✅ Success with cfgrib + filter!\")\n",
    "        methods.append((\"cfgrib_filtered\", ds))\n",
    "        ds.close()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ cfgrib with filter failed: {str(e)[:100]}...\")\n",
    "    \n",
    "    # Method 3: Try with different error handling\n",
    "    try:\n",
    "        print(\"🔄 Trying cfgrib with errors='ignore'...\")\n",
    "        ds = xr.open_dataset(grib_file, engine='cfgrib', \n",
    "                           backend_kwargs={'errors': 'ignore'})\n",
    "        print(\"✅ Success with error ignoring!\")\n",
    "        methods.append((\"cfgrib_ignore_errors\", ds))\n",
    "        ds.close()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ cfgrib with errors='ignore' failed: {str(e)[:100]}...\")\n",
    "    \n",
    "    # Method 4: Try pygrib if available\n",
    "    try:\n",
    "        import pygrib\n",
    "        print(\"🔄 Trying pygrib...\")\n",
    "        grbs = pygrib.open(grib_file)\n",
    "        messages = list(grbs)\n",
    "        print(f\"✅ pygrib found {len(messages)} messages!\")\n",
    "        methods.append((\"pygrib\", messages))\n",
    "        grbs.close()\n",
    "    except ImportError:\n",
    "        print(\"⚠️  pygrib not available\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ pygrib failed: {str(e)[:100]}...\")\n",
    "    \n",
    "    return methods\n",
    "\n",
    "def redownload_era5_data():\n",
    "    \"\"\"Function to re-download ERA5 data properly\"\"\"\n",
    "    import cdsapi\n",
    "    \n",
    "    print(\"🔄 Attempting to re-download ERA5 data...\")\n",
    "    \n",
    "    # Example request for London precipitation data\n",
    "    c = cdsapi.Client()\n",
    "    \n",
    "    request = {\n",
    "        'variable': 'total_precipitation',\n",
    "        'year': '2024',\n",
    "        'month': '01',\n",
    "        'day': ['01', '02', '03'],  # Just a few days for testing\n",
    "        'time': [\n",
    "            '00:00', '01:00', '02:00', '03:00',\n",
    "            '04:00', '05:00', '06:00', '07:00',\n",
    "            '08:00', '09:00', '10:00', '11:00',\n",
    "            '12:00', '13:00', '14:00', '15:00',\n",
    "            '16:00', '17:00', '18:00', '19:00',\n",
    "            '20:00', '21:00', '22:00', '23:00'\n",
    "        ],\n",
    "        'area': [51.7, -0.5, 51.3, 0.2],  # London area: N, W, S, E\n",
    "        'format': 'grib',\n",
    "    }\n",
    "    \n",
    "    output_file = 'london_era5land_hourly_precip_fixed.grib'\n",
    "    \n",
    "    try:\n",
    "        c.retrieve('reanalysis-era5-land', request, output_file)\n",
    "        print(f\"✅ Successfully downloaded to {output_file}\")\n",
    "        return output_file\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Download failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def main_diagnosis(grib_file='london_era5land_hourly_precip.grib'):\n",
    "    \"\"\"Main function to diagnose and fix GRIB file issues\"\"\"\n",
    "    print(\"GRIB File Diagnosis Tool\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Basic file diagnosis\n",
    "    if not diagnose_grib_file(grib_file):\n",
    "        print(\"\\n💡 Recommendations:\")\n",
    "        print(\"1. Re-download the file\")\n",
    "        print(\"2. Check your internet connection during download\")\n",
    "        print(\"3. Verify the CDS API request parameters\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Try different reading methods\n",
    "    print(f\"\\n🔄 Attempting to read {grib_file} with different methods...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    successful_methods = try_multiple_grib_readers(grib_file)\n",
    "    \n",
    "    if successful_methods:\n",
    "        print(f\"\\n✅ Found {len(successful_methods)} working method(s)!\")\n",
    "        for method_name, data in successful_methods:\n",
    "            print(f\"   - {method_name}\")\n",
    "            if hasattr(data, 'dims'):  # xarray dataset\n",
    "                print(f\"     Dimensions: {dict(data.dims)}\")\n",
    "                print(f\"     Variables: {list(data.data_vars)}\")\n",
    "    else:\n",
    "        print(\"\\n❌ No methods worked. The file appears to be corrupted.\")\n",
    "        print(\"\\n💡 Next steps:\")\n",
    "        print(\"1. Delete the current file\")\n",
    "        print(\"2. Re-download using the redownload function\")\n",
    "        print(\"3. Check your CDS API setup\")\n",
    "\n",
    "# Helper function to use a working method\n",
    "def load_grib_data(grib_file, method='cfgrib'):\n",
    "    \"\"\"Load GRIB data using the specified method\"\"\"\n",
    "    cleanup_index_files(grib_file)\n",
    "    \n",
    "    if method == 'cfgrib':\n",
    "        return xr.open_dataset(grib_file, engine='cfgrib')\n",
    "    elif method == 'cfgrib_filtered':\n",
    "        return xr.open_dataset(grib_file, engine='cfgrib', \n",
    "                             backend_kwargs={'filter_by_keys': {'paramId': 228}})\n",
    "    elif method == 'cfgrib_ignore_errors':\n",
    "        return xr.open_dataset(grib_file, engine='cfgrib', \n",
    "                             backend_kwargs={'errors': 'ignore'})\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the diagnosis\n",
    "    main_diagnosis()\n",
    "    \n",
    "    # Example of how to use after diagnosis:\n",
    "    # ds = load_grib_data('london_era5land_hourly_precip.grib', method='cfgrib_ignore_errors')\n",
    "    # print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905c149c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c1ef21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "caec42fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting GRIB ZIP extraction and processing...\n",
      "🌧️  Processing London Precipitation Data\n",
      "==================================================\n",
      "📦 Extracting ZIP file: london_era5land_hourly_precip.grib\n",
      "📋 Files in archive: ['data.grib']\n",
      "✅ Extracted to: .\n",
      "🎯 Found GRIB file: data.grib\n",
      "📊 Loading GRIB file: data.grib\n",
      "🔄 Trying cfgrib...\n",
      "✅ Success with cfgrib!\n",
      "📏 Dataset dimensions: {'time': 366, 'step': 24, 'latitude': 3, 'longitude': 4}\n",
      "📊 Variables: ['tp']\n",
      "\n",
      "📈 Data Summary:\n",
      "==============================\n",
      "🗺️  Coordinates:\n",
      "   number: 0\n",
      "   time: 2021-12-31T00:00:00.000000000 to 2022-12-31T00:00:00.000000000 (366 points)\n",
      "   step: 3600000000000 nanoseconds to 86400000000000 nanoseconds (24 points)\n",
      "   surface: 0.0\n",
      "   latitude: 51.4 to 51.6 (3 points)\n",
      "   longitude: -0.2 to 0.1 (4 points)\n",
      "   valid_time: 2021-12-31T01:00:00.000000000 to 2023-01-01T00:00:00.000000000 (8784 points)\n",
      "\n",
      "📊 Data Variables:\n",
      "   tp: ('time', 'step', 'latitude', 'longitude') - Total precipitation\n",
      "      Units: m\n",
      "\n",
      "🎉 Success! Dataset loaded successfully.\n",
      "\n",
      "To work with your data:\n",
      "1. Use ds[variable_name] to access variables\n",
      "2. Use ds.sel() or ds.isel() to select data\n",
      "3. Use ds.plot() for quick visualizations\n",
      "\n",
      "💡 Dataset stored in variable 'precipitation_data' for further use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\xarray_plugin.py:131: FutureWarning: In a future version, xarray will not decode timedelta values based on the presence of a timedelta-like units attribute by default. Instead it will rely on the presence of a timedelta64 dtype attribute, which is now xarray's default way of encoding timedelta64 values. To continue decoding timedeltas based on the presence of a timedelta-like units attribute, users will need to explicitly opt-in by passing True or CFTimedeltaCoder(decode_via_units=True) to decode_timedelta. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
      "  vars, attrs, coord_names = xr.conventions.decode_cf_variables(\n",
      "C:\\Users\\pacor\\AppData\\Local\\Temp\\ipykernel_3512\\515509864.py:93: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"📏 Dataset dimensions: {dict(ds.dims)}\")\n"
     ]
    }
   ],
   "source": [
    "# possibly working?\n",
    "\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_grib_from_zip(zip_file_path, extract_to=None):\n",
    "    \"\"\"\n",
    "    Extract GRIB file from ZIP archive downloaded from CDS API\n",
    "    \n",
    "    Parameters:\n",
    "    zip_file_path (str): Path to the ZIP file (mistakenly named .grib)\n",
    "    extract_to (str): Directory to extract to (default: same directory as ZIP)\n",
    "    \n",
    "    Returns:\n",
    "    str: Path to the extracted GRIB file\n",
    "    \"\"\"\n",
    "    \n",
    "    zip_path = Path(zip_file_path)\n",
    "    \n",
    "    if extract_to is None:\n",
    "        extract_to = zip_path.parent\n",
    "    else:\n",
    "        extract_to = Path(extract_to)\n",
    "        extract_to.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"📦 Extracting ZIP file: {zip_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            # List contents\n",
    "            file_list = zip_ref.namelist()\n",
    "            print(f\"📋 Files in archive: {file_list}\")\n",
    "            \n",
    "            # Extract all files\n",
    "            zip_ref.extractall(extract_to)\n",
    "            print(f\"✅ Extracted to: {extract_to}\")\n",
    "            \n",
    "            # Find the GRIB file\n",
    "            grib_files = []\n",
    "            for filename in file_list:\n",
    "                extracted_path = extract_to / filename\n",
    "                if extracted_path.exists():\n",
    "                    # Check if it's a GRIB file by reading first few bytes\n",
    "                    with open(extracted_path, 'rb') as f:\n",
    "                        first_bytes = f.read(4)\n",
    "                        if first_bytes == b'GRIB':\n",
    "                            grib_files.append(str(extracted_path))\n",
    "                            print(f\"🎯 Found GRIB file: {filename}\")\n",
    "            \n",
    "            if grib_files:\n",
    "                return grib_files[0]  # Return the first GRIB file found\n",
    "            else:\n",
    "                # If no GRIB magic bytes found, return the first file (might still be GRIB)\n",
    "                first_file = str(extract_to / file_list[0])\n",
    "                print(f\"⚠️  No GRIB magic bytes found, returning first file: {first_file}\")\n",
    "                return first_file\n",
    "                \n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"❌ Error: {zip_file_path} is not a valid ZIP file\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error extracting ZIP file: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_extracted_grib(grib_file_path):\n",
    "    \"\"\"\n",
    "    Load the extracted GRIB file using xarray\n",
    "    \n",
    "    Parameters:\n",
    "    grib_file_path (str): Path to the extracted GRIB file\n",
    "    \n",
    "    Returns:\n",
    "    xarray.Dataset: The loaded dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"📊 Loading GRIB file: {grib_file_path}\")\n",
    "    \n",
    "    # Try different methods to load the GRIB file\n",
    "    methods = [\n",
    "        ('cfgrib', {}),\n",
    "        ('cfgrib with error handling', {'errors': 'ignore'}),\n",
    "        ('cfgrib with filter', {'filter_by_keys': {'paramId': 228}}),\n",
    "    ]\n",
    "    \n",
    "    for method_name, backend_kwargs in methods:\n",
    "        try:\n",
    "            print(f\"🔄 Trying {method_name}...\")\n",
    "            ds = xr.open_dataset(grib_file_path, engine='cfgrib', backend_kwargs=backend_kwargs)\n",
    "            print(f\"✅ Success with {method_name}!\")\n",
    "            print(f\"📏 Dataset dimensions: {dict(ds.dims)}\")\n",
    "            print(f\"📊 Variables: {list(ds.data_vars)}\")\n",
    "            return ds\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {method_name} failed: {str(e)[:100]}...\")\n",
    "            continue\n",
    "    \n",
    "    print(\"❌ All methods failed to load the GRIB file\")\n",
    "    return None\n",
    "\n",
    "def process_london_precipitation_data(zip_file_path):\n",
    "    \"\"\"\n",
    "    Complete workflow to extract and process London precipitation data\n",
    "    \n",
    "    Parameters:\n",
    "    zip_file_path (str): Path to the ZIP file downloaded from CDS\n",
    "    \n",
    "    Returns:\n",
    "    xarray.Dataset: Processed precipitation dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🌧️  Processing London Precipitation Data\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Extract the GRIB file from ZIP\n",
    "    grib_file_path = extract_grib_from_zip(zip_file_path)\n",
    "    \n",
    "    if grib_file_path is None:\n",
    "        print(\"❌ Failed to extract GRIB file\")\n",
    "        return None\n",
    "    \n",
    "    # Step 2: Load the GRIB file\n",
    "    ds = load_extracted_grib(grib_file_path)\n",
    "    \n",
    "    if ds is None:\n",
    "        print(\"❌ Failed to load GRIB file\")\n",
    "        return None\n",
    "    \n",
    "    # Step 3: Basic data exploration\n",
    "    print(\"\\n📈 Data Summary:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Print coordinate information\n",
    "    print(\"🗺️  Coordinates:\")\n",
    "    for coord in ds.coords:\n",
    "        coord_data = ds.coords[coord]\n",
    "        if coord_data.size > 1:\n",
    "            print(f\"   {coord}: {coord_data.min().values} to {coord_data.max().values} ({coord_data.size} points)\")\n",
    "        else:\n",
    "            print(f\"   {coord}: {coord_data.values}\")\n",
    "    \n",
    "    # Print variable information\n",
    "    print(\"\\n📊 Data Variables:\")\n",
    "    for var in ds.data_vars:\n",
    "        var_data = ds[var]\n",
    "        print(f\"   {var}: {var_data.dims} - {var_data.long_name if 'long_name' in var_data.attrs else 'No description'}\")\n",
    "        if hasattr(var_data, 'units'):\n",
    "            print(f\"      Units: {var_data.units}\")\n",
    "    \n",
    "    return ds\n",
    "\n",
    "def quick_visualization(ds, variable_name=None):\n",
    "    \"\"\"\n",
    "    Create a quick visualization of the precipitation data\n",
    "    \n",
    "    Parameters:\n",
    "    ds (xarray.Dataset): The precipitation dataset\n",
    "    variable_name (str): Name of the variable to plot (auto-detect if None)\n",
    "    \"\"\"\n",
    "    \n",
    "    if variable_name is None:\n",
    "        # Auto-detect precipitation variable\n",
    "        possible_names = ['tp', 'total_precipitation', 'precip', 'precipitation']\n",
    "        for name in possible_names:\n",
    "            if name in ds.data_vars:\n",
    "                variable_name = name\n",
    "                break\n",
    "        \n",
    "        if variable_name is None:\n",
    "            variable_name = list(ds.data_vars)[0]  # Use first variable\n",
    "    \n",
    "    print(f\"📊 Creating visualization for variable: {variable_name}\")\n",
    "    \n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        # Get the variable\n",
    "        var = ds[variable_name]\n",
    "        \n",
    "        # If time dimension exists, plot time series\n",
    "        if 'time' in var.dims:\n",
    "            # Average over spatial dimensions if they exist\n",
    "            if 'latitude' in var.dims and 'longitude' in var.dims:\n",
    "                var_avg = var.mean(dim=['latitude', 'longitude'])\n",
    "            else:\n",
    "                var_avg = var\n",
    "            \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            var_avg.plot()\n",
    "            plt.title(f'Time Series: {variable_name}')\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel(f'{variable_name} ({var.units if \"units\" in var.attrs else \"\"})')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # If spatial dimensions exist, plot map for first time step\n",
    "        elif 'latitude' in var.dims and 'longitude' in var.dims:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            var.isel(time=0).plot()\n",
    "            plt.title(f'Spatial Map: {variable_name} (First Time Step)')\n",
    "            plt.show()\n",
    "        \n",
    "        print(\"✅ Visualization created successfully!\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"⚠️  Matplotlib not available for visualization\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Visualization failed: {e}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Process the London precipitation file\n",
    "    zip_file = \"london_era5land_hourly_precip.grib\"  # This is actually a ZIP file\n",
    "    \n",
    "    print(\"🚀 Starting GRIB ZIP extraction and processing...\")\n",
    "    \n",
    "    # Extract and load the data\n",
    "    ds = process_london_precipitation_data(zip_file)\n",
    "    \n",
    "    if ds is not None:\n",
    "        print(\"\\n🎉 Success! Dataset loaded successfully.\")\n",
    "        print(\"\\nTo work with your data:\")\n",
    "        print(\"1. Use ds[variable_name] to access variables\")\n",
    "        print(\"2. Use ds.sel() or ds.isel() to select data\")\n",
    "        print(\"3. Use ds.plot() for quick visualizations\")\n",
    "        \n",
    "        # Optional: Create a quick visualization\n",
    "        # quick_visualization(ds)\n",
    "        \n",
    "        # Keep dataset open for further analysis\n",
    "        globals()['precipitation_data'] = ds\n",
    "        print(\"\\n💡 Dataset stored in variable 'precipitation_data' for further use\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\n❌ Failed to process the data. Please check the file and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80251af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xarray.core.dataset.Dataset"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4ca4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(\n",
       "    --jp-content-font-color0,\n",
       "    var(--pst-color-text-base rgba(0, 0, 0, 1))\n",
       "  );\n",
       "  --xr-font-color2: var(\n",
       "    --jp-content-font-color2,\n",
       "    var(--pst-color-text-base, rgba(0, 0, 0, 0.54))\n",
       "  );\n",
       "  --xr-font-color3: var(\n",
       "    --jp-content-font-color3,\n",
       "    var(--pst-color-text-base, rgba(0, 0, 0, 0.38))\n",
       "  );\n",
       "  --xr-border-color: var(\n",
       "    --jp-border-color2,\n",
       "    hsl(from var(--pst-color-on-background, white) h s calc(l - 10))\n",
       "  );\n",
       "  --xr-disabled-color: var(\n",
       "    --jp-layout-color3,\n",
       "    hsl(from var(--pst-color-on-background, white) h s calc(l - 40))\n",
       "  );\n",
       "  --xr-background-color: var(\n",
       "    --jp-layout-color0,\n",
       "    var(--pst-color-on-background, white)\n",
       "  );\n",
       "  --xr-background-color-row-even: var(\n",
       "    --jp-layout-color1,\n",
       "    hsl(from var(--pst-color-on-background, white) h s calc(l - 5))\n",
       "  );\n",
       "  --xr-background-color-row-odd: var(\n",
       "    --jp-layout-color2,\n",
       "    hsl(from var(--pst-color-on-background, white) h s calc(l - 15))\n",
       "  );\n",
       "}\n",
       "\n",
       "html[theme=\"dark\"],\n",
       "html[data-theme=\"dark\"],\n",
       "body[data-theme=\"dark\"],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: var(\n",
       "    --jp-content-font-color0,\n",
       "    var(--pst-color-text-base, rgba(255, 255, 255, 1))\n",
       "  );\n",
       "  --xr-font-color2: var(\n",
       "    --jp-content-font-color2,\n",
       "    var(--pst-color-text-base, rgba(255, 255, 255, 0.54))\n",
       "  );\n",
       "  --xr-font-color3: var(\n",
       "    --jp-content-font-color3,\n",
       "    var(--pst-color-text-base, rgba(255, 255, 255, 0.38))\n",
       "  );\n",
       "  --xr-border-color: var(\n",
       "    --jp-border-color2,\n",
       "    hsl(from var(--pst-color-on-background, #111111) h s calc(l + 10))\n",
       "  );\n",
       "  --xr-disabled-color: var(\n",
       "    --jp-layout-color3,\n",
       "    hsl(from var(--pst-color-on-background, #111111) h s calc(l + 40))\n",
       "  );\n",
       "  --xr-background-color: var(\n",
       "    --jp-layout-color0,\n",
       "    var(--pst-color-on-background, #111111)\n",
       "  );\n",
       "  --xr-background-color-row-even: var(\n",
       "    --jp-layout-color1,\n",
       "    hsl(from var(--pst-color-on-background, #111111) h s calc(l + 5))\n",
       "  );\n",
       "  --xr-background-color-row-odd: var(\n",
       "    --jp-layout-color2,\n",
       "    hsl(from var(--pst-color-on-background, #111111) h s calc(l + 15))\n",
       "  );\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 0 20px 0 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: inline-block;\n",
       "  opacity: 0;\n",
       "  height: 0;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "  border: 2px solid transparent !important;\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:focus + label {\n",
       "  border: 2px solid var(--xr-font-color0) !important;\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: \"►\";\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: \"▼\";\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: \"(\";\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: \")\";\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: \",\";\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  border-color: var(--xr-background-color-row-odd);\n",
       "  margin-bottom: 0;\n",
       "  padding-top: 2px;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "  border-color: var(--xr-background-color-row-even);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  border-top: 2px dotted var(--xr-background-color);\n",
       "  padding-bottom: 20px !important;\n",
       "  padding-top: 10px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in + label,\n",
       ".xr-var-data-in + label,\n",
       ".xr-index-data-in + label {\n",
       "  padding: 0 1px;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-data > pre,\n",
       ".xr-index-data > pre,\n",
       ".xr-var-data > table > tbody > tr {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked + label > .xr-icon-file-text2,\n",
       ".xr-var-data-in:checked + label > .xr-icon-database,\n",
       ".xr-index-data-in:checked + label > .xr-icon-database {\n",
       "  color: var(--xr-font-color0);\n",
       "  filter: drop-shadow(1px 1px 5px var(--xr-font-color2));\n",
       "  stroke-width: 0.8px;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt; Size: 495kB\n",
       "Dimensions:     (time: 366, step: 24, latitude: 3, longitude: 4)\n",
       "Coordinates:\n",
       "    number      int32 4B 0\n",
       "  * time        (time) datetime64[ns] 3kB 2021-12-31 2022-01-01 ... 2022-12-31\n",
       "  * step        (step) timedelta64[ns] 192B 01:00:00 ... 1 days 00:00:00\n",
       "    surface     float64 8B 0.0\n",
       "  * latitude    (latitude) float64 24B 51.6 51.5 51.4\n",
       "  * longitude   (longitude) float64 32B -0.2 -0.1 2.776e-17 0.1\n",
       "    valid_time  (time, step) datetime64[ns] 70kB 2021-12-31T01:00:00 ... 2023...\n",
       "Data variables:\n",
       "    tp          (time, step, latitude, longitude) float32 422kB ...\n",
       "Attributes:\n",
       "    GRIB_edition:            1\n",
       "    GRIB_centre:             ecmf\n",
       "    GRIB_centreDescription:  European Centre for Medium-Range Weather Forecasts\n",
       "    GRIB_subCentre:          0\n",
       "    Conventions:             CF-1.7\n",
       "    institution:             European Centre for Medium-Range Weather Forecasts\n",
       "    history:                 2025-07-02T16:08 GRIB to CDM+CF via cfgrib-0.9.1...</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-fa4e4dcb-da27-4167-9944-05215d9a5045' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-fa4e4dcb-da27-4167-9944-05215d9a5045' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>time</span>: 366</li><li><span class='xr-has-index'>step</span>: 24</li><li><span class='xr-has-index'>latitude</span>: 3</li><li><span class='xr-has-index'>longitude</span>: 4</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-4960aaaa-6e09-416d-83aa-6f53c4089685' class='xr-section-summary-in' type='checkbox'  checked><label for='section-4960aaaa-6e09-416d-83aa-6f53c4089685' class='xr-section-summary' >Coordinates: <span>(7)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>number</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>int32</div><div class='xr-var-preview xr-preview'>0</div><input id='attrs-bd8c5434-5025-43ff-8e57-0b30d887c3d8' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-bd8c5434-5025-43ff-8e57-0b30d887c3d8' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-451efe5a-31e2-4b9c-be94-2f316af11f6b' class='xr-var-data-in' type='checkbox'><label for='data-451efe5a-31e2-4b9c-be94-2f316af11f6b' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>ensemble member numerical id</dd><dt><span>units :</span></dt><dd>1</dd><dt><span>standard_name :</span></dt><dd>realization</dd></dl></div><div class='xr-var-data'><pre>array(0)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>2021-12-31 ... 2022-12-31</div><input id='attrs-9d7df30c-aa04-4e83-8080-cc20c43aa2c5' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-9d7df30c-aa04-4e83-8080-cc20c43aa2c5' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-763a5a7a-3e64-4b45-b961-40d86d993f6d' class='xr-var-data-in' type='checkbox'><label for='data-763a5a7a-3e64-4b45-b961-40d86d993f6d' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>initial time of forecast</dd><dt><span>standard_name :</span></dt><dd>forecast_reference_time</dd></dl></div><div class='xr-var-data'><pre>array([&#x27;2021-12-31T00:00:00.000000000&#x27;, &#x27;2022-01-01T00:00:00.000000000&#x27;,\n",
       "       &#x27;2022-01-02T00:00:00.000000000&#x27;, ..., &#x27;2022-12-29T00:00:00.000000000&#x27;,\n",
       "       &#x27;2022-12-30T00:00:00.000000000&#x27;, &#x27;2022-12-31T00:00:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>step</span></div><div class='xr-var-dims'>(step)</div><div class='xr-var-dtype'>timedelta64[ns]</div><div class='xr-var-preview xr-preview'>01:00:00 ... 1 days 00:00:00</div><input id='attrs-25c948d4-637c-4e53-ae53-bbb79846eb83' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-25c948d4-637c-4e53-ae53-bbb79846eb83' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-e75372bd-c39a-4586-9be3-5057e6c36635' class='xr-var-data-in' type='checkbox'><label for='data-e75372bd-c39a-4586-9be3-5057e6c36635' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>time since forecast_reference_time</dd><dt><span>standard_name :</span></dt><dd>forecast_period</dd></dl></div><div class='xr-var-data'><pre>array([ 3600000000000,  7200000000000, 10800000000000, 14400000000000,\n",
       "       18000000000000, 21600000000000, 25200000000000, 28800000000000,\n",
       "       32400000000000, 36000000000000, 39600000000000, 43200000000000,\n",
       "       46800000000000, 50400000000000, 54000000000000, 57600000000000,\n",
       "       61200000000000, 64800000000000, 68400000000000, 72000000000000,\n",
       "       75600000000000, 79200000000000, 82800000000000, 86400000000000],\n",
       "      dtype=&#x27;timedelta64[ns]&#x27;)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>surface</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>0.0</div><input id='attrs-d9ecab4c-8d4b-4be0-bbab-e02e147c92e5' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-d9ecab4c-8d4b-4be0-bbab-e02e147c92e5' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-1c48f869-cc8b-4b40-962a-fd7c67bf63df' class='xr-var-data-in' type='checkbox'><label for='data-1c48f869-cc8b-4b40-962a-fd7c67bf63df' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>original GRIB coordinate for key: level(surface)</dd><dt><span>units :</span></dt><dd>1</dd></dl></div><div class='xr-var-data'><pre>array(0.)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>latitude</span></div><div class='xr-var-dims'>(latitude)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>51.6 51.5 51.4</div><input id='attrs-bcf87e9f-52a5-4737-a2bd-67f0f454ef35' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-bcf87e9f-52a5-4737-a2bd-67f0f454ef35' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-00365fa9-f95a-427c-8c10-c243fe81a06c' class='xr-var-data-in' type='checkbox'><label for='data-00365fa9-f95a-427c-8c10-c243fe81a06c' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>degrees_north</dd><dt><span>standard_name :</span></dt><dd>latitude</dd><dt><span>long_name :</span></dt><dd>latitude</dd><dt><span>stored_direction :</span></dt><dd>decreasing</dd></dl></div><div class='xr-var-data'><pre>array([51.6, 51.5, 51.4])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>longitude</span></div><div class='xr-var-dims'>(longitude)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-0.2 -0.1 2.776e-17 0.1</div><input id='attrs-3a1252f3-5c68-4f6a-acd1-3e8b3f1ce4d0' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-3a1252f3-5c68-4f6a-acd1-3e8b3f1ce4d0' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-9fef93fe-b5ab-4c0d-8a38-a3839e17181b' class='xr-var-data-in' type='checkbox'><label for='data-9fef93fe-b5ab-4c0d-8a38-a3839e17181b' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>degrees_east</dd><dt><span>standard_name :</span></dt><dd>longitude</dd><dt><span>long_name :</span></dt><dd>longitude</dd></dl></div><div class='xr-var-data'><pre>array([-2.000000e-01, -1.000000e-01,  2.775558e-17,  1.000000e-01])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>valid_time</span></div><div class='xr-var-dims'>(time, step)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>2021-12-31T01:00:00 ... 2023-01-01</div><input id='attrs-6b1879d6-b61e-49ba-8da8-f067c27dc68b' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-6b1879d6-b61e-49ba-8da8-f067c27dc68b' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-7188fcc3-8085-4205-8c68-63c73652b8b7' class='xr-var-data-in' type='checkbox'><label for='data-7188fcc3-8085-4205-8c68-63c73652b8b7' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>standard_name :</span></dt><dd>time</dd><dt><span>long_name :</span></dt><dd>time</dd></dl></div><div class='xr-var-data'><pre>array([[&#x27;2021-12-31T01:00:00.000000000&#x27;, &#x27;2021-12-31T02:00:00.000000000&#x27;,\n",
       "        &#x27;2021-12-31T03:00:00.000000000&#x27;, ..., &#x27;2021-12-31T22:00:00.000000000&#x27;,\n",
       "        &#x27;2021-12-31T23:00:00.000000000&#x27;, &#x27;2022-01-01T00:00:00.000000000&#x27;],\n",
       "       [&#x27;2022-01-01T01:00:00.000000000&#x27;, &#x27;2022-01-01T02:00:00.000000000&#x27;,\n",
       "        &#x27;2022-01-01T03:00:00.000000000&#x27;, ..., &#x27;2022-01-01T22:00:00.000000000&#x27;,\n",
       "        &#x27;2022-01-01T23:00:00.000000000&#x27;, &#x27;2022-01-02T00:00:00.000000000&#x27;],\n",
       "       [&#x27;2022-01-02T01:00:00.000000000&#x27;, &#x27;2022-01-02T02:00:00.000000000&#x27;,\n",
       "        &#x27;2022-01-02T03:00:00.000000000&#x27;, ..., &#x27;2022-01-02T22:00:00.000000000&#x27;,\n",
       "        &#x27;2022-01-02T23:00:00.000000000&#x27;, &#x27;2022-01-03T00:00:00.000000000&#x27;],\n",
       "       ...,\n",
       "       [&#x27;2022-12-29T01:00:00.000000000&#x27;, &#x27;2022-12-29T02:00:00.000000000&#x27;,\n",
       "        &#x27;2022-12-29T03:00:00.000000000&#x27;, ..., &#x27;2022-12-29T22:00:00.000000000&#x27;,\n",
       "        &#x27;2022-12-29T23:00:00.000000000&#x27;, &#x27;2022-12-30T00:00:00.000000000&#x27;],\n",
       "       [&#x27;2022-12-30T01:00:00.000000000&#x27;, &#x27;2022-12-30T02:00:00.000000000&#x27;,\n",
       "        &#x27;2022-12-30T03:00:00.000000000&#x27;, ..., &#x27;2022-12-30T22:00:00.000000000&#x27;,\n",
       "        &#x27;2022-12-30T23:00:00.000000000&#x27;, &#x27;2022-12-31T00:00:00.000000000&#x27;],\n",
       "       [&#x27;2022-12-31T01:00:00.000000000&#x27;, &#x27;2022-12-31T02:00:00.000000000&#x27;,\n",
       "        &#x27;2022-12-31T03:00:00.000000000&#x27;, ..., &#x27;2022-12-31T22:00:00.000000000&#x27;,\n",
       "        &#x27;2022-12-31T23:00:00.000000000&#x27;, &#x27;2023-01-01T00:00:00.000000000&#x27;]],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-22516b4f-092a-48a0-8b88-f39cc0821558' class='xr-section-summary-in' type='checkbox'  checked><label for='section-22516b4f-092a-48a0-8b88-f39cc0821558' class='xr-section-summary' >Data variables: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>tp</span></div><div class='xr-var-dims'>(time, step, latitude, longitude)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-d6b5af99-7e8c-4f40-b2d6-9f460ebabb53' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-d6b5af99-7e8c-4f40-b2d6-9f460ebabb53' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-9019c8c0-a122-45e5-97d5-f0e9874a5514' class='xr-var-data-in' type='checkbox'><label for='data-9019c8c0-a122-45e5-97d5-f0e9874a5514' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>GRIB_paramId :</span></dt><dd>228</dd><dt><span>GRIB_dataType :</span></dt><dd>fc</dd><dt><span>GRIB_numberOfPoints :</span></dt><dd>12</dd><dt><span>GRIB_typeOfLevel :</span></dt><dd>surface</dd><dt><span>GRIB_stepUnits :</span></dt><dd>1</dd><dt><span>GRIB_stepType :</span></dt><dd>accum</dd><dt><span>GRIB_gridType :</span></dt><dd>regular_ll</dd><dt><span>GRIB_uvRelativeToGrid :</span></dt><dd>0</dd><dt><span>GRIB_NV :</span></dt><dd>0</dd><dt><span>GRIB_Nx :</span></dt><dd>4</dd><dt><span>GRIB_Ny :</span></dt><dd>3</dd><dt><span>GRIB_cfName :</span></dt><dd>unknown</dd><dt><span>GRIB_cfVarName :</span></dt><dd>tp</dd><dt><span>GRIB_gridDefinitionDescription :</span></dt><dd>Latitude/Longitude Grid</dd><dt><span>GRIB_iDirectionIncrementInDegrees :</span></dt><dd>0.1</dd><dt><span>GRIB_iScansNegatively :</span></dt><dd>0</dd><dt><span>GRIB_jDirectionIncrementInDegrees :</span></dt><dd>0.1</dd><dt><span>GRIB_jPointsAreConsecutive :</span></dt><dd>0</dd><dt><span>GRIB_jScansPositively :</span></dt><dd>0</dd><dt><span>GRIB_latitudeOfFirstGridPointInDegrees :</span></dt><dd>51.6</dd><dt><span>GRIB_latitudeOfLastGridPointInDegrees :</span></dt><dd>51.4</dd><dt><span>GRIB_longitudeOfFirstGridPointInDegrees :</span></dt><dd>-0.2</dd><dt><span>GRIB_longitudeOfLastGridPointInDegrees :</span></dt><dd>0.1</dd><dt><span>GRIB_missingValue :</span></dt><dd>3.4028234663852886e+38</dd><dt><span>GRIB_name :</span></dt><dd>Total precipitation</dd><dt><span>GRIB_shortName :</span></dt><dd>tp</dd><dt><span>GRIB_totalNumber :</span></dt><dd>0</dd><dt><span>GRIB_units :</span></dt><dd>m</dd><dt><span>long_name :</span></dt><dd>Total precipitation</dd><dt><span>units :</span></dt><dd>m</dd><dt><span>standard_name :</span></dt><dd>unknown</dd></dl></div><div class='xr-var-data'><pre>[105408 values with dtype=float32]</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-97dedc88-5b6f-448c-9575-549a4502bed3' class='xr-section-summary-in' type='checkbox'  ><label for='section-97dedc88-5b6f-448c-9575-549a4502bed3' class='xr-section-summary' >Indexes: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>time</div></div><div class='xr-index-preview'>PandasIndex</div><input type='checkbox' disabled/><label></label><input id='index-4ac71223-6031-45f0-bc26-a3ff44efa258' class='xr-index-data-in' type='checkbox'/><label for='index-4ac71223-6031-45f0-bc26-a3ff44efa258' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(DatetimeIndex([&#x27;2021-12-31&#x27;, &#x27;2022-01-01&#x27;, &#x27;2022-01-02&#x27;, &#x27;2022-01-03&#x27;,\n",
       "               &#x27;2022-01-04&#x27;, &#x27;2022-01-05&#x27;, &#x27;2022-01-06&#x27;, &#x27;2022-01-07&#x27;,\n",
       "               &#x27;2022-01-08&#x27;, &#x27;2022-01-09&#x27;,\n",
       "               ...\n",
       "               &#x27;2022-12-22&#x27;, &#x27;2022-12-23&#x27;, &#x27;2022-12-24&#x27;, &#x27;2022-12-25&#x27;,\n",
       "               &#x27;2022-12-26&#x27;, &#x27;2022-12-27&#x27;, &#x27;2022-12-28&#x27;, &#x27;2022-12-29&#x27;,\n",
       "               &#x27;2022-12-30&#x27;, &#x27;2022-12-31&#x27;],\n",
       "              dtype=&#x27;datetime64[ns]&#x27;, name=&#x27;time&#x27;, length=366, freq=None))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>step</div></div><div class='xr-index-preview'>PandasIndex</div><input type='checkbox' disabled/><label></label><input id='index-00b879fa-bade-413d-ae17-b4ff6f1f6f21' class='xr-index-data-in' type='checkbox'/><label for='index-00b879fa-bade-413d-ae17-b4ff6f1f6f21' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(TimedeltaIndex([&#x27;0 days 01:00:00&#x27;, &#x27;0 days 02:00:00&#x27;, &#x27;0 days 03:00:00&#x27;,\n",
       "                &#x27;0 days 04:00:00&#x27;, &#x27;0 days 05:00:00&#x27;, &#x27;0 days 06:00:00&#x27;,\n",
       "                &#x27;0 days 07:00:00&#x27;, &#x27;0 days 08:00:00&#x27;, &#x27;0 days 09:00:00&#x27;,\n",
       "                &#x27;0 days 10:00:00&#x27;, &#x27;0 days 11:00:00&#x27;, &#x27;0 days 12:00:00&#x27;,\n",
       "                &#x27;0 days 13:00:00&#x27;, &#x27;0 days 14:00:00&#x27;, &#x27;0 days 15:00:00&#x27;,\n",
       "                &#x27;0 days 16:00:00&#x27;, &#x27;0 days 17:00:00&#x27;, &#x27;0 days 18:00:00&#x27;,\n",
       "                &#x27;0 days 19:00:00&#x27;, &#x27;0 days 20:00:00&#x27;, &#x27;0 days 21:00:00&#x27;,\n",
       "                &#x27;0 days 22:00:00&#x27;, &#x27;0 days 23:00:00&#x27;, &#x27;1 days 00:00:00&#x27;],\n",
       "               dtype=&#x27;timedelta64[ns]&#x27;, name=&#x27;step&#x27;, freq=None))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>latitude</div></div><div class='xr-index-preview'>PandasIndex</div><input type='checkbox' disabled/><label></label><input id='index-b80e1d75-8f83-4376-9760-5f4b2fcacc86' class='xr-index-data-in' type='checkbox'/><label for='index-b80e1d75-8f83-4376-9760-5f4b2fcacc86' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([51.6, 51.5, 51.4], dtype=&#x27;float64&#x27;, name=&#x27;latitude&#x27;))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>longitude</div></div><div class='xr-index-preview'>PandasIndex</div><input type='checkbox' disabled/><label></label><input id='index-1b953025-ffd3-4c17-805c-9f7c3590e77a' class='xr-index-data-in' type='checkbox'/><label for='index-1b953025-ffd3-4c17-805c-9f7c3590e77a' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([-0.2, -0.09999999999999999, 2.7755575615628914e-17, 0.1], dtype=&#x27;float64&#x27;, name=&#x27;longitude&#x27;))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-e27684db-21f4-4ff7-bac1-4b6e296dd045' class='xr-section-summary-in' type='checkbox'  checked><label for='section-e27684db-21f4-4ff7-bac1-4b6e296dd045' class='xr-section-summary' >Attributes: <span>(7)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>GRIB_edition :</span></dt><dd>1</dd><dt><span>GRIB_centre :</span></dt><dd>ecmf</dd><dt><span>GRIB_centreDescription :</span></dt><dd>European Centre for Medium-Range Weather Forecasts</dd><dt><span>GRIB_subCentre :</span></dt><dd>0</dd><dt><span>Conventions :</span></dt><dd>CF-1.7</dd><dt><span>institution :</span></dt><dd>European Centre for Medium-Range Weather Forecasts</dd><dt><span>history :</span></dt><dd>2025-07-02T16:08 GRIB to CDM+CF via cfgrib-0.9.15.0/ecCodes-2.35.0 with {&quot;source&quot;: &quot;data.grib&quot;, &quot;filter_by_keys&quot;: {}, &quot;encode_cf&quot;: [&quot;parameter&quot;, &quot;time&quot;, &quot;geography&quot;, &quot;vertical&quot;]}</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset> Size: 495kB\n",
       "Dimensions:     (time: 366, step: 24, latitude: 3, longitude: 4)\n",
       "Coordinates:\n",
       "    number      int32 4B 0\n",
       "  * time        (time) datetime64[ns] 3kB 2021-12-31 2022-01-01 ... 2022-12-31\n",
       "  * step        (step) timedelta64[ns] 192B 01:00:00 ... 1 days 00:00:00\n",
       "    surface     float64 8B 0.0\n",
       "  * latitude    (latitude) float64 24B 51.6 51.5 51.4\n",
       "  * longitude   (longitude) float64 32B -0.2 -0.1 2.776e-17 0.1\n",
       "    valid_time  (time, step) datetime64[ns] 70kB 2021-12-31T01:00:00 ... 2023...\n",
       "Data variables:\n",
       "    tp          (time, step, latitude, longitude) float32 422kB ...\n",
       "Attributes:\n",
       "    GRIB_edition:            1\n",
       "    GRIB_centre:             ecmf\n",
       "    GRIB_centreDescription:  European Centre for Medium-Range Weather Forecasts\n",
       "    GRIB_subCentre:          0\n",
       "    Conventions:             CF-1.7\n",
       "    institution:             European Centre for Medium-Range Weather Forecasts\n",
       "    history:                 2025-07-02T16:08 GRIB to CDM+CF via cfgrib-0.9.1..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755d1944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "226c9a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precipitation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c7ac09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee43fd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def explore_dataset_structure(ds):\n",
    "    \"\"\"Explore the structure of your dataset to understand what we're working with\"\"\"\n",
    "    print(\"🔍 Dataset Structure Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(f\"📏 Dimensions: {dict(ds.dims)}\")\n",
    "    print(f\"📊 Variables: {list(ds.data_vars)}\")\n",
    "    print(f\"🗺️  Coordinates: {list(ds.coords)}\")\n",
    "    \n",
    "    # Show sample of each coordinate\n",
    "    print(\"\\n📍 Coordinate Details:\")\n",
    "    for coord_name, coord in ds.coords.items():\n",
    "        if coord.size <= 10:\n",
    "            print(f\"   {coord_name}: {coord.values}\")\n",
    "        else:\n",
    "            print(f\"   {coord_name}: {coord.values[0]} to {coord.values[-1]} ({coord.size} points)\")\n",
    "    \n",
    "    # Show variable details\n",
    "    print(\"\\n📊 Variable Details:\")\n",
    "    for var_name in ds.data_vars:\n",
    "        var = ds[var_name]\n",
    "        print(f\"   {var_name}:\")\n",
    "        print(f\"      Shape: {var.shape}\")\n",
    "        print(f\"      Dimensions: {var.dims}\")\n",
    "        if hasattr(var, 'units'):\n",
    "            print(f\"      Units: {var.units}\")\n",
    "        if hasattr(var, 'long_name'):\n",
    "            print(f\"      Description: {var.long_name}\")\n",
    "        print(f\"      Data range: {float(var.min())} to {float(var.max())}\")\n",
    "\n",
    "def convert_to_long_format_dataframe(ds, variables=None):\n",
    "    \"\"\"\n",
    "    Convert xarray dataset to long-format pandas DataFrame\n",
    "    Each row represents one observation with all coordinates as columns\n",
    "    \n",
    "    Parameters:\n",
    "    ds: xarray Dataset\n",
    "    variables: list of variable names to include (None = all variables)\n",
    "    \n",
    "    Returns:\n",
    "    pandas DataFrame in long format\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"📊 Converting to Long Format DataFrame...\")\n",
    "    \n",
    "    if variables is None:\n",
    "        variables = list(ds.data_vars)\n",
    "    \n",
    "    # Convert to DataFrame - this creates a long format automatically\n",
    "    df = ds.to_dataframe()\n",
    "    \n",
    "    # Reset index to make all coordinates into columns\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # Remove any NaN values if present\n",
    "    df = df.dropna()\n",
    "    \n",
    "    print(f\"✅ Created DataFrame with {len(df)} rows and {len(df.columns)} columns\")\n",
    "    print(f\"📊 Columns: {list(df.columns)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def convert_to_time_series_dataframe(ds, lat_lon_method='mean', variables=None):\n",
    "    \"\"\"\n",
    "    Convert to time series DataFrame by aggregating spatial dimensions\n",
    "    \n",
    "    Parameters:\n",
    "    ds: xarray Dataset\n",
    "    lat_lon_method: How to handle lat/lon ('mean', 'median', 'sum', or specific lat/lon values)\n",
    "    variables: list of variable names to include\n",
    "    \n",
    "    Returns:\n",
    "    pandas DataFrame with time as index\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"📈 Converting to Time Series DataFrame (spatial aggregation: {lat_lon_method})...\")\n",
    "    \n",
    "    if variables is None:\n",
    "        variables = list(ds.data_vars)\n",
    "    \n",
    "    # Check if we have spatial dimensions\n",
    "    spatial_dims = [dim for dim in ['latitude', 'longitude', 'lat', 'lon'] if dim in ds.dims]\n",
    "    \n",
    "    if spatial_dims and lat_lon_method in ['mean', 'median', 'sum']:\n",
    "        # Aggregate spatial dimensions\n",
    "        if lat_lon_method == 'mean':\n",
    "            ds_agg = ds.mean(dim=spatial_dims)\n",
    "        elif lat_lon_method == 'median':\n",
    "            ds_agg = ds.median(dim=spatial_dims)\n",
    "        elif lat_lon_method == 'sum':\n",
    "            ds_agg = ds.sum(dim=spatial_dims)\n",
    "        \n",
    "        print(f\"   Aggregated {len(spatial_dims)} spatial dimensions using {lat_lon_method}\")\n",
    "    else:\n",
    "        ds_agg = ds\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = ds_agg.to_dataframe()\n",
    "    \n",
    "    # If time is in the index, keep it there, otherwise reset index\n",
    "    if 'time' in df.index.names:\n",
    "        df = df.reset_index(level=[name for name in df.index.names if name != 'time'])\n",
    "    else:\n",
    "        df = df.reset_index()\n",
    "    \n",
    "    # Remove NaN values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    print(f\"✅ Created time series DataFrame with {len(df)} rows and {len(df.columns)} columns\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def convert_to_spatial_dataframe(ds, time_method='mean', variables=None):\n",
    "    \"\"\"\n",
    "    Convert to spatial DataFrame by aggregating time dimension\n",
    "    \n",
    "    Parameters:\n",
    "    ds: xarray Dataset\n",
    "    time_method: How to handle time ('mean', 'sum', 'max', or specific time index)\n",
    "    variables: list of variable names to include\n",
    "    \n",
    "    Returns:\n",
    "    pandas DataFrame with lat/lon as columns\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"🗺️  Converting to Spatial DataFrame (time aggregation: {time_method})...\")\n",
    "    \n",
    "    if variables is None:\n",
    "        variables = list(ds.data_vars)\n",
    "    \n",
    "    # Check if we have time dimension\n",
    "    if 'time' in ds.dims:\n",
    "        if time_method == 'mean':\n",
    "            ds_agg = ds.mean(dim='time')\n",
    "        elif time_method == 'sum':\n",
    "            ds_agg = ds.sum(dim='time')\n",
    "        elif time_method == 'max':\n",
    "            ds_agg = ds.max(dim='time')\n",
    "        elif isinstance(time_method, int):\n",
    "            ds_agg = ds.isel(time=time_method)\n",
    "        else:\n",
    "            ds_agg = ds.mean(dim='time')  # default\n",
    "        \n",
    "        print(f\"   Aggregated time dimension using {time_method}\")\n",
    "    else:\n",
    "        ds_agg = ds\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = ds_agg.to_dataframe().reset_index()\n",
    "    \n",
    "    # Remove NaN values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    print(f\"✅ Created spatial DataFrame with {len(df)} rows and {len(df.columns)} columns\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_dataframe_to_csv(df, filename, add_metadata=True):\n",
    "    \"\"\"\n",
    "    Save DataFrame to CSV with optional metadata\n",
    "    \n",
    "    Parameters:\n",
    "    df: pandas DataFrame\n",
    "    filename: output filename\n",
    "    add_metadata: whether to add metadata header\n",
    "    \"\"\"\n",
    "    \n",
    "    filepath = Path(filename)\n",
    "    \n",
    "    print(f\"💾 Saving DataFrame to: {filepath}\")\n",
    "    \n",
    "    if add_metadata:\n",
    "        # Create metadata header\n",
    "        metadata_lines = [\n",
    "            f\"# Generated from ERA5 GRIB data\",\n",
    "            f\"# Rows: {len(df)}\",\n",
    "            f\"# Columns: {len(df.columns)}\",\n",
    "            f\"# Column names: {', '.join(df.columns)}\",\n",
    "            f\"# Generated on: {pd.Timestamp.now()}\",\n",
    "            \"#\"\n",
    "        ]\n",
    "        \n",
    "        # Write metadata and data\n",
    "        with open(filepath, 'w', newline='') as f:\n",
    "            # Write metadata\n",
    "            for line in metadata_lines:\n",
    "                f.write(line + '\\n')\n",
    "            \n",
    "            # Write CSV data\n",
    "            df.to_csv(f, index=False)\n",
    "    else:\n",
    "        # Simple CSV save\n",
    "        df.to_csv(filepath, index=False)\n",
    "    \n",
    "    file_size = filepath.stat().st_size / (1024*1024)  # MB\n",
    "    print(f\"✅ Saved successfully! File size: {file_size:.2f} MB\")\n",
    "    \n",
    "    return str(filepath)\n",
    "\n",
    "def create_multiple_csv_formats(ds, base_filename=\"london_precipitation\"):\n",
    "    \"\"\"\n",
    "    Create CSV files in multiple formats for different use cases\n",
    "    \n",
    "    Parameters:\n",
    "    ds: xarray Dataset\n",
    "    base_filename: base name for output files\n",
    "    \n",
    "    Returns:\n",
    "    dict: mapping of format names to file paths\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🚀 Creating multiple CSV formats...\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    output_files = {}\n",
    "    \n",
    "    # 1. Long format (all data points)\n",
    "    try:\n",
    "        df_long = convert_to_long_format_dataframe(ds)\n",
    "        file_long = save_dataframe_to_csv(df_long, f\"{base_filename}_long_format.csv\")\n",
    "        output_files['long_format'] = file_long\n",
    "        print(f\"📊 Long format sample:\\n{df_long.head()}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Long format failed: {e}\")\n",
    "    \n",
    "    # 2. Time series (spatial average)\n",
    "    try:\n",
    "        df_time = convert_to_time_series_dataframe(ds, lat_lon_method='mean')\n",
    "        file_time = save_dataframe_to_csv(df_time, f\"{base_filename}_time_series.csv\")\n",
    "        output_files['time_series'] = file_time\n",
    "        print(f\"📈 Time series sample:\\n{df_time.head()}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Time series failed: {e}\")\n",
    "    \n",
    "    # 3. Spatial average (time averaged)\n",
    "    try:\n",
    "        df_spatial = convert_to_spatial_dataframe(ds, time_method='mean')\n",
    "        file_spatial = save_dataframe_to_csv(df_spatial, f\"{base_filename}_spatial_average.csv\")\n",
    "        output_files['spatial_average'] = file_spatial\n",
    "        print(f\"🗺️  Spatial average sample:\\n{df_spatial.head()}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Spatial average failed: {e}\")\n",
    "    \n",
    "    return output_files\n",
    "\n",
    "def quick_data_summary(df):\n",
    "    \"\"\"Generate a quick summary of the DataFrame\"\"\"\n",
    "    print(\"📊 Data Summary\")\n",
    "    print(\"=\" * 20)\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(\"\\nColumn types:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nBasic statistics:\")\n",
    "    print(df.describe())\n",
    "\n",
    "# Main function to use with your dataset\n",
    "def process_my_data(ds):\n",
    "    \"\"\"\n",
    "    Main function to process your specific dataset\n",
    "    Call this with your loaded dataset: process_my_data(ds)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🌧️  Processing Your London Precipitation Data\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # First, explore the structure\n",
    "    explore_dataset_structure(ds)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    # Create multiple formats\n",
    "    output_files = create_multiple_csv_formats(ds, \"london_era5_precipitation\")\n",
    "    \n",
    "    print(f\"\\n🎉 Processing complete! Created {len(output_files)} files:\")\n",
    "    for format_name, filepath in output_files.items():\n",
    "        print(f\"   📄 {format_name}: {filepath}\")\n",
    "    \n",
    "    # Return the long format DataFrame for immediate use\n",
    "    if 'long_format' in output_files:\n",
    "        df = pd.read_csv(output_files['long_format'], comment='#')\n",
    "        print(f\"\\n💡 Returning long format DataFrame with {len(df)} rows\")\n",
    "        return df\n",
    "    else:\n",
    "        print(\"⚠️  Returning time series DataFrame as fallback\")\n",
    "        return convert_to_time_series_dataframe(ds)\n",
    "\n",
    "# # Example usage:\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Assuming you have your dataset in variable 'ds'\n",
    "#     # df = process_my_data(ds)\n",
    "    \n",
    "#     print(\"🔧 Usage Instructions:\")\n",
    "#     print(\"1. Run: df = process_my_data(ds)\")\n",
    "#     print(\"2. This will create multiple CSV files and return a DataFrame\")\n",
    "#     print(\"3. Use the DataFrame for further analysis in pandas\")\n",
    "    \n",
    "#     # Or create specific formats:\n",
    "#     print(\"\\n🎯 For specific formats:\")\n",
    "#     print(\"- Long format: df = convert_to_long_format_dataframe(ds)\")\n",
    "#     print(\"- Time series: df = convert_to_time_series_dataframe(ds)\")\n",
    "#     print(\"- Spatial map: df = convert_to_spatial_dataframe(ds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d6aca32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌧️  Processing Your London Precipitation Data\n",
      "==================================================\n",
      "🔍 Dataset Structure Analysis\n",
      "========================================\n",
      "📏 Dimensions: {'time': 366, 'step': 24, 'latitude': 3, 'longitude': 4}\n",
      "📊 Variables: ['tp']\n",
      "🗺️  Coordinates: ['number', 'time', 'step', 'surface', 'latitude', 'longitude', 'valid_time']\n",
      "\n",
      "📍 Coordinate Details:\n",
      "   number: 0\n",
      "   time: 2021-12-31T00:00:00.000000000 to 2022-12-31T00:00:00.000000000 (366 points)\n",
      "   step: 3600000000000 nanoseconds to 86400000000000 nanoseconds (24 points)\n",
      "   surface: 0.0\n",
      "   latitude: [51.6 51.5 51.4]\n",
      "   longitude: [-2.00000000e-01 -1.00000000e-01  2.77555756e-17  1.00000000e-01]\n",
      "   valid_time: ['2021-12-31T01:00:00.000000000' '2021-12-31T02:00:00.000000000'\n",
      " '2021-12-31T03:00:00.000000000' '2021-12-31T04:00:00.000000000'\n",
      " '2021-12-31T05:00:00.000000000' '2021-12-31T06:00:00.000000000'\n",
      " '2021-12-31T07:00:00.000000000' '2021-12-31T08:00:00.000000000'\n",
      " '2021-12-31T09:00:00.000000000' '2021-12-31T10:00:00.000000000'\n",
      " '2021-12-31T11:00:00.000000000' '2021-12-31T12:00:00.000000000'\n",
      " '2021-12-31T13:00:00.000000000' '2021-12-31T14:00:00.000000000'\n",
      " '2021-12-31T15:00:00.000000000' '2021-12-31T16:00:00.000000000'\n",
      " '2021-12-31T17:00:00.000000000' '2021-12-31T18:00:00.000000000'\n",
      " '2021-12-31T19:00:00.000000000' '2021-12-31T20:00:00.000000000'\n",
      " '2021-12-31T21:00:00.000000000' '2021-12-31T22:00:00.000000000'\n",
      " '2021-12-31T23:00:00.000000000' '2022-01-01T00:00:00.000000000'] to ['2022-12-31T01:00:00.000000000' '2022-12-31T02:00:00.000000000'\n",
      " '2022-12-31T03:00:00.000000000' '2022-12-31T04:00:00.000000000'\n",
      " '2022-12-31T05:00:00.000000000' '2022-12-31T06:00:00.000000000'\n",
      " '2022-12-31T07:00:00.000000000' '2022-12-31T08:00:00.000000000'\n",
      " '2022-12-31T09:00:00.000000000' '2022-12-31T10:00:00.000000000'\n",
      " '2022-12-31T11:00:00.000000000' '2022-12-31T12:00:00.000000000'\n",
      " '2022-12-31T13:00:00.000000000' '2022-12-31T14:00:00.000000000'\n",
      " '2022-12-31T15:00:00.000000000' '2022-12-31T16:00:00.000000000'\n",
      " '2022-12-31T17:00:00.000000000' '2022-12-31T18:00:00.000000000'\n",
      " '2022-12-31T19:00:00.000000000' '2022-12-31T20:00:00.000000000'\n",
      " '2022-12-31T21:00:00.000000000' '2022-12-31T22:00:00.000000000'\n",
      " '2022-12-31T23:00:00.000000000' '2023-01-01T00:00:00.000000000'] (8784 points)\n",
      "\n",
      "📊 Variable Details:\n",
      "   tp:\n",
      "      Shape: (366, 24, 3, 4)\n",
      "      Dimensions: ('time', 'step', 'latitude', 'longitude')\n",
      "      Units: m\n",
      "      Description: Total precipitation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pacor\\AppData\\Local\\Temp\\ipykernel_3512\\2088330982.py:10: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"📏 Dimensions: {dict(ds.dims)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Data range: 0.0 to 0.0299998726695776\n",
      "\n",
      "==================================================\n",
      "🚀 Creating multiple CSV formats...\n",
      "========================================\n",
      "📊 Converting to Long Format DataFrame...\n",
      "✅ Created DataFrame with 105120 rows and 8 columns\n",
      "📊 Columns: ['time', 'step', 'latitude', 'longitude', 'number', 'surface', 'valid_time', 'tp']\n",
      "💾 Saving DataFrame to: london_era5_precipitation_long_format.csv\n",
      "✅ Saved successfully! File size: 8.38 MB\n",
      "📊 Long format sample:\n",
      "          time   step  latitude     longitude  number  surface valid_time  \\\n",
      "276 2021-12-31 1 days      51.6 -2.000000e-01       0      0.0 2022-01-01   \n",
      "277 2021-12-31 1 days      51.6 -1.000000e-01       0      0.0 2022-01-01   \n",
      "278 2021-12-31 1 days      51.6  2.775558e-17       0      0.0 2022-01-01   \n",
      "279 2021-12-31 1 days      51.6  1.000000e-01       0      0.0 2022-01-01   \n",
      "280 2021-12-31 1 days      51.5 -2.000000e-01       0      0.0 2022-01-01   \n",
      "\n",
      "           tp  \n",
      "276  0.002712  \n",
      "277  0.002751  \n",
      "278  0.002790  \n",
      "279  0.002478  \n",
      "280  0.002195  \n",
      "\n",
      "📈 Converting to Time Series DataFrame (spatial aggregation: mean)...\n",
      "   Aggregated 2 spatial dimensions using mean\n",
      "✅ Created time series DataFrame with 8760 rows and 5 columns\n",
      "💾 Saving DataFrame to: london_era5_precipitation_time_series.csv\n",
      "✅ Saved successfully! File size: 0.46 MB\n",
      "📈 Time series sample:\n",
      "                      step  number  surface          valid_time            tp\n",
      "time                                                                         \n",
      "2021-12-31 1 days 00:00:00       0      0.0 2022-01-01 00:00:00  2.154291e-03\n",
      "2022-01-01 0 days 01:00:00       0      0.0 2022-01-01 01:00:00  1.889033e-07\n",
      "2022-01-01 0 days 02:00:00       0      0.0 2022-01-01 02:00:00  1.890585e-07\n",
      "2022-01-01 0 days 03:00:00       0      0.0 2022-01-01 03:00:00  1.890585e-07\n",
      "2022-01-01 0 days 04:00:00       0      0.0 2022-01-01 04:00:00  1.890585e-07\n",
      "\n",
      "🗺️  Converting to Spatial DataFrame (time aggregation: mean)...\n",
      "   Aggregated time dimension using mean\n",
      "✅ Created spatial DataFrame with 288 rows and 6 columns\n",
      "💾 Saving DataFrame to: london_era5_precipitation_spatial_average.csv\n",
      "✅ Saved successfully! File size: 0.02 MB\n",
      "🗺️  Spatial average sample:\n",
      "             step  latitude     longitude  number  surface        tp\n",
      "0 0 days 01:00:00      51.6 -2.000000e-01       0      0.0  0.000057\n",
      "1 0 days 01:00:00      51.6 -1.000000e-01       0      0.0  0.000055\n",
      "2 0 days 01:00:00      51.6  2.775558e-17       0      0.0  0.000054\n",
      "3 0 days 01:00:00      51.6  1.000000e-01       0      0.0  0.000055\n",
      "4 0 days 01:00:00      51.5 -2.000000e-01       0      0.0  0.000055\n",
      "\n",
      "\n",
      "🎉 Processing complete! Created 3 files:\n",
      "   📄 long_format: london_era5_precipitation_long_format.csv\n",
      "   📄 time_series: london_era5_precipitation_time_series.csv\n",
      "   📄 spatial_average: london_era5_precipitation_spatial_average.csv\n",
      "\n",
      "💡 Returning long format DataFrame with 105120 rows\n"
     ]
    }
   ],
   "source": [
    "df = process_my_data(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cf02fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53ecc417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>step</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>number</th>\n",
       "      <th>surface</th>\n",
       "      <th>valid_time</th>\n",
       "      <th>tp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>1 days 00:00:00</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-2.000000e-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-01-01 00:00:00</td>\n",
       "      <td>0.002712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>1 days 00:00:00</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-1.000000e-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-01-01 00:00:00</td>\n",
       "      <td>0.002751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>1 days 00:00:00</td>\n",
       "      <td>51.6</td>\n",
       "      <td>2.775558e-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-01-01 00:00:00</td>\n",
       "      <td>0.002790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>1 days 00:00:00</td>\n",
       "      <td>51.6</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-01-01 00:00:00</td>\n",
       "      <td>0.002478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>1 days 00:00:00</td>\n",
       "      <td>51.5</td>\n",
       "      <td>-2.000000e-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-01-01 00:00:00</td>\n",
       "      <td>0.002195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105115</th>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>0 days 23:00:00</td>\n",
       "      <td>51.5</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-12-31 23:00:00</td>\n",
       "      <td>0.008670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105116</th>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>0 days 23:00:00</td>\n",
       "      <td>51.4</td>\n",
       "      <td>-2.000000e-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-12-31 23:00:00</td>\n",
       "      <td>0.009620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105117</th>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>0 days 23:00:00</td>\n",
       "      <td>51.4</td>\n",
       "      <td>-1.000000e-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-12-31 23:00:00</td>\n",
       "      <td>0.010117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105118</th>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>0 days 23:00:00</td>\n",
       "      <td>51.4</td>\n",
       "      <td>2.775558e-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-12-31 23:00:00</td>\n",
       "      <td>0.010687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105119</th>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>0 days 23:00:00</td>\n",
       "      <td>51.4</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-12-31 23:00:00</td>\n",
       "      <td>0.010271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105120 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              time             step  latitude     longitude  number  surface  \\\n",
       "0       2021-12-31  1 days 00:00:00      51.6 -2.000000e-01       0      0.0   \n",
       "1       2021-12-31  1 days 00:00:00      51.6 -1.000000e-01       0      0.0   \n",
       "2       2021-12-31  1 days 00:00:00      51.6  2.775558e-17       0      0.0   \n",
       "3       2021-12-31  1 days 00:00:00      51.6  1.000000e-01       0      0.0   \n",
       "4       2021-12-31  1 days 00:00:00      51.5 -2.000000e-01       0      0.0   \n",
       "...            ...              ...       ...           ...     ...      ...   \n",
       "105115  2022-12-31  0 days 23:00:00      51.5  1.000000e-01       0      0.0   \n",
       "105116  2022-12-31  0 days 23:00:00      51.4 -2.000000e-01       0      0.0   \n",
       "105117  2022-12-31  0 days 23:00:00      51.4 -1.000000e-01       0      0.0   \n",
       "105118  2022-12-31  0 days 23:00:00      51.4  2.775558e-17       0      0.0   \n",
       "105119  2022-12-31  0 days 23:00:00      51.4  1.000000e-01       0      0.0   \n",
       "\n",
       "                 valid_time        tp  \n",
       "0       2022-01-01 00:00:00  0.002712  \n",
       "1       2022-01-01 00:00:00  0.002751  \n",
       "2       2022-01-01 00:00:00  0.002790  \n",
       "3       2022-01-01 00:00:00  0.002478  \n",
       "4       2022-01-01 00:00:00  0.002195  \n",
       "...                     ...       ...  \n",
       "105115  2022-12-31 23:00:00  0.008670  \n",
       "105116  2022-12-31 23:00:00  0.009620  \n",
       "105117  2022-12-31 23:00:00  0.010117  \n",
       "105118  2022-12-31 23:00:00  0.010687  \n",
       "105119  2022-12-31 23:00:00  0.010271  \n",
       "\n",
       "[105120 rows x 8 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dae2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['valid_time'] == '2023-01-01 00:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47ade23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_51_01 = df[(df['latitude'] == 51.5) & (df['longitude'] == 0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7517e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_51_01_tp = (df_51_01\n",
    "               .groupby('time')['tp']\n",
    "               .sum().reset_index().sort_values('time')\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0902374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_51_01_tp['tp_mm'] = 1000* df_51_01_tp['tp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88002e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_51_01_tp.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736aba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_51_01_2023 = df_51_01.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6365ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_51_01_2023['year']  = pd.to_numeric(df_51_01_2023['valid_time'].str[:4])\n",
    "df_51_01_2023['month']  = pd.to_numeric(df_51_01_2023['valid_time'].str[5:7])\n",
    "df_51_01_2023['day']  = pd.to_numeric(df_51_01_2023['valid_time'].str[8:10])\n",
    "df_51_01_2023['hour']  =pd.to_numeric(df_51_01_2023['valid_time'].str[11:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a49ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = (df_51_01\n",
    "           .groupby('time')['tp']\n",
    "           .max()\n",
    "           ).reset_index()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a9bdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['month'] = pd.to_numeric(df_test['time'].str[5:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a05bea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['tp_mm'] = 1000 * df_test['tp']\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb2f8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_month = (df_test\n",
    " .groupby('month')['tp_mm']\n",
    " .sum()\n",
    " ).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7921166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_month['tp_mm'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bfcbc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06f71b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101e51f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce744d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9146838e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b4c057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3907a2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f0b408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7876f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee5a535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0719ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86c45a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f050c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2b12ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35c6336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66934ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_51_01_tp['year']  = pd.to_numeric(df_51_01_tp['time'].str[:4])\n",
    "df_51_01_tp['month'] = pd.to_numeric(df_51_01_tp['time'].str[5:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52181792",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_51_01_tp_2023 =  df_51_01_tp[df_51_01_tp['year'] >2022] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d716186",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_51_01_tp_2023\n",
    " .groupby('month')['tp']\n",
    " .sum()\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6892da49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_51_01[df_51_01['time'] == '2023-01-10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c41ac9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b7cdff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9e0985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416451b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d4ee88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e47ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0856d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cbb845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98662d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def download_era5_land_precipitation(years, months, area=None, output_file='era5_land_precip.nc'):\n",
    "    \"\"\"\n",
    "    Download ERA5-Land total precipitation data\n",
    "    \n",
    "    Parameters:\n",
    "    years: list of years as strings (e.g., ['2023', '2024'])\n",
    "    months: list of months as strings (e.g., ['01', '02', '03'])\n",
    "    area: [North, West, South, East] bounding box for London: [51.7, -0.5, 51.3, 0.2]\n",
    "    output_file: output NetCDF filename\n",
    "    \"\"\"\n",
    "    \n",
    "    # Option 1: Use .cdsapirc file (recommended)\n",
    "    # Option 1: Use .cdsapirc file (recommended)\n",
    "    c = cdsapi.Client()\n",
    "    \n",
    "    # Option 2: Set credentials programmatically (if .cdsapirc doesn't work)\n",
    "    # c = cdsapi.Client(url='https://cds.climate.copernicus.eu/api/v2',\n",
    "    #                   key='YOUR_UID:YOUR_API_KEY')\n",
    "    \n",
    "    # Option 2: Set credentials programmatically (if .cdsapirc doesn't work)\n",
    "    # c = cdsapi.Client(url='https://cds.climate.copernicus.eu/api/v2',\n",
    "    #                   key='YOUR_UID:YOUR_API_KEY')\n",
    "    \n",
    "    # Default to London area if not specified\n",
    "    if area is None:\n",
    "        area = [51.7, -0.5, 51.3, 0.2]  # London bounding box\n",
    "    \n",
    "    # For hourly data (original temporal resolution)\n",
    "    c.retrieve(\n",
    "        'reanalysis-era5-land',\n",
    "        {\n",
    "            'variable': 'total_precipitation',\n",
    "            'year': years,\n",
    "            'month': months,\n",
    "            'day': [\n",
    "                '01', '02', '03', '04', '05', '06',\n",
    "                '07', '08', '09', '10', '11', '12',\n",
    "                '13', '14', '15', '16', '17', '18',\n",
    "                '19', '20', '21', '22', '23', '24',\n",
    "                '25', '26', '27', '28', '29', '30', '31'\n",
    "            ],\n",
    "            'time': [\n",
    "                '00:00', '01:00', '02:00', '03:00', '04:00', '05:00',\n",
    "                '06:00', '07:00', '08:00', '09:00', '10:00', '11:00',\n",
    "                '12:00', '13:00', '14:00', '15:00', '16:00', '17:00',\n",
    "                '18:00', '19:00', '20:00', '21:00', '22:00', '23:00'\n",
    "            ],\n",
    "            'area': area,  # [North, West, South, East]\n",
    "            'format': 'netcdf',\n",
    "        },\n",
    "        output_file\n",
    "    )\n",
    "    \n",
    "    print(f\"Data downloaded to {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "def download_era5_land_daily_stats(years, months, daily_statistic='daily_sum', area=None, output_file='era5_land_daily.nc'):\n",
    "    \"\"\"\n",
    "    Download ERA5-Land daily statistics (recommended for precipitation sums)\n",
    "    \n",
    "    Parameters:\n",
    "    daily_statistic: 'daily_sum', 'daily_mean', 'daily_maximum', 'daily_minimum'\n",
    "    \"\"\"\n",
    "    \n",
    "    c = cdsapi.Client()\n",
    "    \n",
    "    if area is None:\n",
    "        area = [51.7, -0.5, 51.3, 0.2]  # London bounding box\n",
    "    \n",
    "    # For daily statistics - using the correct dataset name and parameters\n",
    "    c.retrieve(\n",
    "        'derived-era5-land-daily-statistics',\n",
    "        {\n",
    "            'variable': 'total_precipitation',\n",
    "            'daily_statistic': daily_statistic,\n",
    "            'year': years,\n",
    "            'month': months,\n",
    "            'day': [\n",
    "                '01', '02', '03', '04', '05', '06',\n",
    "                '07', '08', '09', '10', '11', '12',\n",
    "                '13', '14', '15', '16', '17', '18',\n",
    "                '19', '20', '21', '22', '23', '24',\n",
    "                '25', '26', '27', '28', '29', '30', '31'\n",
    "            ],\n",
    "            'area': area,\n",
    "            'format': 'netcdf',\n",
    "        },\n",
    "        output_file\n",
    "    )\n",
    "    \n",
    "    print(f\"Daily statistics data downloaded to {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "def process_precipitation_data(grib_file, threshold_mm=0.1):\n",
    "    \"\"\"\n",
    "    Process ERA5-Land precipitation data\n",
    "    \n",
    "    Parameters:\n",
    "    grib_file: path to downloaded GRIB file\n",
    "    threshold_mm: minimum precipitation threshold (to match gauge detection limits)\n",
    "    \"\"\"\n",
    "    \n",
    "    import os\n",
    "    \n",
    "    # Check if file exists and get info\n",
    "    if not os.path.exists(grib_file):\n",
    "        raise FileNotFoundError(f\"File not found: {grib_file}\")\n",
    "    \n",
    "    file_size = os.path.getsize(grib_file)\n",
    "    print(f\"File size: {file_size / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    # Check if file is very small (likely an error file)\n",
    "    if file_size < 1000:  # Less than 1KB suggests an error\n",
    "        try:\n",
    "            with open(grib_file, 'r') as f:\n",
    "                content = f.read()\n",
    "                print(f\"File content (first 500 chars): {content[:500]}\")\n",
    "        except:\n",
    "            print(\"Cannot read file as text - might be binary but corrupted\")\n",
    "        raise ValueError(\"Downloaded file appears to be corrupted or contains an error message\")\n",
    "    \n",
    "    # Try to load the data - GRIB files should use cfgrib\n",
    "    try:\n",
    "        ds = xr.open_dataset(grib_file, engine='cfgrib')\n",
    "        print(\"Successfully opened GRIB file with cfgrib engine\")\n",
    "    except Exception as e1:\n",
    "        print(f\"Failed with cfgrib: {e1}\")\n",
    "        # Fallback to other engines\n",
    "        try:\n",
    "            ds = xr.open_dataset(grib_file, engine='netcdf4')\n",
    "            print(\"Successfully opened with netcdf4 engine\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Failed with netcdf4: {e2}\")\n",
    "            raise ValueError(f\"Could not open GRIB file. Primary error: {e1}\")\n",
    "    \n",
    "    print(f\"Dataset variables: {list(ds.variables.keys())}\")\n",
    "    print(f\"Dataset dimensions: {ds.dims}\")\n",
    "    print(f\"Dataset coordinates: {list(ds.coords.keys())}\")\n",
    "    \n",
    "    # Get the precipitation variable (check common names)\n",
    "    precip_var = None\n",
    "    possible_names = ['tp', 'total_precipitation', 'precipitation', 'precip']\n",
    "    \n",
    "    for var_name in possible_names:\n",
    "        if var_name in ds.variables:\n",
    "            precip_var = var_name\n",
    "            break\n",
    "    \n",
    "    if precip_var is None:\n",
    "        raise ValueError(f\"Could not find precipitation variable. Available variables: {list(ds.variables.keys())}\")\n",
    "    \n",
    "    print(f\"Using precipitation variable: {precip_var}\")\n",
    "    \n",
    "    # Convert from m/hour to mm/hour (ERA5-Land precip is in meters)\n",
    "    precip_mm = ds[precip_var] * 1000  # Convert m to mm\n",
    "    \n",
    "    print(f\"Original data shape: {precip_mm.shape}\")\n",
    "    print(f\"Data range: {precip_mm.min().values:.6f} to {precip_mm.max().values:.6f} mm\")\n",
    "    \n",
    "    # Apply threshold if specified\n",
    "    if threshold_mm > 0:\n",
    "        precip_mm = precip_mm.where(precip_mm >= threshold_mm, 0)\n",
    "        print(f\"Applied {threshold_mm} mm threshold\")\n",
    "    \n",
    "    # Calculate daily, monthly, and yearly sums\n",
    "    daily_sum = precip_mm.resample(time='D').sum()\n",
    "    monthly_sum = precip_mm.resample(time='M').sum()\n",
    "    yearly_sum = precip_mm.resample(time='Y').sum()\n",
    "    \n",
    "    # Get London area average (if multiple grid points)\n",
    "    if 'latitude' in precip_mm.dims and 'longitude' in precip_mm.dims:\n",
    "        daily_avg = daily_sum.mean(dim=['latitude', 'longitude'])\n",
    "        monthly_avg = monthly_sum.mean(dim=['latitude', 'longitude'])\n",
    "        yearly_avg = yearly_sum.mean(dim=['latitude', 'longitude'])\n",
    "        print(\"Averaged over spatial dimensions\")\n",
    "    else:\n",
    "        daily_avg = daily_sum\n",
    "        monthly_avg = monthly_sum  \n",
    "        yearly_avg = yearly_sum\n",
    "        print(\"No spatial averaging needed\")\n",
    "    \n",
    "    return {\n",
    "        'daily': daily_avg,\n",
    "        'monthly': monthly_avg,\n",
    "        'yearly': yearly_avg,\n",
    "        'raw_data': precip_mm\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Start with a smaller test - just one month\n",
    "    years = ['2023']\n",
    "    months = ['01']  # Start with January only\n",
    "    \n",
    "    print(\"Downloading ERA5-Land hourly precipitation data in GRIB format...\")\n",
    "    \n",
    "    # Method 1: Download hourly data in GRIB format (more reliable)\n",
    "    hourly_file = download_era5_land_precipitation(\n",
    "        years=years, \n",
    "        months=months, \n",
    "        output_file='london_era5land_hourly_precip.grib'\n",
    "    )\n",
    "    \n",
    "    # Process the data\n",
    "    results = process_precipitation_data(hourly_file, threshold_mm=0.1)\n",
    "    \n",
    "    # Display some statistics\n",
    "    print(\"\\n=== Precipitation Statistics for London ===\")\n",
    "    print(f\"Daily mean: {results['daily'].mean().values:.2f} mm/day\")\n",
    "    print(f\"Monthly mean: {results['monthly'].mean().values:.2f} mm/month\") \n",
    "    print(f\"Yearly total: {results['yearly'].sum().values:.2f} mm/year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295ddd5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7903e054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def download_era5_land_precipitation(years, months, area=None, output_file='era5_land_precip.nc'):\n",
    "#     \"\"\"\n",
    "#     Download ERA5-Land total precipitation data\n",
    "    \n",
    "#     Parameters:\n",
    "#     years: list of years as strings (e.g., ['2023', '2024'])\n",
    "#     months: list of months as strings (e.g., ['01', '02', '03'])\n",
    "#     area: [North, West, South, East] bounding box for London: [51.7, -0.5, 51.3, 0.2]\n",
    "#     output_file: output NetCDF filename\n",
    "#     \"\"\"\n",
    "    \n",
    "#     c = cdsapi.Client()\n",
    "    \n",
    "#     # Default to London area if not specified\n",
    "#     if area is None:\n",
    "#         area = [51.7, -0.5, 51.3, 0.2]  # London bounding box\n",
    "    \n",
    "#     # For hourly data (original temporal resolution)\n",
    "#     c.retrieve(\n",
    "#         'reanalysis-era5-land',\n",
    "#         {\n",
    "#             'variable': 'total_precipitation',\n",
    "#             'year': years,\n",
    "#             'month': months,\n",
    "#             'day': [\n",
    "#                 '01', '02', '03', '04', '05', '06',\n",
    "#                 '07', '08', '09', '10', '11', '12',\n",
    "#                 '13', '14', '15', '16', '17', '18',\n",
    "#                 '19', '20', '21', '22', '23', '24',\n",
    "#                 '25', '26', '27', '28', '29', '30', '31'\n",
    "#             ],\n",
    "#             'time': [\n",
    "#                 '00:00', '01:00', '02:00', '03:00', '04:00', '05:00',\n",
    "#                 '06:00', '07:00', '08:00', '09:00', '10:00', '11:00',\n",
    "#                 '12:00', '13:00', '14:00', '15:00', '16:00', '17:00',\n",
    "#                 '18:00', '19:00', '20:00', '21:00', '22:00', '23:00'\n",
    "#             ],\n",
    "#             'area': area,  # [North, West, South, East]\n",
    "#             'format': 'netcdf',\n",
    "#         },\n",
    "#         output_file\n",
    "#     )\n",
    "    \n",
    "#     print(f\"Data downloaded to {output_file}\")\n",
    "#     return output_file\n",
    "\n",
    "# def download_era5_land_daily_stats(years, months, statistic='daily_sum', area=None, output_file='era5_land_daily.nc'):\n",
    "#     \"\"\"\n",
    "#     Download ERA5-Land daily statistics (recommended for precipitation sums)\n",
    "    \n",
    "#     Parameters:\n",
    "#     statistic: 'daily_sum', 'daily_mean', 'daily_maximum', 'daily_minimum'\n",
    "#     \"\"\"\n",
    "    \n",
    "#     c = cdsapi.Client()\n",
    "    \n",
    "#     if area is None:\n",
    "#         area = [51.7, -0.5, 51.3, 0.2]  # London bounding box\n",
    "    \n",
    "#     # For daily statistics (more efficient for precipitation sums)\n",
    "#     c.retrieve(\n",
    "#         'derived-era5-land-daily-statistics',\n",
    "#         {\n",
    "#             'variable': 'total_precipitation',\n",
    "#             'statistic': statistic,\n",
    "#             'year': years,\n",
    "#             'month': months,\n",
    "#             'day': [\n",
    "#                 '01', '02', '03', '04', '05', '06',\n",
    "#                 '07', '08', '09', '10', '11', '12',\n",
    "#                 '13', '14', '15', '16', '17', '18',\n",
    "#                 '19', '20', '21', '22', '23', '24',\n",
    "#                 '25', '26', '27', '28', '29', '30', '31'\n",
    "#             ],\n",
    "#             'area': area,\n",
    "#             'format': 'netcdf',\n",
    "#         },\n",
    "#         output_file\n",
    "#     )\n",
    "    \n",
    "#     print(f\"Daily statistics data downloaded to {output_file}\")\n",
    "#     return output_file\n",
    "\n",
    "# def process_precipitation_data(netcdf_file, threshold_mm=0.1):\n",
    "#     \"\"\"\n",
    "#     Process ERA5-Land precipitation data\n",
    "    \n",
    "#     Parameters:\n",
    "#     netcdf_file: path to downloaded NetCDF file\n",
    "#     threshold_mm: minimum precipitation threshold (to match gauge detection limits)\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Load the data\n",
    "#     ds = xr.open_dataset(netcdf_file)\n",
    "    \n",
    "#     # Convert from m/hour to mm/hour (ERA5-Land precip is in meters)\n",
    "#     precip_mm = ds['tp'] * 1000  # Convert m to mm\n",
    "    \n",
    "#     # Apply threshold if specified\n",
    "#     if threshold_mm > 0:\n",
    "#         precip_mm = precip_mm.where(precip_mm >= threshold_mm, 0)\n",
    "    \n",
    "#     # Calculate daily, monthly, and yearly sums\n",
    "#     daily_sum = precip_mm.resample(time='D').sum()\n",
    "#     monthly_sum = precip_mm.resample(time='M').sum()\n",
    "#     yearly_sum = precip_mm.resample(time='Y').sum()\n",
    "    \n",
    "#     # Get London area average (if multiple grid points)\n",
    "#     if len(precip_mm.dims) > 1:\n",
    "#         daily_avg = daily_sum.mean(dim=['latitude', 'longitude'])\n",
    "#         monthly_avg = monthly_sum.mean(dim=['latitude', 'longitude'])\n",
    "#         yearly_avg = yearly_sum.mean(dim=['latitude', 'longitude'])\n",
    "#     else:\n",
    "#         daily_avg = daily_sum\n",
    "#         monthly_avg = monthly_sum  \n",
    "#         yearly_avg = yearly_sum\n",
    "    \n",
    "#     return {\n",
    "#         'daily': daily_avg,\n",
    "#         'monthly': monthly_avg,\n",
    "#         'yearly': yearly_avg,\n",
    "#         'raw_data': precip_mm\n",
    "#     }\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Download data for London, 2023-2024\n",
    "#     years = ['2023']\n",
    "#     months = ['01', '02', '03', '04', '05', '06', \n",
    "#               '07', '08', '09', '10', '11', '12']\n",
    "    \n",
    "#     # Method 1: Download daily sums (recommended for precipitation)\n",
    "#     daily_file = download_era5_land_daily_stats(\n",
    "#         years=years, \n",
    "#         months=months, \n",
    "#         statistic='daily_sum',\n",
    "#         output_file='london_era5land_daily_precip.nc'\n",
    "#     )\n",
    "    \n",
    "#     # Method 2: Download hourly data (if you need hourly resolution)\n",
    "#     # hourly_file = download_era5_land_precipitation(\n",
    "#     #     years=['2023'], \n",
    "#     #     months=['01', '02'], \n",
    "#     #     output_file='london_era5land_hourly_precip.nc'\n",
    "#     # )\n",
    "    \n",
    "#     # Process the data\n",
    "#     results = process_precipitation_data(daily_file, threshold_mm=0.1)\n",
    "    \n",
    "#     # Display some statistics\n",
    "#     print(\"\\n=== Precipitation Statistics for London ===\")\n",
    "#     print(f\"Daily mean: {results['daily'].mean().values:.2f} mm/day\")\n",
    "#     print(f\"Monthly mean: {results['monthly'].mean().values:.2f} mm/month\") \n",
    "#     print(f\"Yearly total: {results['yearly'].sum().values:.2f} mm/year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d51206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cdsapi\n",
    "# import xarray as xr\n",
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# def download_era5_land_precipitation(years, months, area=None, output_file='era5_land_precip.nc'):\n",
    "#     \"\"\"\n",
    "#     Download ERA5-Land total precipitation data\n",
    "    \n",
    "#     Parameters:\n",
    "#     years: list of years as strings (e.g., ['2023', '2024'])\n",
    "#     months: list of months as strings (e.g., ['01', '02', '03'])\n",
    "#     area: [North, West, South, East] bounding box for London: [51.7, -0.5, 51.3, 0.2]\n",
    "#     output_file: output NetCDF filename\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Option 1: Use .cdsapirc file (recommended)\n",
    "#     # Option 1: Use .cdsapirc file (recommended)\n",
    "#     c = cdsapi.Client()\n",
    "    \n",
    "#     # Option 2: Set credentials programmatically (if .cdsapirc doesn't work)\n",
    "#     # c = cdsapi.Client(url='https://cds.climate.copernicus.eu/api/v2',\n",
    "#     #                   key='YOUR_UID:YOUR_API_KEY')\n",
    "    \n",
    "#     # Option 2: Set credentials programmatically (if .cdsapirc doesn't work)\n",
    "#     # c = cdsapi.Client(url='https://cds.climate.copernicus.eu/api/v2',\n",
    "#     #                   key='YOUR_UID:YOUR_API_KEY')\n",
    "    \n",
    "#     # Default to London area if not specified\n",
    "#     if area is None:\n",
    "#         area = [51.7, -0.5, 51.3, 0.2]  # London bounding box\n",
    "    \n",
    "#     # For hourly data (original temporal resolution)\n",
    "#     c.retrieve(\n",
    "#         'reanalysis-era5-land',\n",
    "#         {\n",
    "#             'variable': 'total_precipitation',\n",
    "#             'year': years,\n",
    "#             'month': months,\n",
    "#             'day': [\n",
    "#                 '01', '02', '03', '04', '05', '06',\n",
    "#                 '07', '08', '09', '10', '11', '12',\n",
    "#                 '13', '14', '15', '16', '17', '18',\n",
    "#                 '19', '20', '21', '22', '23', '24',\n",
    "#                 '25', '26', '27', '28', '29', '30', '31'\n",
    "#             ],\n",
    "#             'time': [\n",
    "#                 '00:00', '01:00', '02:00', '03:00', '04:00', '05:00',\n",
    "#                 '06:00', '07:00', '08:00', '09:00', '10:00', '11:00',\n",
    "#                 '12:00', '13:00', '14:00', '15:00', '16:00', '17:00',\n",
    "#                 '18:00', '19:00', '20:00', '21:00', '22:00', '23:00'\n",
    "#             ],\n",
    "#             'area': area,  # [North, West, South, East]\n",
    "#             'format': 'netcdf',\n",
    "#         },\n",
    "#         output_file\n",
    "#     )\n",
    "    \n",
    "#     print(f\"Data downloaded to {output_file}\")\n",
    "#     return output_file\n",
    "\n",
    "# def download_era5_land_daily_stats(years, months, daily_statistic='daily_sum', area=None, output_file='era5_land_daily.nc'):\n",
    "#     \"\"\"\n",
    "#     Download ERA5-Land daily statistics (recommended for precipitation sums)\n",
    "    \n",
    "#     Parameters:\n",
    "#     daily_statistic: 'daily_sum', 'daily_mean', 'daily_maximum', 'daily_minimum'\n",
    "#     \"\"\"\n",
    "    \n",
    "#     c = cdsapi.Client()\n",
    "    \n",
    "#     if area is None:\n",
    "#         area = [51.7, -0.5, 51.3, 0.2]  # London bounding box\n",
    "    \n",
    "#     # For daily statistics - using the correct dataset name and parameters\n",
    "#     c.retrieve(\n",
    "#         'derived-era5-land-daily-statistics',\n",
    "#         {\n",
    "#             'variable': 'total_precipitation',\n",
    "#             'daily_statistic': daily_statistic,\n",
    "#             'year': years,\n",
    "#             'month': months,\n",
    "#             'day': [\n",
    "#                 '01', '02', '03', '04', '05', '06',\n",
    "#                 '07', '08', '09', '10', '11', '12',\n",
    "#                 '13', '14', '15', '16', '17', '18',\n",
    "#                 '19', '20', '21', '22', '23', '24',\n",
    "#                 '25', '26', '27', '28', '29', '30', '31'\n",
    "#             ],\n",
    "#             'area': area,\n",
    "#             'format': 'netcdf',\n",
    "#         },\n",
    "#         output_file\n",
    "#     )\n",
    "    \n",
    "#     print(f\"Daily statistics data downloaded to {output_file}\")\n",
    "#     return output_file\n",
    "\n",
    "# def process_precipitation_data(netcdf_file, threshold_mm=0.1):\n",
    "#     \"\"\"\n",
    "#     Process ERA5-Land precipitation data\n",
    "    \n",
    "#     Parameters:\n",
    "#     netcdf_file: path to downloaded NetCDF file\n",
    "#     threshold_mm: minimum precipitation threshold (to match gauge detection limits)\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Load the data\n",
    "#     ds = xr.open_dataset(netcdf_file)\n",
    "    \n",
    "#     # Convert from m/hour to mm/hour (ERA5-Land precip is in meters)\n",
    "#     precip_mm = ds['tp'] * 1000  # Convert m to mm\n",
    "    \n",
    "#     # Apply threshold if specified\n",
    "#     if threshold_mm > 0:\n",
    "#         precip_mm = precip_mm.where(precip_mm >= threshold_mm, 0)\n",
    "    \n",
    "#     # Calculate daily, monthly, and yearly sums\n",
    "#     daily_sum = precip_mm.resample(time='D').sum()\n",
    "#     monthly_sum = precip_mm.resample(time='M').sum()\n",
    "#     yearly_sum = precip_mm.resample(time='Y').sum()\n",
    "    \n",
    "#     # Get London area average (if multiple grid points)\n",
    "#     if len(precip_mm.dims) > 1:\n",
    "#         daily_avg = daily_sum.mean(dim=['latitude', 'longitude'])\n",
    "#         monthly_avg = monthly_sum.mean(dim=['latitude', 'longitude'])\n",
    "#         yearly_avg = yearly_sum.mean(dim=['latitude', 'longitude'])\n",
    "#     else:\n",
    "#         daily_avg = daily_sum\n",
    "#         monthly_avg = monthly_sum  \n",
    "#         yearly_avg = yearly_sum\n",
    "    \n",
    "#     return {\n",
    "#         'daily': daily_avg,\n",
    "#         'monthly': monthly_avg,\n",
    "#         'yearly': yearly_avg,\n",
    "#         'raw_data': precip_mm\n",
    "#     }\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Download data for London, 2023-2024\n",
    "#     years = ['2023']\n",
    "#     months = ['01', '02', '03', '04', '05', '06', \n",
    "#               '07', '08', '09', '10', '11', '12']\n",
    "    \n",
    "#     # Method 1: Download daily sums (recommended for precipitation)\n",
    "#     daily_file = download_era5_land_daily_stats(\n",
    "#         years=years, \n",
    "#         months=months, \n",
    "#         daily_statistic='daily_sum',\n",
    "#         output_file='london_era5land_daily_precip.nc'\n",
    "#     )\n",
    "    \n",
    "#     # Method 2: Download hourly data (if you need hourly resolution)\n",
    "#     # hourly_file = download_era5_land_precipitation(\n",
    "#     #     years=['2023'], \n",
    "#     #     months=['01', '02'], \n",
    "#     #     output_file='london_era5land_hourly_precip.nc'\n",
    "#     # )\n",
    "    \n",
    "#     # Process the data\n",
    "#     results = process_precipitation_data(daily_file, threshold_mm=0.1)\n",
    "    \n",
    "#     # Display some statistics\n",
    "#     print(\"\\n=== Precipitation Statistics for London ===\")\n",
    "#     print(f\"Daily mean: {results['daily'].mean().values:.2f} mm/day\")\n",
    "#     print(f\"Monthly mean: {results['monthly'].mean().values:.2f} mm/month\") \n",
    "#     print(f\"Yearly total: {results['yearly'].sum().values:.2f} mm/year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5065152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def download_era5_land_precipitation(years, months, area=None, output_file='era5_land_precip.nc'):\n",
    "    \"\"\"\n",
    "    Download ERA5-Land total precipitation data\n",
    "    \n",
    "    Parameters:\n",
    "    years: list of years as strings (e.g., ['2023', '2024'])\n",
    "    months: list of months as strings (e.g., ['01', '02', '03'])\n",
    "    area: [North, West, South, East] bounding box for London: [51.7, -0.5, 51.3, 0.2]\n",
    "    output_file: output NetCDF filename\n",
    "    \"\"\"\n",
    "    \n",
    "    # Option 1: Use .cdsapirc file (recommended)\n",
    "    # Option 1: Use .cdsapirc file (recommended)\n",
    "    c = cdsapi.Client()\n",
    "    \n",
    "    # Option 2: Set credentials programmatically (if .cdsapirc doesn't work)\n",
    "    # c = cdsapi.Client(url='https://cds.climate.copernicus.eu/api/v2',\n",
    "    #                   key='YOUR_UID:YOUR_API_KEY')\n",
    "    \n",
    "    # Option 2: Set credentials programmatically (if .cdsapirc doesn't work)\n",
    "    # c = cdsapi.Client(url='https://cds.climate.copernicus.eu/api/v2',\n",
    "    #                   key='YOUR_UID:YOUR_API_KEY')\n",
    "    \n",
    "    # Default to London area if not specified\n",
    "    if area is None:\n",
    "        area = [51.7, -0.5, 51.3, 0.2]  # London bounding box\n",
    "    \n",
    "    # For hourly data (original temporal resolution)\n",
    "    c.retrieve(\n",
    "        'reanalysis-era5-land',\n",
    "        {\n",
    "            'variable': 'total_precipitation',\n",
    "            'year': years,\n",
    "            'month': months,\n",
    "            'day': [\n",
    "                '01', '02', '03', '04', '05', '06',\n",
    "                '07', '08', '09', '10', '11', '12',\n",
    "                '13', '14', '15', '16', '17', '18',\n",
    "                '19', '20', '21', '22', '23', '24',\n",
    "                '25', '26', '27', '28', '29', '30', '31'\n",
    "            ],\n",
    "            'time': [\n",
    "                '00:00', '01:00', '02:00', '03:00', '04:00', '05:00',\n",
    "                '06:00', '07:00', '08:00', '09:00', '10:00', '11:00',\n",
    "                '12:00', '13:00', '14:00', '15:00', '16:00', '17:00',\n",
    "                '18:00', '19:00', '20:00', '21:00', '22:00', '23:00'\n",
    "            ],\n",
    "            'area': area,  # [North, West, South, East]\n",
    "            'format': 'netcdf',\n",
    "        },\n",
    "        output_file\n",
    "    )\n",
    "    \n",
    "    print(f\"Data downloaded to {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "def download_era5_land_daily_stats(years, months, daily_statistic='daily_sum', area=None, output_file='era5_land_daily.nc'):\n",
    "    \"\"\"\n",
    "    Download ERA5-Land daily statistics (recommended for precipitation sums)\n",
    "    \n",
    "    Parameters:\n",
    "    daily_statistic: 'daily_sum', 'daily_mean', 'daily_maximum', 'daily_minimum'\n",
    "    \"\"\"\n",
    "    \n",
    "    c = cdsapi.Client()\n",
    "    \n",
    "    if area is None:\n",
    "        area = [51.7, -0.5, 51.3, 0.2]  # London bounding box\n",
    "    \n",
    "    # For daily statistics - using the correct dataset name and parameters\n",
    "    c.retrieve(\n",
    "        'derived-era5-land-daily-statistics',\n",
    "        {\n",
    "            'variable': 'total_precipitation',\n",
    "            'daily_statistic': daily_statistic,\n",
    "            'year': years,\n",
    "            'month': months,\n",
    "            'day': [\n",
    "                '01', '02', '03', '04', '05', '06',\n",
    "                '07', '08', '09', '10', '11', '12',\n",
    "                '13', '14', '15', '16', '17', '18',\n",
    "                '19', '20', '21', '22', '23', '24',\n",
    "                '25', '26', '27', '28', '29', '30', '31'\n",
    "            ],\n",
    "            'area': area,\n",
    "            'format': 'netcdf',\n",
    "        },\n",
    "        output_file\n",
    "    )\n",
    "    \n",
    "    print(f\"Daily statistics data downloaded to {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "def process_precipitation_data(netcdf_file, threshold_mm=0.1):\n",
    "    \"\"\"\n",
    "    Process ERA5-Land precipitation data\n",
    "    \n",
    "    Parameters:\n",
    "    netcdf_file: path to downloaded NetCDF file\n",
    "    threshold_mm: minimum precipitation threshold (to match gauge detection limits)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the data\n",
    "    ds = xr.open_dataset(netcdf_file)\n",
    "    \n",
    "    # Convert from m/hour to mm/hour (ERA5-Land precip is in meters)\n",
    "    precip_mm = ds['tp'] * 1000  # Convert m to mm\n",
    "    \n",
    "    # Apply threshold if specified\n",
    "    if threshold_mm > 0:\n",
    "        precip_mm = precip_mm.where(precip_mm >= threshold_mm, 0)\n",
    "    \n",
    "    # Calculate daily, monthly, and yearly sums\n",
    "    daily_sum = precip_mm.resample(time='D').sum()\n",
    "    monthly_sum = precip_mm.resample(time='M').sum()\n",
    "    yearly_sum = precip_mm.resample(time='Y').sum()\n",
    "    \n",
    "    # Get London area average (if multiple grid points)\n",
    "    if len(precip_mm.dims) > 1:\n",
    "        daily_avg = daily_sum.mean(dim=['latitude', 'longitude'])\n",
    "        monthly_avg = monthly_sum.mean(dim=['latitude', 'longitude'])\n",
    "        yearly_avg = yearly_sum.mean(dim=['latitude', 'longitude'])\n",
    "    else:\n",
    "        daily_avg = daily_sum\n",
    "        monthly_avg = monthly_sum  \n",
    "        yearly_avg = yearly_sum\n",
    "    \n",
    "    return {\n",
    "        'daily': daily_avg,\n",
    "        'monthly': monthly_avg,\n",
    "        'yearly': yearly_avg,\n",
    "        'raw_data': precip_mm\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Start with a smaller test - just one month\n",
    "    years = ['2023']\n",
    "    months = ['01']  # Start with January only\n",
    "    \n",
    "    print(\"Downloading ERA5-Land hourly precipitation data...\")\n",
    "    \n",
    "    # Method 1: Download hourly data (more reliable)\n",
    "    hourly_file = download_era5_land_precipitation(\n",
    "        years=years, \n",
    "        months=months, \n",
    "        output_file='london_era5land_hourly_precip.nc'\n",
    "    )\n",
    "    \n",
    "    # Method 2: Try daily statistics (if hourly works)\n",
    "    # daily_file = download_era5_land_daily_stats(\n",
    "    #     years=years, \n",
    "    #     months=months, \n",
    "    #     daily_statistic='daily_sum',\n",
    "    #     output_file='london_era5land_daily_precip.nc'\n",
    "    # )\n",
    "    \n",
    "    # Process the data\n",
    "    results = process_precipitation_data(hourly_file, threshold_mm=0.1)\n",
    "    \n",
    "    # Display some statistics\n",
    "    print(\"\\n=== Precipitation Statistics for London ===\")\n",
    "    print(f\"Daily mean: {results['daily'].mean().values:.2f} mm/day\")\n",
    "    print(f\"Monthly mean: {results['monthly'].mean().values:.2f} mm/month\") \n",
    "    print(f\"Yearly total: {results['yearly'].sum().values:.2f} mm/year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0260a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def download_era5_land_precipitation(years, months, area=None, output_file='era5_land_precip.nc'):\n",
    "    \"\"\"\n",
    "    Download ERA5-Land total precipitation data\n",
    "    \n",
    "    Parameters:\n",
    "    years: list of years as strings (e.g., ['2023', '2024'])\n",
    "    months: list of months as strings (e.g., ['01', '02', '03'])\n",
    "    area: [North, West, South, East] bounding box for London: [51.7, -0.5, 51.3, 0.2]\n",
    "    output_file: output NetCDF filename\n",
    "    \"\"\"\n",
    "    \n",
    "    # Option 1: Use .cdsapirc file (recommended)\n",
    "    # Option 1: Use .cdsapirc file (recommended)\n",
    "    c = cdsapi.Client()\n",
    "    \n",
    "    # Option 2: Set credentials programmatically (if .cdsapirc doesn't work)\n",
    "    # c = cdsapi.Client(url='https://cds.climate.copernicus.eu/api/v2',\n",
    "    #                   key='YOUR_UID:YOUR_API_KEY')\n",
    "    \n",
    "    # Option 2: Set credentials programmatically (if .cdsapirc doesn't work)\n",
    "    # c = cdsapi.Client(url='https://cds.climate.copernicus.eu/api/v2',\n",
    "    #                   key='YOUR_UID:YOUR_API_KEY')\n",
    "    \n",
    "    # Default to London area if not specified\n",
    "    if area is None:\n",
    "        area = [51.7, -0.5, 51.3, 0.2]  # London bounding box\n",
    "    \n",
    "    # For hourly data (original temporal resolution)\n",
    "    c.retrieve(\n",
    "        'reanalysis-era5-land',\n",
    "        {\n",
    "            'variable': 'total_precipitation',\n",
    "            'year': years,\n",
    "            'month': months,\n",
    "            'day': [\n",
    "                '01', '02', '03', '04', '05', '06',\n",
    "                '07', '08', '09', '10', '11', '12',\n",
    "                '13', '14', '15', '16', '17', '18',\n",
    "                '19', '20', '21', '22', '23', '24',\n",
    "                '25', '26', '27', '28', '29', '30', '31'\n",
    "            ],\n",
    "            'time': [\n",
    "                '00:00', '01:00', '02:00', '03:00', '04:00', '05:00',\n",
    "                '06:00', '07:00', '08:00', '09:00', '10:00', '11:00',\n",
    "                '12:00', '13:00', '14:00', '15:00', '16:00', '17:00',\n",
    "                '18:00', '19:00', '20:00', '21:00', '22:00', '23:00'\n",
    "            ],\n",
    "            'area': area,  # [North, West, South, East]\n",
    "            'format': 'netcdf',\n",
    "        },\n",
    "        output_file\n",
    "    )\n",
    "    \n",
    "    print(f\"Data downloaded to {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "def download_era5_land_daily_stats(years, months, daily_statistic='daily_sum', area=None, output_file='era5_land_daily.nc'):\n",
    "    \"\"\"\n",
    "    Download ERA5-Land daily statistics (recommended for precipitation sums)\n",
    "    \n",
    "    Parameters:\n",
    "    daily_statistic: 'daily_sum', 'daily_mean', 'daily_maximum', 'daily_minimum'\n",
    "    \"\"\"\n",
    "    \n",
    "    c = cdsapi.Client()\n",
    "    \n",
    "    if area is None:\n",
    "        area = [51.7, -0.5, 51.3, 0.2]  # London bounding box\n",
    "    \n",
    "    # For daily statistics - using the correct dataset name and parameters\n",
    "    c.retrieve(\n",
    "        'derived-era5-land-daily-statistics',\n",
    "        {\n",
    "            'variable': 'total_precipitation',\n",
    "            'daily_statistic': daily_statistic,\n",
    "            'year': years,\n",
    "            'month': months,\n",
    "            'day': [\n",
    "                '01', '02', '03', '04', '05', '06',\n",
    "                '07', '08', '09', '10', '11', '12',\n",
    "                '13', '14', '15', '16', '17', '18',\n",
    "                '19', '20', '21', '22', '23', '24',\n",
    "                '25', '26', '27', '28', '29', '30', '31'\n",
    "            ],\n",
    "            'area': area,\n",
    "            'format': 'netcdf',\n",
    "        },\n",
    "        output_file\n",
    "    )\n",
    "    \n",
    "    print(f\"Daily statistics data downloaded to {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "def process_precipitation_data(netcdf_file, threshold_mm=0.1):\n",
    "    \"\"\"\n",
    "    Process ERA5-Land precipitation data\n",
    "    \n",
    "    Parameters:\n",
    "    netcdf_file: path to downloaded NetCDF file\n",
    "    threshold_mm: minimum precipitation threshold (to match gauge detection limits)\n",
    "    \"\"\"\n",
    "    \n",
    "    import os\n",
    "    \n",
    "    # Check if file exists and get info\n",
    "    if not os.path.exists(netcdf_file):\n",
    "        raise FileNotFoundError(f\"File not found: {netcdf_file}\")\n",
    "    \n",
    "    file_size = os.path.getsize(netcdf_file)\n",
    "    print(f\"File size: {file_size / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    # Check if file is very small (likely an error file)\n",
    "    if file_size < 1000:  # Less than 1KB suggests an error\n",
    "        with open(netcdf_file, 'r') as f:\n",
    "            content = f.read()\n",
    "            print(f\"File content (first 500 chars): {content[:500]}\")\n",
    "        raise ValueError(\"Downloaded file appears to be corrupted or contains an error message\")\n",
    "    \n",
    "    # Try to load the data with explicit engine\n",
    "    try:\n",
    "        ds = xr.open_dataset(netcdf_file, engine='netcdf4')\n",
    "        print(\"Successfully opened with netcdf4 engine\")\n",
    "    except Exception as e1:\n",
    "        print(f\"Failed with netcdf4: {e1}\")\n",
    "        try:\n",
    "            ds = xr.open_dataset(netcdf_file, engine='scipy')\n",
    "            print(\"Successfully opened with scipy engine\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Failed with scipy: {e2}\")\n",
    "            try:\n",
    "                # Try cfgrib for GRIB format\n",
    "                ds = xr.open_dataset(netcdf_file, engine='cfgrib')\n",
    "                print(\"Successfully opened with cfgrib engine (GRIB format)\")\n",
    "            except Exception as e3:\n",
    "                print(f\"Failed with cfgrib: {e3}\")\n",
    "                raise ValueError(f\"Could not open file with any engine. Engines tried: netcdf4, scipy, cfgrib\")\n",
    "    \n",
    "    print(f\"Dataset variables: {list(ds.variables.keys())}\")\n",
    "    print(f\"Dataset dimensions: {ds.dims}\")\n",
    "    print(f\"Dataset coordinates: {list(ds.coords.keys())}\")\n",
    "    \n",
    "    # Get the precipitation variable (check common names)\n",
    "    precip_var = None\n",
    "    possible_names = ['tp', 'total_precipitation', 'precipitation', 'precip']\n",
    "    \n",
    "    for var_name in possible_names:\n",
    "        if var_name in ds.variables:\n",
    "            precip_var = var_name\n",
    "            break\n",
    "    \n",
    "    if precip_var is None:\n",
    "        raise ValueError(f\"Could not find precipitation variable. Available variables: {list(ds.variables.keys())}\")\n",
    "    \n",
    "    print(f\"Using precipitation variable: {precip_var}\")\n",
    "    \n",
    "    # Convert from m/hour to mm/hour (ERA5-Land precip is in meters)\n",
    "    precip_mm = ds[precip_var] * 1000  # Convert m to mm\n",
    "    \n",
    "    print(f\"Original data shape: {precip_mm.shape}\")\n",
    "    print(f\"Data range: {precip_mm.min().values:.6f} to {precip_mm.max().values:.6f} mm\")\n",
    "    \n",
    "    # Apply threshold if specified\n",
    "    if threshold_mm > 0:\n",
    "        precip_mm = precip_mm.where(precip_mm >= threshold_mm, 0)\n",
    "        print(f\"Applied {threshold_mm} mm threshold\")\n",
    "    \n",
    "    # Calculate daily, monthly, and yearly sums\n",
    "    daily_sum = precip_mm.resample(time='D').sum()\n",
    "    monthly_sum = precip_mm.resample(time='M').sum()\n",
    "    yearly_sum = precip_mm.resample(time='Y').sum()\n",
    "    \n",
    "    # Get London area average (if multiple grid points)\n",
    "    if 'latitude' in precip_mm.dims and 'longitude' in precip_mm.dims:\n",
    "        daily_avg = daily_sum.mean(dim=['latitude', 'longitude'])\n",
    "        monthly_avg = monthly_sum.mean(dim=['latitude', 'longitude'])\n",
    "        yearly_avg = yearly_sum.mean(dim=['latitude', 'longitude'])\n",
    "        print(\"Averaged over spatial dimensions\")\n",
    "    else:\n",
    "        daily_avg = daily_sum\n",
    "        monthly_avg = monthly_sum  \n",
    "        yearly_avg = yearly_sum\n",
    "        print(\"No spatial averaging needed\")\n",
    "    \n",
    "    return {\n",
    "        'daily': daily_avg,\n",
    "        'monthly': monthly_avg,\n",
    "        'yearly': yearly_avg,\n",
    "        'raw_data': precip_mm\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Start with a smaller test - just one month\n",
    "    years = ['2023']\n",
    "    months = ['01']  # Start with January only\n",
    "    \n",
    "    print(\"Downloading ERA5-Land hourly precipitation data...\")\n",
    "    \n",
    "    # Method 1: Download hourly data (more reliable)\n",
    "    hourly_file = download_era5_land_precipitation(\n",
    "        years=years, \n",
    "        months=months, \n",
    "        output_file='london_era5land_hourly_precip.nc'\n",
    "    )\n",
    "    \n",
    "    # Method 2: Try daily statistics (if hourly works)\n",
    "    # daily_file = download_era5_land_daily_stats(\n",
    "    #     years=years, \n",
    "    #     months=months, \n",
    "    #     daily_statistic='daily_sum',\n",
    "    #     output_file='london_era5land_daily_precip.nc'\n",
    "    # )\n",
    "    \n",
    "    # Process the data\n",
    "    results = process_precipitation_data(hourly_file, threshold_mm=0.1)\n",
    "    \n",
    "    # Display some statistics\n",
    "    print(\"\\n=== Precipitation Statistics for London ===\")\n",
    "    print(f\"Daily mean: {results['daily'].mean().values:.2f} mm/day\")\n",
    "    print(f\"Monthly mean: {results['monthly'].mean().values:.2f} mm/month\") \n",
    "    print(f\"Yearly total: {results['yearly'].sum().values:.2f} mm/year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b714f52a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
