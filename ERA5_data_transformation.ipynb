{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96b58b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.data_manipulation import transform_data_datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2897389",
   "metadata": {},
   "source": [
    "## London Grid\n",
    "\n",
    "- [Copernicus Website](https://cds.climate.copernicus.eu/requests?tab=all)\n",
    "\n",
    "```\n",
    "North: 51.6\n",
    "West:  -0.2\n",
    "South: 51.4\n",
    "East:   0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57c9142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRIB File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e5e1ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading GRIB file: ./data/ERA5/ERA5_hourly_data_on_single_levels_V1.grib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\xarray_plugin.py:131: FutureWarning: In a future version, xarray will not decode timedelta values based on the presence of a timedelta-like units attribute by default. Instead it will rely on the presence of a timedelta64 dtype attribute, which is now xarray's default way of encoding timedelta64 values. To continue decoding timedeltas based on the presence of a timedelta-like units attribute, users will need to explicitly opt-in by passing True or CFTimedeltaCoder(decode_via_units=True) to decode_timedelta. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
      "  vars, attrs, coord_names = xr.conventions.decode_cf_variables(\n",
      "C:\\Users\\pacor\\AppData\\Local\\Temp\\ipykernel_22512\\606618702.py:30: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"Dataset dimensions: {dict(ds.dims)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Dataset dimensions: {'time': 62421, 'step': 2, 'latitude': 413, 'longitude': 2}\n",
      "Dataset variables: ['tp']\n",
      "Dataset coordinates: ['number', 'time', 'step', 'surface', 'latitude', 'longitude', 'valid_time']\n",
      "Longitude range: -0.2 to 0.05\n",
      "Using -180-180 longitude format: -0.1278\n",
      "Target coordinates: (51.5074, -0.1278)\n",
      "Nearest grid point: (51.6, -0.2)\n",
      "DataFrame shape: (124842, 12)\n",
      "DataFrame columns: ['time', 'step', 'number', 'surface', 'latitude', 'longitude', 'valid_time', 'tp', 'actual_latitude', 'actual_longitude', 'target_latitude', 'target_longitude']\n",
      "\n",
      "First few rows:\n",
      "                 time            step  number  surface  latitude  longitude  \\\n",
      "0 1940-01-01 06:00:00 0 days 05:00:00       0      0.0      51.6       -0.2   \n",
      "1 1940-01-01 06:00:00 0 days 07:00:00       0      0.0      51.6       -0.2   \n",
      "2 1940-01-01 18:00:00 0 days 05:00:00       0      0.0      51.6       -0.2   \n",
      "3 1940-01-01 18:00:00 0 days 07:00:00       0      0.0      51.6       -0.2   \n",
      "4 1940-01-02 06:00:00 0 days 05:00:00       0      0.0      51.6       -0.2   \n",
      "\n",
      "           valid_time            tp  actual_latitude  actual_longitude  \\\n",
      "0 1940-01-01 11:00:00           NaN             51.6              -0.2   \n",
      "1 1940-01-01 13:00:00  2.038479e-05             51.6              -0.2   \n",
      "2 1940-01-01 23:00:00  5.662441e-07             51.6              -0.2   \n",
      "3 1940-01-02 01:00:00           NaN             51.6              -0.2   \n",
      "4 1940-01-02 11:00:00           NaN             51.6              -0.2   \n",
      "\n",
      "   target_latitude  target_longitude  \n",
      "0          51.5074           -0.1278  \n",
      "1          51.5074           -0.1278  \n",
      "2          51.5074           -0.1278  \n",
      "3          51.5074           -0.1278  \n",
      "4          51.5074           -0.1278  \n",
      "\n",
      "Data saved to: ./output/london_weather_data_V1.csv\n",
      "\n",
      "==================================================\n",
      "SUMMARY\n",
      "==================================================\n",
      "Total records: 124842\n",
      "Time range: 1940-01-01 06:00:00 to 2025-06-13 06:00:00\n",
      "Variables extracted: ['step', 'number', 'surface', 'tp']\n",
      "\n",
      "==================================================\n",
      "VERIFICATION - Loading CSV back into pandas:\n",
      "==================================================\n",
      "Loaded DataFrame shape: (124842, 12)\n",
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 124842 entries, 0 to 124841\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   time              124842 non-null  object \n",
      " 1   step              124842 non-null  object \n",
      " 2   number            124842 non-null  int64  \n",
      " 3   surface           124842 non-null  float64\n",
      " 4   latitude          124842 non-null  float64\n",
      " 5   longitude         124842 non-null  float64\n",
      " 6   valid_time        124842 non-null  object \n",
      " 7   tp                62421 non-null   float64\n",
      " 8   actual_latitude   124842 non-null  float64\n",
      " 9   actual_longitude  124842 non-null  float64\n",
      " 10  target_latitude   124842 non-null  float64\n",
      " 11  target_longitude  124842 non-null  float64\n",
      "dtypes: float64(8), int64(1), object(3)\n",
      "memory usage: 11.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def grib_to_csv_london(grib_file_path, output_csv_path=None):\n",
    "    \"\"\"\n",
    "    Read GRIB file, extract data for London coordinates, and save as CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    grib_file_path (str): Path to the GRIB file\n",
    "    output_csv_path (str): Path for output CSV file (optional)\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with the extracted data\n",
    "    \"\"\"\n",
    "    \n",
    "    # London coordinates\n",
    "    london_lat = 51.5074\n",
    "    london_lon = -0.1278  # Note: GRIB files often use 0-360 longitude\n",
    "    london_lon_360 = 359.8722  # London longitude in 0-360 format\n",
    "    \n",
    "    print(f\"Reading GRIB file: {grib_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Open GRIB file with xarray\n",
    "        ds = xr.open_dataset(grib_file_path, engine='cfgrib')\n",
    "        \n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        print(f\"Dataset dimensions: {dict(ds.dims)}\")\n",
    "        print(f\"Dataset variables: {list(ds.data_vars)}\")\n",
    "        print(f\"Dataset coordinates: {list(ds.coords)}\")\n",
    "        \n",
    "        # Check longitude format (0-360 vs -180-180)\n",
    "        lon_min = ds.longitude.min().values\n",
    "        lon_max = ds.longitude.max().values\n",
    "        print(f\"Longitude range: {lon_min} to {lon_max}\")\n",
    "        \n",
    "        # Use appropriate London longitude based on the dataset's longitude format\n",
    "        if lon_max > 180:\n",
    "            target_lon = london_lon_360\n",
    "            print(f\"Using 0-360 longitude format: {target_lon}\")\n",
    "        else:\n",
    "            target_lon = london_lon\n",
    "            print(f\"Using -180-180 longitude format: {target_lon}\")\n",
    "        \n",
    "        # Find nearest grid point to London\n",
    "        lat_idx = np.abs(ds.latitude - london_lat).argmin()\n",
    "        lon_idx = np.abs(ds.longitude - target_lon).argmin()\n",
    "        \n",
    "        actual_lat = ds.latitude[lat_idx].values\n",
    "        actual_lon = ds.longitude[lon_idx].values\n",
    "        \n",
    "        print(f\"Target coordinates: ({london_lat}, {target_lon})\")\n",
    "        print(f\"Nearest grid point: ({actual_lat}, {actual_lon})\")\n",
    "        \n",
    "        # Extract data for London coordinates\n",
    "        london_data = ds.isel(latitude=lat_idx, longitude=lon_idx)\n",
    "        \n",
    "        # Convert to pandas DataFrame\n",
    "        df_list = []\n",
    "        \n",
    "        for var_name in london_data.data_vars:\n",
    "            var_data = london_data[var_name]\n",
    "            \n",
    "            # Handle different time dimensions\n",
    "            if 'time' in var_data.dims:\n",
    "                # Create DataFrame with time index\n",
    "                df_var = var_data.to_dataframe().reset_index()\n",
    "                df_var = df_var.rename(columns={var_name: var_name})\n",
    "                df_list.append(df_var)\n",
    "            elif 'valid_time' in var_data.dims:\n",
    "                # Handle valid_time dimension\n",
    "                df_var = var_data.to_dataframe().reset_index()\n",
    "                df_var = df_var.rename(columns={var_name: var_name})\n",
    "                df_list.append(df_var)\n",
    "            else:\n",
    "                # Single value variables\n",
    "                df_var = pd.DataFrame({\n",
    "                    var_name: [var_data.values],\n",
    "                    'latitude': [actual_lat],\n",
    "                    'longitude': [actual_lon]\n",
    "                })\n",
    "                df_list.append(df_var)\n",
    "        \n",
    "        # Combine all variables into one DataFrame\n",
    "        if len(df_list) > 1:\n",
    "            # Merge on common columns (time, latitude, longitude)\n",
    "            df = df_list[0]\n",
    "            for df_var in df_list[1:]:\n",
    "                common_cols = set(df.columns) & set(df_var.columns)\n",
    "                if common_cols:\n",
    "                    df = pd.merge(df, df_var, on=list(common_cols), how='outer')\n",
    "                else:\n",
    "                    # If no common columns, concatenate side by side\n",
    "                    df = pd.concat([df, df_var], axis=1)\n",
    "        else:\n",
    "            df = df_list[0]\n",
    "        \n",
    "        # Add metadata columns\n",
    "        df['actual_latitude'] = actual_lat\n",
    "        df['actual_longitude'] = actual_lon\n",
    "        df['target_latitude'] = london_lat\n",
    "        df['target_longitude'] = target_lon\n",
    "        \n",
    "        # Sort by time if time column exists\n",
    "        time_cols = [col for col in df.columns if 'time' in col.lower()]\n",
    "        if time_cols:\n",
    "            df = df.sort_values(time_cols[0])\n",
    "        \n",
    "        print(f\"DataFrame shape: {df.shape}\")\n",
    "        print(f\"DataFrame columns: {list(df.columns)}\")\n",
    "        print(\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Save to CSV\n",
    "        if output_csv_path is None:\n",
    "            output_csv_path = Path(grib_file_path).stem + \"_london_data.csv\"\n",
    "        \n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData saved to: {output_csv_path}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing GRIB file: {e}\")\n",
    "        print(\"Make sure you have the required packages installed:\")\n",
    "        print(\"pip install xarray cfgrib pandas numpy\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "    grib_file = \"./data/ERA5/ERA5_hourly_data_on_single_levels_V1.grib\"  # Replace with your GRIB file path\n",
    "    output_csv = \"./output/london_weather_data_V1.csv\"  # Replace with desired output path\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not Path(grib_file).exists():\n",
    "        print(f\"GRIB file not found: {grib_file}\")\n",
    "        print(\"Please update the 'grib_file' variable with the correct path to your GRIB file.\")\n",
    "        return\n",
    "    \n",
    "    # Process the file\n",
    "    try:\n",
    "        df = grib_to_csv_london(grib_file, output_csv)\n",
    "        \n",
    "        # Display summary statistics\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Total records: {len(df)}\")\n",
    "        \n",
    "        # Show time range if available\n",
    "        time_cols = [col for col in df.columns if 'time' in col.lower()]\n",
    "        if time_cols:\n",
    "            time_col = time_cols[0]\n",
    "            print(f\"Time range: {df[time_col].min()} to {df[time_col].max()}\")\n",
    "        \n",
    "        print(f\"Variables extracted: {[col for col in df.columns if col not in ['latitude', 'longitude', 'actual_latitude', 'actual_longitude', 'target_latitude', 'target_longitude'] + time_cols]}\")\n",
    "        \n",
    "        # Load the CSV back into pandas to verify\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"VERIFICATION - Loading CSV back into pandas:\")\n",
    "        print(\"=\"*50)\n",
    "        df_loaded = pd.read_csv(output_csv)\n",
    "        print(f\"Loaded DataFrame shape: {df_loaded.shape}\")\n",
    "        print(\"DataFrame info:\")\n",
    "        print(df_loaded.info())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process GRIB file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2817333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>step</th>\n",
       "      <th>number</th>\n",
       "      <th>surface</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>valid_time</th>\n",
       "      <th>tp</th>\n",
       "      <th>actual_latitude</th>\n",
       "      <th>actual_longitude</th>\n",
       "      <th>target_latitude</th>\n",
       "      <th>target_longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124837</th>\n",
       "      <td>2025-06-12 06:00:00</td>\n",
       "      <td>0 days 07:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>2025-06-12 13:00:00</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>-0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124838</th>\n",
       "      <td>2025-06-12 18:00:00</td>\n",
       "      <td>0 days 05:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>2025-06-12 23:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>-0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124839</th>\n",
       "      <td>2025-06-12 18:00:00</td>\n",
       "      <td>0 days 07:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>2025-06-13 01:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>-0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124840</th>\n",
       "      <td>2025-06-13 06:00:00</td>\n",
       "      <td>0 days 05:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>2025-06-13 11:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>-0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124841</th>\n",
       "      <td>2025-06-13 06:00:00</td>\n",
       "      <td>0 days 07:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>2025-06-13 13:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>-0.1278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       time             step  number  surface  latitude  \\\n",
       "124837  2025-06-12 06:00:00  0 days 07:00:00       0      0.0      51.6   \n",
       "124838  2025-06-12 18:00:00  0 days 05:00:00       0      0.0      51.6   \n",
       "124839  2025-06-12 18:00:00  0 days 07:00:00       0      0.0      51.6   \n",
       "124840  2025-06-13 06:00:00  0 days 05:00:00       0      0.0      51.6   \n",
       "124841  2025-06-13 06:00:00  0 days 07:00:00       0      0.0      51.6   \n",
       "\n",
       "        longitude           valid_time        tp  actual_latitude  \\\n",
       "124837       -0.2  2025-06-12 13:00:00  0.000118             51.6   \n",
       "124838       -0.2  2025-06-12 23:00:00  0.000000             51.6   \n",
       "124839       -0.2  2025-06-13 01:00:00       NaN             51.6   \n",
       "124840       -0.2  2025-06-13 11:00:00       NaN             51.6   \n",
       "124841       -0.2  2025-06-13 13:00:00  0.000000             51.6   \n",
       "\n",
       "        actual_longitude  target_latitude  target_longitude  \n",
       "124837              -0.2          51.5074           -0.1278  \n",
       "124838              -0.2          51.5074           -0.1278  \n",
       "124839              -0.2          51.5074           -0.1278  \n",
       "124840              -0.2          51.5074           -0.1278  \n",
       "124841              -0.2          51.5074           -0.1278  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_csv(\"./output/london_weather_data_V1.csv\")\n",
    "df_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bbada62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['year'] = pd.to_datetime(df_data['time'], errors='coerce').dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cded2ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year\n",
       "1940    0.045477\n",
       "1941    0.042473\n",
       "1942    0.043283\n",
       "1943    0.042122\n",
       "1944    0.042223\n",
       "          ...   \n",
       "2021    0.077198\n",
       "2022    0.054406\n",
       "2023    0.064124\n",
       "2024    0.073855\n",
       "2025    0.020036\n",
       "Name: tp, Length: 86, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_data\n",
    " .groupby('year', observed=True)['tp']\n",
    " .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625f3ac5",
   "metadata": {},
   "source": [
    "### NC files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16b7dc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading NetCDF file: ./data/ERA5/rainfall_hadukgrid_uk_1km_ann-30y_196101-199012.nc\n",
      "Dataset loaded successfully!\n",
      "Dataset dimensions: {'time': 1, 'projection_y_coordinate': 1450, 'projection_x_coordinate': 900, 'bnds': 2}\n",
      "Dataset variables: ['rainfall', 'transverse_mercator', 'time_bnds', 'projection_y_coordinate_bnds', 'projection_x_coordinate_bnds']\n",
      "Dataset coordinates: ['time', 'projection_y_coordinate', 'projection_x_coordinate', 'latitude', 'longitude']\n",
      "\n",
      "Dataset overview:\n",
      "<xarray.Dataset> Size: 31MB\n",
      "Dimensions:                       (time: 1, projection_y_coordinate: 1450,\n",
      "                                   projection_x_coordinate: 900, bnds: 2)\n",
      "Coordinates:\n",
      "  * time                          (time) datetime64[ns] 8B 1961-07-01\n",
      "  * projection_y_coordinate       (projection_y_coordinate) float64 12kB -1.9...\n",
      "  * projection_x_coordinate       (projection_x_coordinate) float64 7kB -1.99...\n",
      "    latitude                      (projection_y_coordinate, projection_x_coordinate) float64 10MB ...\n",
      "    longitude                     (projection_y_coordinate, projection_x_coordinate) float64 10MB ...\n",
      "Dimensions without coordinates: bnds\n",
      "Data variables:\n",
      "    rainfall                      (time, projection_y_coordinate, projection_x_coordinate) float64 10MB ...\n",
      "    transverse_mercator           int32 4B ...\n",
      "    time_bnds                     (time, bnds) datetime64[ns] 16B ...\n",
      "    projection_y_coordinate_bnds  (projection_y_coordinate, bnds) float64 23kB ...\n",
      "    projection_x_coordinate_bnds  (projection_x_coordinate, bnds) float64 14kB ...\n",
      "Attributes:\n",
      "    comment:        Annual resolution gridded climate observations\n",
      "    creation_date:  2024-06-06T15:03:56\n",
      "    frequency:      ann-30y\n",
      "    institution:    Met Office\n",
      "    lta_period:     1961-1990\n",
      "    references:     doi: 10.1002/gdj3.78\n",
      "    short_name:     annual_rainfall_climatology\n",
      "    source:         HadUK-Grid_v1.3.0.0\n",
      "    title:          Gridded surface climate observations data for the UK\n",
      "    version:        v20240514\n",
      "    Conventions:    CF-1.7\n",
      "\n",
      "Identified coordinates:\n",
      "Latitude coordinate: latitude\n",
      "Longitude coordinate: longitude\n",
      "Time coordinate: time\n",
      "Latitude range: 47.82437687984449 to 61.12827676046806\n",
      "Longitude range: -13.009019292543481 to 3.545127555239017\n",
      "\n",
      "Detected projected coordinates (likely OSGB)\n",
      "You may need to convert London lat/lon to the projection coordinates\n",
      "For now, finding nearest grid point in projected space...\n",
      "\n",
      "Extracting region around London:\n",
      "Latitude range: 51.007 to 52.007\n",
      "Longitude range: -0.628 to 0.372\n",
      "Using 2D coordinate arrays to find region\n",
      "Found 7721 grid points in region\n",
      "Grid index ranges: y=324-436, x=694-765\n",
      "Actual extracted region:\n",
      "  Latitude: 50.996 to 52.018\n",
      "  Longitude: -0.653 to 0.411\n",
      "\n",
      "Extracted region data shape: {'time': 1, 'projection_y_coordinate': 113, 'projection_x_coordinate': 72, 'bnds': 2}\n",
      "\n",
      "Extracted data variables: ['rainfall', 'transverse_mercator', 'time_bnds', 'projection_y_coordinate_bnds', 'projection_x_coordinate_bnds']\n",
      "\n",
      "Extracting data for London...\n",
      "Main data variables to extract: ['rainfall']\n",
      "\n",
      "Processing variable: rainfall\n",
      "Variable dimensions: ('time', 'projection_y_coordinate', 'projection_x_coordinate')\n",
      "Variable shape: (1, 113, 72)\n",
      "Variable attributes: {'standard_name': 'lwe_thickness_of_precipitation_amount', 'long_name': 'Total rainfall', 'units': 'mm', 'description': 'Total rainfall', 'label_units': 'mm', 'level': 'NA', 'plot_label': 'Total rainfall (mm)', 'cell_methods': 'time: sum within days time: sum over days time: mean over years', 'grid_mapping': 'transverse_mercator'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pacor\\AppData\\Local\\Temp\\ipykernel_8912\\4207545074.py:31: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"Dataset dimensions: {dict(ds.dims)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted by time column: time\n",
      "\n",
      "Final DataFrame shape: (8136, 7)\n",
      "DataFrame columns: ['rainfall', 'rainfall_units', 'rainfall_long_name', 'time', 'target_latitude', 'target_longitude', 'coordinate_system']\n",
      "\n",
      "Data types:\n",
      "  rainfall: float64\n",
      "  rainfall_units: object\n",
      "  rainfall_long_name: object\n",
      "  time: int64\n",
      "  target_latitude: float64\n",
      "  target_longitude: float64\n",
      "  coordinate_system: object\n",
      "\n",
      "First few rows:\n",
      "        rainfall rainfall_units rainfall_long_name                time  \\\n",
      "0     813.187760             mm     Total rainfall -268358400000000000   \n",
      "5431  712.100289             mm     Total rainfall -268358400000000000   \n",
      "5430  699.608436             mm     Total rainfall -268358400000000000   \n",
      "5429  682.856004             mm     Total rainfall -268358400000000000   \n",
      "5428  675.770487             mm     Total rainfall -268358400000000000   \n",
      "\n",
      "      target_latitude  target_longitude    coordinate_system  \n",
      "0             51.5074           -0.1278  latitude, longitude  \n",
      "5431          51.5074           -0.1278  latitude, longitude  \n",
      "5430          51.5074           -0.1278  latitude, longitude  \n",
      "5429          51.5074           -0.1278  latitude, longitude  \n",
      "5428          51.5074           -0.1278  latitude, longitude  \n",
      "\n",
      "Last few rows:\n",
      "        rainfall rainfall_units rainfall_long_name                time  \\\n",
      "2705  763.680790             mm     Total rainfall -268358400000000000   \n",
      "2704  800.578576             mm     Total rainfall -268358400000000000   \n",
      "2703  775.578975             mm     Total rainfall -268358400000000000   \n",
      "2716  783.358186             mm     Total rainfall -268358400000000000   \n",
      "8135  615.532970             mm     Total rainfall -268358400000000000   \n",
      "\n",
      "      target_latitude  target_longitude    coordinate_system  \n",
      "2705          51.5074           -0.1278  latitude, longitude  \n",
      "2704          51.5074           -0.1278  latitude, longitude  \n",
      "2703          51.5074           -0.1278  latitude, longitude  \n",
      "2716          51.5074           -0.1278  latitude, longitude  \n",
      "8135          51.5074           -0.1278  latitude, longitude  \n",
      "\n",
      "Data saved to: ./output/london_region_rainfall_1961_1990_V4.csv\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "Total grid points extracted: 8136\n",
      "Time range: -268358400000000000 to -268358400000000000\n",
      "\n",
      "Regional precipitation statistics for rainfall:\n",
      "count    8112.000000\n",
      "mean      683.870742\n",
      "std        88.403186\n",
      "min       507.432680\n",
      "25%       614.138137\n",
      "50%       659.392188\n",
      "75%       753.210531\n",
      "max      1005.195970\n",
      "Name: rainfall, dtype: float64\n",
      "Grid points with data: 8112/8136\n",
      "\n",
      "============================================================\n",
      "VERIFICATION - Loading CSV back into pandas:\n",
      "============================================================\n",
      "Loaded DataFrame shape: (8136, 7)\n",
      "Sample of loaded data:\n",
      "     rainfall rainfall_units rainfall_long_name                time  \\\n",
      "0  813.187760             mm     Total rainfall -268358400000000000   \n",
      "1  712.100289             mm     Total rainfall -268358400000000000   \n",
      "2  699.608436             mm     Total rainfall -268358400000000000   \n",
      "\n",
      "   target_latitude  target_longitude    coordinate_system  \n",
      "0          51.5074           -0.1278  latitude, longitude  \n",
      "1          51.5074           -0.1278  latitude, longitude  \n",
      "2          51.5074           -0.1278  latitude, longitude  \n"
     ]
    }
   ],
   "source": [
    "# V4 \n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def netcdf_to_csv_london(nc_file_path, output_csv_path=None):\n",
    "    \"\"\"\n",
    "    Read NetCDF file, extract precipitation data for London coordinates, and save as CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    nc_file_path (str): Path to the NetCDF file\n",
    "    output_csv_path (str): Path for output CSV file (optional)\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with the extracted data\n",
    "    \"\"\"\n",
    "    \n",
    "    # London coordinates\n",
    "    london_lat = 51.5074\n",
    "    london_lon = -0.1278\n",
    "    \n",
    "    print(f\"Reading NetCDF file: {nc_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Open NetCDF file with xarray\n",
    "        ds = xr.open_dataset(nc_file_path)\n",
    "        \n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        print(f\"Dataset dimensions: {dict(ds.dims)}\")\n",
    "        print(f\"Dataset variables: {list(ds.data_vars)}\")\n",
    "        print(f\"Dataset coordinates: {list(ds.coords)}\")\n",
    "        \n",
    "        # Display dataset info\n",
    "        print(\"\\nDataset overview:\")\n",
    "        print(ds)\n",
    "        \n",
    "        # Check coordinate names (common variations)\n",
    "        lat_names = ['lat', 'latitude', 'y', 'projection_y_coordinate']\n",
    "        lon_names = ['lon', 'longitude', 'x', 'projection_x_coordinate']\n",
    "        time_names = ['time', 'time_bnds', 'yyyymm']\n",
    "        \n",
    "        # Find actual coordinate names in the dataset\n",
    "        lat_coord = None\n",
    "        lon_coord = None\n",
    "        time_coord = None\n",
    "        \n",
    "        for name in lat_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                lat_coord = name\n",
    "                break\n",
    "                \n",
    "        for name in lon_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                lon_coord = name\n",
    "                break\n",
    "                \n",
    "        for name in time_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                time_coord = name\n",
    "                break\n",
    "        \n",
    "        print(f\"\\nIdentified coordinates:\")\n",
    "        print(f\"Latitude coordinate: {lat_coord}\")\n",
    "        print(f\"Longitude coordinate: {lon_coord}\")\n",
    "        print(f\"Time coordinate: {time_coord}\")\n",
    "        \n",
    "        if lat_coord is None or lon_coord is None:\n",
    "            print(\"Warning: Could not identify latitude/longitude coordinates\")\n",
    "            print(\"Available coordinates:\", list(ds.coords))\n",
    "            return None\n",
    "        \n",
    "        # Check coordinate ranges\n",
    "        if lat_coord in ds.coords:\n",
    "            lat_min = ds.coords[lat_coord].min().values\n",
    "            lat_max = ds.coords[lat_coord].max().values\n",
    "            print(f\"Latitude range: {lat_min} to {lat_max}\")\n",
    "        \n",
    "        if lon_coord in ds.coords:\n",
    "            lon_min = ds.coords[lon_coord].min().values\n",
    "            lon_max = ds.coords[lon_coord].max().values\n",
    "            print(f\"Longitude range: {lon_min} to {lon_max}\")\n",
    "        \n",
    "        # Handle projected coordinates (if using OSGB or similar)\n",
    "        if 'projection' in str(ds.coords) or lat_coord in ['y', 'projection_y_coordinate']:\n",
    "            print(\"\\nDetected projected coordinates (likely OSGB)\")\n",
    "            print(\"You may need to convert London lat/lon to the projection coordinates\")\n",
    "            print(\"For now, finding nearest grid point in projected space...\")\n",
    "        \n",
    "        # Define region around London (in degrees)\n",
    "        region_size = 0.5  # degrees (approximately 50km at London's latitude)\n",
    "        \n",
    "        # Define bounding box around London\n",
    "        lat_min = london_lat - region_size\n",
    "        lat_max = london_lat + region_size\n",
    "        lon_min = london_lon - region_size\n",
    "        lon_max = london_lon + region_size\n",
    "        \n",
    "        print(f\"\\nExtracting region around London:\")\n",
    "        print(f\"Latitude range: {lat_min:.3f} to {lat_max:.3f}\")\n",
    "        print(f\"Longitude range: {lon_min:.3f} to {lon_max:.3f}\")\n",
    "        \n",
    "        # Find grid indices for the region\n",
    "        if lat_coord in ds.coords and lon_coord in ds.coords:\n",
    "            # Handle 2D coordinate arrays (common in projected datasets)\n",
    "            lat_data = ds.coords[lat_coord]\n",
    "            lon_data = ds.coords[lon_coord]\n",
    "            \n",
    "            if lat_data.ndim == 2 and lon_data.ndim == 2:\n",
    "                # 2D coordinate arrays - find region\n",
    "                print(\"Using 2D coordinate arrays to find region\")\n",
    "                \n",
    "                # Find points within the bounding box\n",
    "                within_region = ((lat_data >= lat_min) & (lat_data <= lat_max) & \n",
    "                               (lon_data >= lon_min) & (lon_data <= lon_max))\n",
    "                \n",
    "                # Find the bounding indices of the region\n",
    "                y_indices, x_indices = np.where(within_region)\n",
    "                \n",
    "                if len(y_indices) == 0:\n",
    "                    print(\"No grid points found in the specified region!\")\n",
    "                    return None\n",
    "                \n",
    "                y_min_idx = y_indices.min()\n",
    "                y_max_idx = y_indices.max()\n",
    "                x_min_idx = x_indices.min()\n",
    "                x_max_idx = x_indices.max()\n",
    "                \n",
    "                print(f\"Found {len(y_indices)} grid points in region\")\n",
    "                print(f\"Grid index ranges: y={y_min_idx}-{y_max_idx}, x={x_min_idx}-{x_max_idx}\")\n",
    "                \n",
    "                # Store the slice ranges for extraction\n",
    "                y_slice = slice(y_min_idx, y_max_idx + 1)\n",
    "                x_slice = slice(x_min_idx, x_max_idx + 1)\n",
    "                \n",
    "                # Get actual coordinate bounds of extracted region\n",
    "                region_lat_min = lat_data[y_min_idx:y_max_idx+1, x_min_idx:x_max_idx+1].min().values\n",
    "                region_lat_max = lat_data[y_min_idx:y_max_idx+1, x_min_idx:x_max_idx+1].max().values\n",
    "                region_lon_min = lon_data[y_min_idx:y_max_idx+1, x_min_idx:x_max_idx+1].min().values\n",
    "                region_lon_max = lon_data[y_min_idx:y_max_idx+1, x_min_idx:x_max_idx+1].max().values\n",
    "                \n",
    "                print(f\"Actual extracted region:\")\n",
    "                print(f\"  Latitude: {region_lat_min:.3f} to {region_lat_max:.3f}\")\n",
    "                print(f\"  Longitude: {region_lon_min:.3f} to {region_lon_max:.3f}\")\n",
    "                \n",
    "            else:\n",
    "                # 1D coordinate arrays\n",
    "                lat_mask = (lat_data >= lat_min) & (lat_data <= lat_max)\n",
    "                lon_mask = (lon_data >= lon_min) & (lon_data <= lon_max)\n",
    "                \n",
    "                y_indices = np.where(lat_mask)[0]\n",
    "                x_indices = np.where(lon_mask)[0]\n",
    "                \n",
    "                if len(y_indices) == 0 or len(x_indices) == 0:\n",
    "                    print(\"No grid points found in the specified region!\")\n",
    "                    return None\n",
    "                \n",
    "                y_slice = slice(y_indices.min(), y_indices.max() + 1)\n",
    "                x_slice = slice(x_indices.min(), x_indices.max() + 1)\n",
    "                \n",
    "                print(f\"Grid slices: y={y_slice}, x={x_slice}\")\n",
    "        else:\n",
    "            print(\"Could not find coordinate values for region extraction\")\n",
    "            return None\n",
    "        \n",
    "        # Extract data for London region\n",
    "        # Use the projection coordinate names for selection\n",
    "        proj_y_coord = 'projection_y_coordinate'\n",
    "        proj_x_coord = 'projection_x_coordinate'\n",
    "        \n",
    "        selection_dict = {proj_y_coord: y_slice, proj_x_coord: x_slice}\n",
    "        london_region_data = ds.isel(selection_dict)\n",
    "        \n",
    "        print(f\"\\nExtracted region data shape: {dict(london_region_data.sizes)}\")\n",
    "        \n",
    "        print(f\"\\nExtracted data variables: {list(london_region_data.data_vars)}\")\n",
    "        \n",
    "        # Convert to pandas DataFrame - simplified approach\n",
    "        print(f\"\\nExtracting data for London...\")\n",
    "        \n",
    "        # Create a simple DataFrame with the main data\n",
    "        data_dict = {}\n",
    "        \n",
    "        # Extract main variables (skip coordinate and bounds variables)\n",
    "        main_vars = [var for var in london_region_data.data_vars \n",
    "                    if not any(skip in var for skip in ['_bnds', 'transverse_mercator', 'bounds'])]\n",
    "        \n",
    "        print(f\"Main data variables to extract: {main_vars}\")\n",
    "        \n",
    "        for var_name in main_vars:\n",
    "            var_data = london_region_data[var_name]\n",
    "            print(f\"\\nProcessing variable: {var_name}\")\n",
    "            print(f\"Variable dimensions: {var_data.dims}\")\n",
    "            print(f\"Variable shape: {var_data.shape}\")\n",
    "            \n",
    "            # Get variable attributes\n",
    "            if hasattr(var_data, 'attrs'):\n",
    "                print(f\"Variable attributes: {var_data.attrs}\")\n",
    "            \n",
    "            # Extract the actual values\n",
    "            if var_data.size == 1:\n",
    "                # Single value\n",
    "                data_dict[var_name] = [float(var_data.values)]\n",
    "            elif time_coord and time_coord in var_data.dims:\n",
    "                # Time series data\n",
    "                values = var_data.values\n",
    "                if values.ndim > 1:\n",
    "                    values = values.flatten()\n",
    "                data_dict[var_name] = values.tolist()\n",
    "            else:\n",
    "                # Other cases\n",
    "                values = var_data.values\n",
    "                if hasattr(values, 'flatten'):\n",
    "                    values = values.flatten()\n",
    "                data_dict[var_name] = [float(values)] if np.isscalar(values) else values.tolist()\n",
    "            \n",
    "            # Add metadata\n",
    "            if hasattr(var_data, 'attrs'):\n",
    "                units = var_data.attrs.get('units', 'unknown')\n",
    "                long_name = var_data.attrs.get('long_name', var_name)\n",
    "                data_dict[f'{var_name}_units'] = [units] * len(data_dict[var_name])\n",
    "                data_dict[f'{var_name}_long_name'] = [long_name] * len(data_dict[var_name])\n",
    "        \n",
    "        # Add time information if available\n",
    "        if time_coord and time_coord in london_region_data.coords:\n",
    "            time_values = london_region_data.coords[time_coord].values\n",
    "            if hasattr(time_values, 'flatten'):\n",
    "                time_values = time_values.flatten()\n",
    "            data_dict[time_coord] = time_values.tolist() if hasattr(time_values, 'tolist') else [time_values]\n",
    "        \n",
    "        # Create DataFrame\n",
    "        # First, make sure all lists have the same length\n",
    "        max_length = max(len(v) if isinstance(v, list) else 1 for v in data_dict.values())\n",
    "        \n",
    "        for key, value in data_dict.items():\n",
    "            if isinstance(value, list) and len(value) == 1 and max_length > 1:\n",
    "                data_dict[key] = value * max_length\n",
    "        \n",
    "        df = pd.DataFrame(data_dict)\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"Warning: No data extracted\")\n",
    "            return None\n",
    "        \n",
    "        # Add location metadata\n",
    "        df['target_latitude'] = london_lat\n",
    "        df['target_longitude'] = london_lon\n",
    "        df['coordinate_system'] = f\"{lat_coord}, {lon_coord}\"\n",
    "        \n",
    "        # Sort by time if time column exists\n",
    "        time_cols = [col for col in df.columns if any(t in col.lower() for t in ['time', 'date', 'year'])]\n",
    "        if time_cols:\n",
    "            df = df.sort_values(time_cols[0])\n",
    "            print(f\"Sorted by time column: {time_cols[0]}\")\n",
    "        \n",
    "        print(f\"\\nFinal DataFrame shape: {df.shape}\")\n",
    "        print(f\"DataFrame columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Show data types\n",
    "        print(f\"\\nData types:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"  {col}: {df[col].dtype}\")\n",
    "        \n",
    "        print(f\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        if len(df) > 5:\n",
    "            print(f\"\\nLast few rows:\")\n",
    "            print(df.tail())\n",
    "        \n",
    "        # Save to CSV\n",
    "        if output_csv_path is None:\n",
    "            output_csv_path = Path(nc_file_path).stem + \"_london_region_precipitation.csv\"\n",
    "        \n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData saved to: {output_csv_path}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing NetCDF file: {e}\")\n",
    "        print(\"Make sure you have the required packages installed:\")\n",
    "        print(\"pip install xarray netcdf4 pandas numpy\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # Your specific file\n",
    "    nc_file = \"./data/ERA5/rainfall_hadukgrid_uk_1km_ann-30y_196101-199012.nc\"\n",
    "    output_csv = \"./output/london_region_rainfall_1961_1990_V4.csv\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not Path(nc_file).exists():\n",
    "        print(f\"NetCDF file not found: {nc_file}\")\n",
    "        print(\"Please make sure the file is in the current directory or update the path.\")\n",
    "        return\n",
    "    \n",
    "    # Process the file\n",
    "    try:\n",
    "        df = netcdf_to_csv_london(nc_file, output_csv)\n",
    "        \n",
    "        if df is not None:\n",
    "            # Display summary statistics\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"SUMMARY\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"Total grid points extracted: {len(df)}\")\n",
    "            \n",
    "            # Show spatial extent\n",
    "            if 'latitude' in df.columns and 'longitude' in df.columns:\n",
    "                print(f\"Latitude range: {df['latitude'].min():.3f} to {df['latitude'].max():.3f}\")\n",
    "                print(f\"Longitude range: {df['longitude'].min():.3f} to {df['longitude'].max():.3f}\")\n",
    "            \n",
    "            # Show time range if available\n",
    "            time_cols = [col for col in df.columns if any(t in col.lower() for t in ['time', 'date', 'year'])]\n",
    "            if time_cols:\n",
    "                time_col = time_cols[0]\n",
    "                print(f\"Time range: {df[time_col].min()} to {df[time_col].max()}\")\n",
    "            \n",
    "            # Show precipitation statistics\n",
    "            precip_cols = [col for col in df.columns if any(p in col.lower() for p in ['rain', 'precip', 'precipitation'])]\n",
    "            if precip_cols:\n",
    "                print(f\"\\nRegional precipitation statistics for {precip_cols[0]}:\")\n",
    "                print(df[precip_cols[0]].describe())\n",
    "                print(f\"Grid points with data: {df[precip_cols[0]].notna().sum()}/{len(df)}\")\n",
    "            \n",
    "            # Load the CSV back into pandas to verify\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"VERIFICATION - Loading CSV back into pandas:\")\n",
    "            print(\"=\"*60)\n",
    "            df_loaded = pd.read_csv(output_csv)\n",
    "            print(f\"Loaded DataFrame shape: {df_loaded.shape}\")\n",
    "            print(\"Sample of loaded data:\")\n",
    "            print(df_loaded.head(3))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process NetCDF file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e38b32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c110fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c425bce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading NetCDF file: ./data/ERA5/rainfall_hadukgrid_uk_1km_ann-30y_196101-199012.nc\n",
      "Dataset loaded successfully!\n",
      "Dataset dimensions: {'time': 1, 'projection_y_coordinate': 1450, 'projection_x_coordinate': 900, 'bnds': 2}\n",
      "Dataset variables: ['rainfall', 'transverse_mercator', 'time_bnds', 'projection_y_coordinate_bnds', 'projection_x_coordinate_bnds']\n",
      "Dataset coordinates: ['time', 'projection_y_coordinate', 'projection_x_coordinate', 'latitude', 'longitude']\n",
      "\n",
      "Dataset overview:\n",
      "<xarray.Dataset> Size: 31MB\n",
      "Dimensions:                       (time: 1, projection_y_coordinate: 1450,\n",
      "                                   projection_x_coordinate: 900, bnds: 2)\n",
      "Coordinates:\n",
      "  * time                          (time) datetime64[ns] 8B 1961-07-01\n",
      "  * projection_y_coordinate       (projection_y_coordinate) float64 12kB -1.9...\n",
      "  * projection_x_coordinate       (projection_x_coordinate) float64 7kB -1.99...\n",
      "    latitude                      (projection_y_coordinate, projection_x_coordinate) float64 10MB ...\n",
      "    longitude                     (projection_y_coordinate, projection_x_coordinate) float64 10MB ...\n",
      "Dimensions without coordinates: bnds\n",
      "Data variables:\n",
      "    rainfall                      (time, projection_y_coordinate, projection_x_coordinate) float64 10MB ...\n",
      "    transverse_mercator           int32 4B ...\n",
      "    time_bnds                     (time, bnds) datetime64[ns] 16B ...\n",
      "    projection_y_coordinate_bnds  (projection_y_coordinate, bnds) float64 23kB ...\n",
      "    projection_x_coordinate_bnds  (projection_x_coordinate, bnds) float64 14kB ...\n",
      "Attributes:\n",
      "    comment:        Annual resolution gridded climate observations\n",
      "    creation_date:  2024-06-06T15:03:56\n",
      "    frequency:      ann-30y\n",
      "    institution:    Met Office\n",
      "    lta_period:     1961-1990\n",
      "    references:     doi: 10.1002/gdj3.78\n",
      "    short_name:     annual_rainfall_climatology\n",
      "    source:         HadUK-Grid_v1.3.0.0\n",
      "    title:          Gridded surface climate observations data for the UK\n",
      "    version:        v20240514\n",
      "    Conventions:    CF-1.7\n",
      "\n",
      "Identified coordinates:\n",
      "Latitude coordinate: latitude\n",
      "Longitude coordinate: longitude\n",
      "Time coordinate: time\n",
      "Latitude range: 47.82437687984449 to 61.12827676046806\n",
      "Longitude range: -13.009019292543481 to 3.545127555239017\n",
      "\n",
      "Detected projected coordinates (likely OSGB)\n",
      "You may need to convert London lat/lon to the projection coordinates\n",
      "For now, finding nearest grid point in projected space...\n",
      "Using 2D coordinate arrays to find nearest point\n",
      "\n",
      "Target coordinates: (51.5074, -0.1278)\n",
      "Nearest grid point: (51.50808870839424, -0.1337647443941244)\n",
      "Grid indices: y=380, x=729\n",
      "\n",
      "Extracted data variables: ['rainfall', 'transverse_mercator', 'time_bnds', 'projection_y_coordinate_bnds', 'projection_x_coordinate_bnds']\n",
      "\n",
      "Extracting data for London...\n",
      "Main data variables to extract: ['rainfall']\n",
      "\n",
      "Processing variable: rainfall\n",
      "Variable dimensions: ('time',)\n",
      "Variable shape: (1,)\n",
      "Variable attributes: {'standard_name': 'lwe_thickness_of_precipitation_amount', 'long_name': 'Total rainfall', 'units': 'mm', 'description': 'Total rainfall', 'label_units': 'mm', 'level': 'NA', 'plot_label': 'Total rainfall (mm)', 'cell_methods': 'time: sum within days time: sum over days time: mean over years', 'grid_mapping': 'transverse_mercator'}\n",
      "Sorted by time column: time\n",
      "\n",
      "Final DataFrame shape: (1, 9)\n",
      "DataFrame columns: ['rainfall', 'rainfall_units', 'rainfall_long_name', 'time', 'actual_latitude', 'actual_longitude', 'target_latitude', 'target_longitude', 'coordinate_system']\n",
      "\n",
      "Data types:\n",
      "  rainfall: float64\n",
      "  rainfall_units: object\n",
      "  rainfall_long_name: object\n",
      "  time: int64\n",
      "  actual_latitude: float64\n",
      "  actual_longitude: float64\n",
      "  target_latitude: float64\n",
      "  target_longitude: float64\n",
      "  coordinate_system: object\n",
      "\n",
      "First few rows:\n",
      "   rainfall rainfall_units rainfall_long_name                time  \\\n",
      "0  624.2972             mm     Total rainfall -268358400000000000   \n",
      "\n",
      "   actual_latitude  actual_longitude  target_latitude  target_longitude  \\\n",
      "0        51.508089         -0.133765          51.5074           -0.1278   \n",
      "\n",
      "     coordinate_system  \n",
      "0  latitude, longitude  \n",
      "\n",
      "Data saved to: ./output/london_rainfall_1961_1990_V3.csv\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "Total records: 1\n",
      "Time range: -268358400000000000 to -268358400000000000\n",
      "\n",
      "Precipitation statistics for rainfall:\n",
      "count      1.0000\n",
      "mean     624.2972\n",
      "std           NaN\n",
      "min      624.2972\n",
      "25%      624.2972\n",
      "50%      624.2972\n",
      "75%      624.2972\n",
      "max      624.2972\n",
      "Name: rainfall, dtype: float64\n",
      "\n",
      "============================================================\n",
      "VERIFICATION - Loading CSV back into pandas:\n",
      "============================================================\n",
      "Loaded DataFrame shape: (1, 9)\n",
      "Sample of loaded data:\n",
      "   rainfall rainfall_units rainfall_long_name                time  \\\n",
      "0  624.2972             mm     Total rainfall -268358400000000000   \n",
      "\n",
      "   actual_latitude  actual_longitude  target_latitude  target_longitude  \\\n",
      "0        51.508089         -0.133765          51.5074           -0.1278   \n",
      "\n",
      "     coordinate_system  \n",
      "0  latitude, longitude  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pacor\\AppData\\Local\\Temp\\ipykernel_8912\\2614367773.py:31: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"Dataset dimensions: {dict(ds.dims)}\")\n",
      "C:\\Users\\pacor\\AppData\\Local\\Temp\\ipykernel_8912\\2614367773.py:170: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  data_dict[var_name] = [float(var_data.values)]\n"
     ]
    }
   ],
   "source": [
    "# V3\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def netcdf_to_csv_london(nc_file_path, output_csv_path=None):\n",
    "    \"\"\"\n",
    "    Read NetCDF file, extract precipitation data for London coordinates, and save as CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    nc_file_path (str): Path to the NetCDF file\n",
    "    output_csv_path (str): Path for output CSV file (optional)\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with the extracted data\n",
    "    \"\"\"\n",
    "    \n",
    "    # London coordinates\n",
    "    london_lat = 51.5074\n",
    "    london_lon = -0.1278\n",
    "    \n",
    "    print(f\"Reading NetCDF file: {nc_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Open NetCDF file with xarray\n",
    "        ds = xr.open_dataset(nc_file_path)\n",
    "        \n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        print(f\"Dataset dimensions: {dict(ds.dims)}\")\n",
    "        print(f\"Dataset variables: {list(ds.data_vars)}\")\n",
    "        print(f\"Dataset coordinates: {list(ds.coords)}\")\n",
    "        \n",
    "        # Display dataset info\n",
    "        print(\"\\nDataset overview:\")\n",
    "        print(ds)\n",
    "        \n",
    "        # Check coordinate names (common variations)\n",
    "        lat_names = ['lat', 'latitude', 'y', 'projection_y_coordinate']\n",
    "        lon_names = ['lon', 'longitude', 'x', 'projection_x_coordinate']\n",
    "        time_names = ['time', 'time_bnds', 'yyyymm']\n",
    "        \n",
    "        # Find actual coordinate names in the dataset\n",
    "        lat_coord = None\n",
    "        lon_coord = None\n",
    "        time_coord = None\n",
    "        \n",
    "        for name in lat_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                lat_coord = name\n",
    "                break\n",
    "                \n",
    "        for name in lon_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                lon_coord = name\n",
    "                break\n",
    "                \n",
    "        for name in time_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                time_coord = name\n",
    "                break\n",
    "        \n",
    "        print(f\"\\nIdentified coordinates:\")\n",
    "        print(f\"Latitude coordinate: {lat_coord}\")\n",
    "        print(f\"Longitude coordinate: {lon_coord}\")\n",
    "        print(f\"Time coordinate: {time_coord}\")\n",
    "        \n",
    "        if lat_coord is None or lon_coord is None:\n",
    "            print(\"Warning: Could not identify latitude/longitude coordinates\")\n",
    "            print(\"Available coordinates:\", list(ds.coords))\n",
    "            return None\n",
    "        \n",
    "        # Check coordinate ranges\n",
    "        if lat_coord in ds.coords:\n",
    "            lat_min = ds.coords[lat_coord].min().values\n",
    "            lat_max = ds.coords[lat_coord].max().values\n",
    "            print(f\"Latitude range: {lat_min} to {lat_max}\")\n",
    "        \n",
    "        if lon_coord in ds.coords:\n",
    "            lon_min = ds.coords[lon_coord].min().values\n",
    "            lon_max = ds.coords[lon_coord].max().values\n",
    "            print(f\"Longitude range: {lon_min} to {lon_max}\")\n",
    "        \n",
    "        # Handle projected coordinates (if using OSGB or similar)\n",
    "        if 'projection' in str(ds.coords) or lat_coord in ['y', 'projection_y_coordinate']:\n",
    "            print(\"\\nDetected projected coordinates (likely OSGB)\")\n",
    "            print(\"You may need to convert London lat/lon to the projection coordinates\")\n",
    "            print(\"For now, finding nearest grid point in projected space...\")\n",
    "        \n",
    "        # Find nearest grid point to London\n",
    "        if lat_coord in ds.coords and lon_coord in ds.coords:\n",
    "            # Handle 2D coordinate arrays (common in projected datasets)\n",
    "            lat_data = ds.coords[lat_coord]\n",
    "            lon_data = ds.coords[lon_coord]\n",
    "            \n",
    "            if lat_data.ndim == 2 and lon_data.ndim == 2:\n",
    "                # 2D coordinate arrays - find nearest point in 2D space\n",
    "                print(\"Using 2D coordinate arrays to find nearest point\")\n",
    "                \n",
    "                # Calculate distance from London to all grid points\n",
    "                lat_diff = lat_data - london_lat\n",
    "                lon_diff = lon_data - london_lon\n",
    "                distances = np.sqrt(lat_diff**2 + lon_diff**2)\n",
    "                \n",
    "                # Find indices of minimum distance\n",
    "                min_idx = np.unravel_index(distances.argmin(), distances.shape)\n",
    "                y_idx, x_idx = min_idx\n",
    "                \n",
    "                actual_lat = lat_data[y_idx, x_idx].values\n",
    "                actual_lon = lon_data[y_idx, x_idx].values\n",
    "                \n",
    "                print(f\"\\nTarget coordinates: ({london_lat}, {london_lon})\")\n",
    "                print(f\"Nearest grid point: ({actual_lat}, {actual_lon})\")\n",
    "                print(f\"Grid indices: y={y_idx}, x={x_idx}\")\n",
    "                \n",
    "                # Use the projection coordinate indices for selection\n",
    "                lat_idx = y_idx  # This corresponds to projection_y_coordinate index\n",
    "                lon_idx = x_idx  # This corresponds to projection_x_coordinate index\n",
    "                \n",
    "            else:\n",
    "                # 1D coordinate arrays\n",
    "                lat_idx = np.abs(lat_data - london_lat).argmin()\n",
    "                lon_idx = np.abs(lon_data - london_lon).argmin()\n",
    "                \n",
    "                actual_lat = lat_data[lat_idx].values\n",
    "                actual_lon = lon_data[lon_idx].values\n",
    "                \n",
    "                print(f\"\\nTarget coordinates: ({london_lat}, {london_lon})\")\n",
    "                print(f\"Nearest grid point: ({actual_lat}, {actual_lon})\")\n",
    "        else:\n",
    "            print(\"Could not find coordinate values for nearest point calculation\")\n",
    "            return None\n",
    "        \n",
    "        # Extract data for London coordinates\n",
    "        # Use the projection coordinate names for selection\n",
    "        proj_y_coord = 'projection_y_coordinate'\n",
    "        proj_x_coord = 'projection_x_coordinate'\n",
    "        \n",
    "        selection_dict = {proj_y_coord: lat_idx, proj_x_coord: lon_idx}\n",
    "        london_data = ds.isel(selection_dict)\n",
    "        \n",
    "        print(f\"\\nExtracted data variables: {list(london_data.data_vars)}\")\n",
    "        \n",
    "        # Convert to pandas DataFrame - simplified approach\n",
    "        print(f\"\\nExtracting data for London...\")\n",
    "        \n",
    "        # Create a simple DataFrame with the main data\n",
    "        data_dict = {}\n",
    "        \n",
    "        # Extract main variables (skip coordinate and bounds variables)\n",
    "        main_vars = [var for var in london_data.data_vars \n",
    "                    if not any(skip in var for skip in ['_bnds', 'transverse_mercator', 'bounds'])]\n",
    "        \n",
    "        print(f\"Main data variables to extract: {main_vars}\")\n",
    "        \n",
    "        for var_name in main_vars:\n",
    "            var_data = london_data[var_name]\n",
    "            print(f\"\\nProcessing variable: {var_name}\")\n",
    "            print(f\"Variable dimensions: {var_data.dims}\")\n",
    "            print(f\"Variable shape: {var_data.shape}\")\n",
    "            \n",
    "            # Get variable attributes\n",
    "            if hasattr(var_data, 'attrs'):\n",
    "                print(f\"Variable attributes: {var_data.attrs}\")\n",
    "            \n",
    "            # Extract the actual values\n",
    "            if var_data.size == 1:\n",
    "                # Single value\n",
    "                data_dict[var_name] = [float(var_data.values)]\n",
    "            elif time_coord and time_coord in var_data.dims:\n",
    "                # Time series data\n",
    "                values = var_data.values\n",
    "                if values.ndim > 1:\n",
    "                    values = values.flatten()\n",
    "                data_dict[var_name] = values.tolist()\n",
    "            else:\n",
    "                # Other cases\n",
    "                values = var_data.values\n",
    "                if hasattr(values, 'flatten'):\n",
    "                    values = values.flatten()\n",
    "                data_dict[var_name] = [float(values)] if np.isscalar(values) else values.tolist()\n",
    "            \n",
    "            # Add metadata\n",
    "            if hasattr(var_data, 'attrs'):\n",
    "                units = var_data.attrs.get('units', 'unknown')\n",
    "                long_name = var_data.attrs.get('long_name', var_name)\n",
    "                data_dict[f'{var_name}_units'] = [units] * len(data_dict[var_name])\n",
    "                data_dict[f'{var_name}_long_name'] = [long_name] * len(data_dict[var_name])\n",
    "        \n",
    "        # Add time information if available\n",
    "        if time_coord and time_coord in london_data.coords:\n",
    "            time_values = london_data.coords[time_coord].values\n",
    "            if hasattr(time_values, 'flatten'):\n",
    "                time_values = time_values.flatten()\n",
    "            data_dict[time_coord] = time_values.tolist() if hasattr(time_values, 'tolist') else [time_values]\n",
    "        \n",
    "        # Create DataFrame\n",
    "        # First, make sure all lists have the same length\n",
    "        max_length = max(len(v) if isinstance(v, list) else 1 for v in data_dict.values())\n",
    "        \n",
    "        for key, value in data_dict.items():\n",
    "            if isinstance(value, list) and len(value) == 1 and max_length > 1:\n",
    "                data_dict[key] = value * max_length\n",
    "        \n",
    "        df = pd.DataFrame(data_dict)\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"Warning: No data extracted\")\n",
    "            return None\n",
    "        \n",
    "        # Add location metadata\n",
    "        df['actual_latitude'] = actual_lat\n",
    "        df['actual_longitude'] = actual_lon\n",
    "        df['target_latitude'] = london_lat\n",
    "        df['target_longitude'] = london_lon\n",
    "        df['coordinate_system'] = f\"{lat_coord}, {lon_coord}\"\n",
    "        \n",
    "        # Sort by time if time column exists\n",
    "        time_cols = [col for col in df.columns if any(t in col.lower() for t in ['time', 'date', 'year'])]\n",
    "        if time_cols:\n",
    "            df = df.sort_values(time_cols[0])\n",
    "            print(f\"Sorted by time column: {time_cols[0]}\")\n",
    "        \n",
    "        print(f\"\\nFinal DataFrame shape: {df.shape}\")\n",
    "        print(f\"DataFrame columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Show data types\n",
    "        print(f\"\\nData types:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"  {col}: {df[col].dtype}\")\n",
    "        \n",
    "        print(f\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        if len(df) > 5:\n",
    "            print(f\"\\nLast few rows:\")\n",
    "            print(df.tail())\n",
    "        \n",
    "        # Save to CSV\n",
    "        if output_csv_path is None:\n",
    "            output_csv_path = Path(nc_file_path).stem + \"_london_precipitation.csv\"\n",
    "        \n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData saved to: {output_csv_path}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing NetCDF file: {e}\")\n",
    "        print(\"Make sure you have the required packages installed:\")\n",
    "        print(\"pip install xarray netcdf4 pandas numpy\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # Your specific file\n",
    "    nc_file = \"./data/ERA5/rainfall_hadukgrid_uk_1km_ann-30y_196101-199012.nc\"\n",
    "    output_csv = \"./output/london_rainfall_1961_1990_V3.csv\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not Path(nc_file).exists():\n",
    "        print(f\"NetCDF file not found: {nc_file}\")\n",
    "        print(\"Please make sure the file is in the current directory or update the path.\")\n",
    "        return\n",
    "    \n",
    "    # Process the file\n",
    "    try:\n",
    "        df = netcdf_to_csv_london(nc_file, output_csv)\n",
    "        \n",
    "        if df is not None:\n",
    "            # Display summary statistics\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"SUMMARY\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"Total records: {len(df)}\")\n",
    "            \n",
    "            # Show time range if available\n",
    "            time_cols = [col for col in df.columns if any(t in col.lower() for t in ['time', 'date', 'year'])]\n",
    "            if time_cols:\n",
    "                time_col = time_cols[0]\n",
    "                print(f\"Time range: {df[time_col].min()} to {df[time_col].max()}\")\n",
    "            \n",
    "            # Show precipitation statistics\n",
    "            precip_cols = [col for col in df.columns if any(p in col.lower() for p in ['rain', 'precip', 'precipitation'])]\n",
    "            if precip_cols:\n",
    "                print(f\"\\nPrecipitation statistics for {precip_cols[0]}:\")\n",
    "                print(df[precip_cols[0]].describe())\n",
    "            \n",
    "            # Load the CSV back into pandas to verify\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"VERIFICATION - Loading CSV back into pandas:\")\n",
    "            print(\"=\"*60)\n",
    "            df_loaded = pd.read_csv(output_csv)\n",
    "            print(f\"Loaded DataFrame shape: {df_loaded.shape}\")\n",
    "            print(\"Sample of loaded data:\")\n",
    "            print(df_loaded.head(3))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process NetCDF file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c4e0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca1f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading NetCDF file: ./data/ERA5/rainfall_hadukgrid_uk_1km_ann-30y_196101-199012.nc\n",
      "Dataset loaded successfully!\n",
      "Dataset dimensions: {'time': 1, 'projection_y_coordinate': 1450, 'projection_x_coordinate': 900, 'bnds': 2}\n",
      "Dataset variables: ['rainfall', 'transverse_mercator', 'time_bnds', 'projection_y_coordinate_bnds', 'projection_x_coordinate_bnds']\n",
      "Dataset coordinates: ['time', 'projection_y_coordinate', 'projection_x_coordinate', 'latitude', 'longitude']\n",
      "\n",
      "Dataset overview:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pacor\\AppData\\Local\\Temp\\ipykernel_8912\\4294618856.py:29: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"Dataset dimensions: {dict(ds.dims)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 31MB\n",
      "Dimensions:                       (time: 1, projection_y_coordinate: 1450,\n",
      "                                   projection_x_coordinate: 900, bnds: 2)\n",
      "Coordinates:\n",
      "  * time                          (time) datetime64[ns] 8B 1961-07-01\n",
      "  * projection_y_coordinate       (projection_y_coordinate) float64 12kB -1.9...\n",
      "  * projection_x_coordinate       (projection_x_coordinate) float64 7kB -1.99...\n",
      "    latitude                      (projection_y_coordinate, projection_x_coordinate) float64 10MB ...\n",
      "    longitude                     (projection_y_coordinate, projection_x_coordinate) float64 10MB ...\n",
      "Dimensions without coordinates: bnds\n",
      "Data variables:\n",
      "    rainfall                      (time, projection_y_coordinate, projection_x_coordinate) float64 10MB ...\n",
      "    transverse_mercator           int32 4B ...\n",
      "    time_bnds                     (time, bnds) datetime64[ns] 16B ...\n",
      "    projection_y_coordinate_bnds  (projection_y_coordinate, bnds) float64 23kB ...\n",
      "    projection_x_coordinate_bnds  (projection_x_coordinate, bnds) float64 14kB ...\n",
      "Attributes:\n",
      "    comment:        Annual resolution gridded climate observations\n",
      "    creation_date:  2024-06-06T15:03:56\n",
      "    frequency:      ann-30y\n",
      "    institution:    Met Office\n",
      "    lta_period:     1961-1990\n",
      "    references:     doi: 10.1002/gdj3.78\n",
      "    short_name:     annual_rainfall_climatology\n",
      "    source:         HadUK-Grid_v1.3.0.0\n",
      "    title:          Gridded surface climate observations data for the UK\n",
      "    version:        v20240514\n",
      "    Conventions:    CF-1.7\n",
      "\n",
      "Identified coordinates:\n",
      "Latitude coordinate: latitude\n",
      "Longitude coordinate: longitude\n",
      "Time coordinate: time\n",
      "Latitude range: 47.82437687984449 to 61.12827676046806\n",
      "Longitude range: -13.009019292543481 to 3.545127555239017\n",
      "\n",
      "Detected projected coordinates (likely OSGB)\n",
      "You may need to convert London lat/lon to the projection coordinates\n",
      "For now, finding nearest grid point in projected space...\n",
      "Using 2D coordinate arrays to find nearest point\n",
      "\n",
      "Target coordinates: (51.5074, -0.1278)\n",
      "Nearest grid point: (51.50808870839424, -0.1337647443941244)\n",
      "Grid indices: y=380, x=729\n",
      "\n",
      "Extracted data variables: ['rainfall', 'transverse_mercator', 'time_bnds', 'projection_y_coordinate_bnds', 'projection_x_coordinate_bnds']\n",
      "\n",
      "Processing variable: rainfall\n",
      "Variable dimensions: ('time',)\n",
      "Variable shape: (1,)\n",
      "Variable attributes: {'standard_name': 'lwe_thickness_of_precipitation_amount', 'long_name': 'Total rainfall', 'units': 'mm', 'description': 'Total rainfall', 'label_units': 'mm', 'level': 'NA', 'plot_label': 'Total rainfall (mm)', 'cell_methods': 'time: sum within days time: sum over days time: mean over years', 'grid_mapping': 'transverse_mercator'}\n",
      "\n",
      "Processing variable: transverse_mercator\n",
      "Variable dimensions: ()\n",
      "Variable shape: ()\n",
      "Variable attributes: {'grid_mapping_name': 'transverse_mercator', 'longitude_of_prime_meridian': 0.0, 'semi_major_axis': 6377563.396, 'semi_minor_axis': 6356256.909, 'longitude_of_central_meridian': -2.0, 'latitude_of_projection_origin': 49.0, 'false_easting': 400000.0, 'false_northing': -100000.0, 'scale_factor_at_central_meridian': 0.9996012717}\n",
      "\n",
      "Processing variable: time_bnds\n",
      "Variable dimensions: ('time', 'bnds')\n",
      "Variable shape: (1, 2)\n",
      "Variable attributes: {}\n",
      "\n",
      "Processing variable: projection_y_coordinate_bnds\n",
      "Variable dimensions: ('bnds',)\n",
      "Variable shape: (2,)\n",
      "Variable attributes: {}\n",
      "\n",
      "Processing variable: projection_x_coordinate_bnds\n",
      "Variable dimensions: ('bnds',)\n",
      "Variable shape: (2,)\n",
      "Variable attributes: {}\n",
      "Error processing NetCDF file: You are trying to merge on float64 and object columns for key 'longitude'. If you wish to proceed you should use pd.concat\n",
      "Make sure you have the required packages installed:\n",
      "pip install xarray netcdf4 pandas numpy\n",
      "Failed to process NetCDF file: You are trying to merge on float64 and object columns for key 'longitude'. If you wish to proceed you should use pd.concat\n"
     ]
    }
   ],
   "source": [
    "# Not working for datatype mismatch issues\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def netcdf_to_csv_london(nc_file_path, output_csv_path=None):\n",
    "    \"\"\"\n",
    "    Read NetCDF file, extract precipitation data for London coordinates, and save as CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    nc_file_path (str): Path to the NetCDF file\n",
    "    output_csv_path (str): Path for output CSV file (optional)\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with the extracted data\n",
    "    \"\"\"\n",
    "    \n",
    "    # London coordinates\n",
    "    london_lat = 51.5074\n",
    "    london_lon = -0.1278\n",
    "    \n",
    "    print(f\"Reading NetCDF file: {nc_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Open NetCDF file with xarray\n",
    "        ds = xr.open_dataset(nc_file_path)\n",
    "        \n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        print(f\"Dataset dimensions: {dict(ds.dims)}\")\n",
    "        print(f\"Dataset variables: {list(ds.data_vars)}\")\n",
    "        print(f\"Dataset coordinates: {list(ds.coords)}\")\n",
    "        \n",
    "        # Display dataset info\n",
    "        print(\"\\nDataset overview:\")\n",
    "        print(ds)\n",
    "        \n",
    "        # Check coordinate names (common variations)\n",
    "        lat_names = ['lat', 'latitude', 'y', 'projection_y_coordinate']\n",
    "        lon_names = ['lon', 'longitude', 'x', 'projection_x_coordinate']\n",
    "        time_names = ['time', 'time_bnds', 'yyyymm']\n",
    "        \n",
    "        # Find actual coordinate names in the dataset\n",
    "        lat_coord = None\n",
    "        lon_coord = None\n",
    "        time_coord = None\n",
    "        \n",
    "        for name in lat_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                lat_coord = name\n",
    "                break\n",
    "                \n",
    "        for name in lon_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                lon_coord = name\n",
    "                break\n",
    "                \n",
    "        for name in time_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                time_coord = name\n",
    "                break\n",
    "        \n",
    "        print(f\"\\nIdentified coordinates:\")\n",
    "        print(f\"Latitude coordinate: {lat_coord}\")\n",
    "        print(f\"Longitude coordinate: {lon_coord}\")\n",
    "        print(f\"Time coordinate: {time_coord}\")\n",
    "        \n",
    "        if lat_coord is None or lon_coord is None:\n",
    "            print(\"Warning: Could not identify latitude/longitude coordinates\")\n",
    "            print(\"Available coordinates:\", list(ds.coords))\n",
    "            return None\n",
    "        \n",
    "        # Check coordinate ranges\n",
    "        if lat_coord in ds.coords:\n",
    "            lat_min = ds.coords[lat_coord].min().values\n",
    "            lat_max = ds.coords[lat_coord].max().values\n",
    "            print(f\"Latitude range: {lat_min} to {lat_max}\")\n",
    "        \n",
    "        if lon_coord in ds.coords:\n",
    "            lon_min = ds.coords[lon_coord].min().values\n",
    "            lon_max = ds.coords[lon_coord].max().values\n",
    "            print(f\"Longitude range: {lon_min} to {lon_max}\")\n",
    "        \n",
    "        # Handle projected coordinates (if using OSGB or similar)\n",
    "        if 'projection' in str(ds.coords) or lat_coord in ['y', 'projection_y_coordinate']:\n",
    "            print(\"\\nDetected projected coordinates (likely OSGB)\")\n",
    "            print(\"You may need to convert London lat/lon to the projection coordinates\")\n",
    "            print(\"For now, finding nearest grid point in projected space...\")\n",
    "        \n",
    "        # Find nearest grid point to London\n",
    "        if lat_coord in ds.coords and lon_coord in ds.coords:\n",
    "            # Handle 2D coordinate arrays (common in projected datasets)\n",
    "            lat_data = ds.coords[lat_coord]\n",
    "            lon_data = ds.coords[lon_coord]\n",
    "            \n",
    "            if lat_data.ndim == 2 and lon_data.ndim == 2:\n",
    "                # 2D coordinate arrays - find nearest point in 2D space\n",
    "                print(\"Using 2D coordinate arrays to find nearest point\")\n",
    "                \n",
    "                # Calculate distance from London to all grid points\n",
    "                lat_diff = lat_data - london_lat\n",
    "                lon_diff = lon_data - london_lon\n",
    "                distances = np.sqrt(lat_diff**2 + lon_diff**2)\n",
    "                \n",
    "                # Find indices of minimum distance\n",
    "                min_idx = np.unravel_index(distances.argmin(), distances.shape)\n",
    "                y_idx, x_idx = min_idx\n",
    "                \n",
    "                actual_lat = lat_data[y_idx, x_idx].values\n",
    "                actual_lon = lon_data[y_idx, x_idx].values\n",
    "                \n",
    "                print(f\"\\nTarget coordinates: ({london_lat}, {london_lon})\")\n",
    "                print(f\"Nearest grid point: ({actual_lat}, {actual_lon})\")\n",
    "                print(f\"Grid indices: y={y_idx}, x={x_idx}\")\n",
    "                \n",
    "                # Use the projection coordinate indices for selection\n",
    "                lat_idx = y_idx  # This corresponds to projection_y_coordinate index\n",
    "                lon_idx = x_idx  # This corresponds to projection_x_coordinate index\n",
    "                \n",
    "            else:\n",
    "                # 1D coordinate arrays\n",
    "                lat_idx = np.abs(lat_data - london_lat).argmin()\n",
    "                lon_idx = np.abs(lon_data - london_lon).argmin()\n",
    "                \n",
    "                actual_lat = lat_data[lat_idx].values\n",
    "                actual_lon = lon_data[lon_idx].values\n",
    "                \n",
    "                print(f\"\\nTarget coordinates: ({london_lat}, {london_lon})\")\n",
    "                print(f\"Nearest grid point: ({actual_lat}, {actual_lon})\")\n",
    "        else:\n",
    "            print(\"Could not find coordinate values for nearest point calculation\")\n",
    "            return None\n",
    "        \n",
    "        # Extract data for London coordinates\n",
    "        # Use the projection coordinate names for selection\n",
    "        proj_y_coord = 'projection_y_coordinate'\n",
    "        proj_x_coord = 'projection_x_coordinate'\n",
    "        \n",
    "        selection_dict = {proj_y_coord: lat_idx, proj_x_coord: lon_idx}\n",
    "        london_data = ds.isel(selection_dict)\n",
    "        \n",
    "        print(f\"\\nExtracted data variables: {list(london_data.data_vars)}\")\n",
    "        \n",
    "        # Convert to pandas DataFrame\n",
    "        df_list = []\n",
    "        \n",
    "        for var_name in london_data.data_vars:\n",
    "            var_data = london_data[var_name]\n",
    "            print(f\"\\nProcessing variable: {var_name}\")\n",
    "            print(f\"Variable dimensions: {var_data.dims}\")\n",
    "            print(f\"Variable shape: {var_data.shape}\")\n",
    "            \n",
    "            # Get variable attributes\n",
    "            if hasattr(var_data, 'attrs'):\n",
    "                print(f\"Variable attributes: {var_data.attrs}\")\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            if time_coord and time_coord in var_data.dims:\n",
    "                # Time series data\n",
    "                df_var = var_data.to_dataframe().reset_index()\n",
    "                \n",
    "                # Rename the variable column to be more descriptive\n",
    "                if var_name in df_var.columns:\n",
    "                    # Keep original variable name but add units if available\n",
    "                    units = var_data.attrs.get('units', '')\n",
    "                    long_name = var_data.attrs.get('long_name', var_name)\n",
    "                    \n",
    "                    col_name = var_name\n",
    "                    if units:\n",
    "                        col_name += f\"_{units}\".replace(\" \", \"_\")\n",
    "                    \n",
    "                    df_var = df_var.rename(columns={var_name: col_name})\n",
    "                    \n",
    "                    # Add metadata as separate columns\n",
    "                    df_var[f'{var_name}_long_name'] = long_name\n",
    "                    df_var[f'{var_name}_units'] = units\n",
    "                \n",
    "                df_list.append(df_var)\n",
    "            else:\n",
    "                # Single value or no time dimension\n",
    "                df_var = pd.DataFrame({\n",
    "                    var_name: [var_data.values],\n",
    "                    'latitude': [actual_lat],\n",
    "                    'longitude': [actual_lon]\n",
    "                })\n",
    "                \n",
    "                # Add metadata\n",
    "                if hasattr(var_data, 'attrs'):\n",
    "                    for attr_name, attr_value in var_data.attrs.items():\n",
    "                        df_var[f'{var_name}_{attr_name}'] = str(attr_value)\n",
    "                \n",
    "                df_list.append(df_var)\n",
    "        \n",
    "        # Combine all variables into one DataFrame\n",
    "        if len(df_list) > 1:\n",
    "            # Find common columns for merging\n",
    "            df = df_list[0]\n",
    "            for df_var in df_list[1:]:\n",
    "                common_cols = set(df.columns) & set(df_var.columns)\n",
    "                if common_cols and len(common_cols) > 1:  # More than just the variable column\n",
    "                    df = pd.merge(df, df_var, on=list(common_cols), how='outer')\n",
    "                else:\n",
    "                    # Concatenate side by side if same length\n",
    "                    if len(df) == len(df_var):\n",
    "                        df = pd.concat([df, df_var], axis=1)\n",
    "                    else:\n",
    "                        print(f\"Warning: Cannot merge {df_var.columns} - different lengths\")\n",
    "        else:\n",
    "            df = df_list[0] if df_list else pd.DataFrame()\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"Warning: No data extracted\")\n",
    "            return None\n",
    "        \n",
    "        # Add location metadata\n",
    "        df['actual_latitude'] = actual_lat\n",
    "        df['actual_longitude'] = actual_lon\n",
    "        df['target_latitude'] = london_lat\n",
    "        df['target_longitude'] = london_lon\n",
    "        df['coordinate_system'] = f\"{lat_coord}, {lon_coord}\"\n",
    "        \n",
    "        # Sort by time if time column exists\n",
    "        time_cols = [col for col in df.columns if any(t in col.lower() for t in ['time', 'date', 'year'])]\n",
    "        if time_cols:\n",
    "            df = df.sort_values(time_cols[0])\n",
    "            print(f\"Sorted by time column: {time_cols[0]}\")\n",
    "        \n",
    "        print(f\"\\nFinal DataFrame shape: {df.shape}\")\n",
    "        print(f\"DataFrame columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Show data types\n",
    "        print(f\"\\nData types:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"  {col}: {df[col].dtype}\")\n",
    "        \n",
    "        print(f\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        if len(df) > 5:\n",
    "            print(f\"\\nLast few rows:\")\n",
    "            print(df.tail())\n",
    "        \n",
    "        # Save to CSV\n",
    "        if output_csv_path is None:\n",
    "            output_csv_path = Path(nc_file_path).stem + \"_london_precipitation.csv\"\n",
    "        \n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData saved to: {output_csv_path}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing NetCDF file: {e}\")\n",
    "        print(\"Make sure you have the required packages installed:\")\n",
    "        print(\"pip install xarray netcdf4 pandas numpy\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # Your specific file\n",
    "    nc_file = \"./data/ERA5/rainfall_hadukgrid_uk_1km_ann-30y_196101-199012.nc\"\n",
    "    output_csv = \"./output/london_rainfall_1961_1990_V2.csv\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not Path(nc_file).exists():\n",
    "        print(f\"NetCDF file not found: {nc_file}\")\n",
    "        print(\"Please make sure the file is in the current directory or update the path.\")\n",
    "        return\n",
    "    \n",
    "    # Process the file\n",
    "    try:\n",
    "        df = netcdf_to_csv_london(nc_file, output_csv)\n",
    "        \n",
    "        if df is not None:\n",
    "            # Display summary statistics\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"SUMMARY\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"Total records: {len(df)}\")\n",
    "            \n",
    "            # Show time range if available\n",
    "            time_cols = [col for col in df.columns if any(t in col.lower() for t in ['time', 'date', 'year'])]\n",
    "            if time_cols:\n",
    "                time_col = time_cols[0]\n",
    "                print(f\"Time range: {df[time_col].min()} to {df[time_col].max()}\")\n",
    "            \n",
    "            # Show precipitation statistics\n",
    "            precip_cols = [col for col in df.columns if any(p in col.lower() for p in ['rain', 'precip', 'precipitation'])]\n",
    "            if precip_cols:\n",
    "                print(f\"\\nPrecipitation statistics for {precip_cols[0]}:\")\n",
    "                print(df[precip_cols[0]].describe())\n",
    "            \n",
    "            # Load the CSV back into pandas to verify\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"VERIFICATION - Loading CSV back into pandas:\")\n",
    "            print(\"=\"*60)\n",
    "            df_loaded = pd.read_csv(output_csv)\n",
    "            print(f\"Loaded DataFrame shape: {df_loaded.shape}\")\n",
    "            print(\"Sample of loaded data:\")\n",
    "            print(df_loaded.head(3))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process NetCDF file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0546e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d259e374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading GRIB file: ./data/ERA5/ERA5_hourly_data_on_single_levels_from_1940_to_present.grib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\xarray_plugin.py:131: FutureWarning: In a future version, xarray will not decode timedelta values based on the presence of a timedelta-like units attribute by default. Instead it will rely on the presence of a timedelta64 dtype attribute, which is now xarray's default way of encoding timedelta64 values. To continue decoding timedeltas based on the presence of a timedelta-like units attribute, users will need to explicitly opt-in by passing True or CFTimedeltaCoder(decode_via_units=True) to decode_timedelta. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
      "  vars, attrs, coord_names = xr.conventions.decode_cf_variables(\n",
      "C:\\Users\\pacor\\AppData\\Local\\Temp\\ipykernel_8912\\3625627430.py:31: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"Dataset dimensions: {dict(ds.dims)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Dataset dimensions: {'time': 2193, 'step': 12, 'latitude': 413, 'longitude': 2}\n",
      "Dataset variables: ['tp']\n",
      "Dataset coordinates: ['number', 'time', 'step', 'surface', 'latitude', 'longitude', 'valid_time']\n",
      "Longitude range: -0.2 to 0.05\n",
      "Using -180-180 longitude format: -0.1278\n",
      "Target coordinates: (51.5074, -0.1278)\n",
      "Nearest grid point: (51.6, -0.2)\n",
      "DataFrame shape: (26316, 12)\n",
      "DataFrame columns: ['time', 'step', 'number', 'surface', 'latitude', 'longitude', 'valid_time', 'tp', 'actual_latitude', 'actual_longitude', 'target_latitude', 'target_longitude']\n",
      "\n",
      "First few rows:\n",
      "                  time            step  number  surface  latitude  longitude  \\\n",
      "0  2021-12-31 18:00:00 0 days 01:00:00       0      0.0      51.6       -0.2   \n",
      "11 2021-12-31 18:00:00 0 days 12:00:00       0      0.0      51.6       -0.2   \n",
      "10 2021-12-31 18:00:00 0 days 11:00:00       0      0.0      51.6       -0.2   \n",
      "9  2021-12-31 18:00:00 0 days 10:00:00       0      0.0      51.6       -0.2   \n",
      "7  2021-12-31 18:00:00 0 days 08:00:00       0      0.0      51.6       -0.2   \n",
      "\n",
      "            valid_time            tp  actual_latitude  actual_longitude  \\\n",
      "0  2021-12-31 19:00:00           NaN             51.6              -0.2   \n",
      "11 2022-01-01 06:00:00  2.868474e-07             51.6              -0.2   \n",
      "10 2022-01-01 05:00:00  0.000000e+00             51.6              -0.2   \n",
      "9  2022-01-01 04:00:00  0.000000e+00             51.6              -0.2   \n",
      "7  2022-01-01 02:00:00  0.000000e+00             51.6              -0.2   \n",
      "\n",
      "    target_latitude  target_longitude  \n",
      "0           51.5074           -0.1278  \n",
      "11          51.5074           -0.1278  \n",
      "10          51.5074           -0.1278  \n",
      "9           51.5074           -0.1278  \n",
      "7           51.5074           -0.1278  \n",
      "\n",
      "Data saved to: ./output/london_weather_data.csv\n",
      "\n",
      "==================================================\n",
      "SUMMARY\n",
      "==================================================\n",
      "Total records: 26316\n",
      "Time range: 2021-12-31 18:00:00 to 2024-12-31 18:00:00\n",
      "Variables extracted: ['step', 'number', 'surface', 'tp']\n",
      "\n",
      "==================================================\n",
      "VERIFICATION - Loading CSV back into pandas:\n",
      "==================================================\n",
      "Loaded DataFrame shape: (26316, 12)\n",
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26316 entries, 0 to 26315\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   time              26316 non-null  object \n",
      " 1   step              26316 non-null  object \n",
      " 2   number            26316 non-null  int64  \n",
      " 3   surface           26316 non-null  float64\n",
      " 4   latitude          26316 non-null  float64\n",
      " 5   longitude         26316 non-null  float64\n",
      " 6   valid_time        26316 non-null  object \n",
      " 7   tp                26304 non-null  float64\n",
      " 8   actual_latitude   26316 non-null  float64\n",
      " 9   actual_longitude  26316 non-null  float64\n",
      " 10  target_latitude   26316 non-null  float64\n",
      " 11  target_longitude  26316 non-null  float64\n",
      "dtypes: float64(8), int64(1), object(3)\n",
      "memory usage: 2.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def grib_to_csv_london(grib_file_path, output_csv_path=None):\n",
    "    \"\"\"\n",
    "    Read GRIB file, extract data for London coordinates, and save as CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    grib_file_path (str): Path to the GRIB file\n",
    "    output_csv_path (str): Path for output CSV file (optional)\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with the extracted data\n",
    "    \"\"\"\n",
    "    \n",
    "    # London coordinates\n",
    "    london_lat = 51.5074\n",
    "    london_lon = -0.1278  # Note: GRIB files often use 0-360 longitude\n",
    "    london_lon_360 = 359.8722  # London longitude in 0-360 format\n",
    "    \n",
    "    print(f\"Reading GRIB file: {grib_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Open GRIB file with xarray\n",
    "        ds = xr.open_dataset(grib_file_path, engine='cfgrib')\n",
    "        \n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        print(f\"Dataset dimensions: {dict(ds.dims)}\")\n",
    "        print(f\"Dataset variables: {list(ds.data_vars)}\")\n",
    "        print(f\"Dataset coordinates: {list(ds.coords)}\")\n",
    "        \n",
    "        # Check longitude format (0-360 vs -180-180)\n",
    "        lon_min = ds.longitude.min().values\n",
    "        lon_max = ds.longitude.max().values\n",
    "        print(f\"Longitude range: {lon_min} to {lon_max}\")\n",
    "        \n",
    "        # Use appropriate London longitude based on the dataset's longitude format\n",
    "        if lon_max > 180:\n",
    "            target_lon = london_lon_360\n",
    "            print(f\"Using 0-360 longitude format: {target_lon}\")\n",
    "        else:\n",
    "            target_lon = london_lon\n",
    "            print(f\"Using -180-180 longitude format: {target_lon}\")\n",
    "        \n",
    "        # Find nearest grid point to London\n",
    "        lat_idx = np.abs(ds.latitude - london_lat).argmin()\n",
    "        lon_idx = np.abs(ds.longitude - target_lon).argmin()\n",
    "        \n",
    "        actual_lat = ds.latitude[lat_idx].values\n",
    "        actual_lon = ds.longitude[lon_idx].values\n",
    "        \n",
    "        print(f\"Target coordinates: ({london_lat}, {target_lon})\")\n",
    "        print(f\"Nearest grid point: ({actual_lat}, {actual_lon})\")\n",
    "        \n",
    "        # Extract data for London coordinates\n",
    "        london_data = ds.isel(latitude=lat_idx, longitude=lon_idx)\n",
    "        \n",
    "        # Convert to pandas DataFrame\n",
    "        df_list = []\n",
    "        \n",
    "        for var_name in london_data.data_vars:\n",
    "            var_data = london_data[var_name]\n",
    "            \n",
    "            # Handle different time dimensions\n",
    "            if 'time' in var_data.dims:\n",
    "                # Create DataFrame with time index\n",
    "                df_var = var_data.to_dataframe().reset_index()\n",
    "                df_var = df_var.rename(columns={var_name: var_name})\n",
    "                df_list.append(df_var)\n",
    "            elif 'valid_time' in var_data.dims:\n",
    "                # Handle valid_time dimension\n",
    "                df_var = var_data.to_dataframe().reset_index()\n",
    "                df_var = df_var.rename(columns={var_name: var_name})\n",
    "                df_list.append(df_var)\n",
    "            else:\n",
    "                # Single value variables\n",
    "                df_var = pd.DataFrame({\n",
    "                    var_name: [var_data.values],\n",
    "                    'latitude': [actual_lat],\n",
    "                    'longitude': [actual_lon]\n",
    "                })\n",
    "                df_list.append(df_var)\n",
    "        \n",
    "        # Combine all variables into one DataFrame\n",
    "        if len(df_list) > 1:\n",
    "            # Merge on common columns (time, latitude, longitude)\n",
    "            df = df_list[0]\n",
    "            for df_var in df_list[1:]:\n",
    "                common_cols = set(df.columns) & set(df_var.columns)\n",
    "                if common_cols:\n",
    "                    df = pd.merge(df, df_var, on=list(common_cols), how='outer')\n",
    "                else:\n",
    "                    # If no common columns, concatenate side by side\n",
    "                    df = pd.concat([df, df_var], axis=1)\n",
    "        else:\n",
    "            df = df_list[0]\n",
    "        \n",
    "        # Add metadata columns\n",
    "        df['actual_latitude'] = actual_lat\n",
    "        df['actual_longitude'] = actual_lon\n",
    "        df['target_latitude'] = london_lat\n",
    "        df['target_longitude'] = target_lon\n",
    "        \n",
    "        # Sort by time if time column exists\n",
    "        time_cols = [col for col in df.columns if 'time' in col.lower()]\n",
    "        if time_cols:\n",
    "            df = df.sort_values(time_cols[0])\n",
    "        \n",
    "        print(f\"DataFrame shape: {df.shape}\")\n",
    "        print(f\"DataFrame columns: {list(df.columns)}\")\n",
    "        print(\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Save to CSV\n",
    "        if output_csv_path is None:\n",
    "            output_csv_path = Path(grib_file_path).stem + \"_london_data.csv\"\n",
    "        \n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData saved to: {output_csv_path}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing GRIB file: {e}\")\n",
    "        print(\"Make sure you have the required packages installed:\")\n",
    "        print(\"pip install xarray cfgrib pandas numpy\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "    grib_file = \"./data/ERA5/ERA5_hourly_data_on_single_levels_from_1940_to_present.grib\"  # Replace with your GRIB file path\n",
    "    output_csv = \"./output/london_weather_data.csv\"  # Replace with desired output path\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not Path(grib_file).exists():\n",
    "        print(f\"GRIB file not found: {grib_file}\")\n",
    "        print(\"Please update the 'grib_file' variable with the correct path to your GRIB file.\")\n",
    "        return\n",
    "    \n",
    "    # Process the file\n",
    "    try:\n",
    "        df = grib_to_csv_london(grib_file, output_csv)\n",
    "        \n",
    "        # Display summary statistics\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Total records: {len(df)}\")\n",
    "        \n",
    "        # Show time range if available\n",
    "        time_cols = [col for col in df.columns if 'time' in col.lower()]\n",
    "        if time_cols:\n",
    "            time_col = time_cols[0]\n",
    "            print(f\"Time range: {df[time_col].min()} to {df[time_col].max()}\")\n",
    "        \n",
    "        print(f\"Variables extracted: {[col for col in df.columns if col not in ['latitude', 'longitude', 'actual_latitude', 'actual_longitude', 'target_latitude', 'target_longitude'] + time_cols]}\")\n",
    "        \n",
    "        # Load the CSV back into pandas to verify\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"VERIFICATION - Loading CSV back into pandas:\")\n",
    "        print(\"=\"*50)\n",
    "        df_loaded = pd.read_csv(output_csv)\n",
    "        print(f\"Loaded DataFrame shape: {df_loaded.shape}\")\n",
    "        print(\"DataFrame info:\")\n",
    "        print(df_loaded.info())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process GRIB file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44a783e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>step</th>\n",
       "      <th>number</th>\n",
       "      <th>surface</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>valid_time</th>\n",
       "      <th>tp</th>\n",
       "      <th>actual_latitude</th>\n",
       "      <th>actual_longitude</th>\n",
       "      <th>target_latitude</th>\n",
       "      <th>target_longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 01:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>2021-12-31 19:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>-0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 12:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>2022-01-01 06:00:00</td>\n",
       "      <td>2.868473e-07</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>-0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 11:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>2022-01-01 05:00:00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>-0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 10:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>2022-01-01 04:00:00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>-0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 08:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>2022-01-01 02:00:00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>-0.1278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time             step  number  surface  latitude  longitude  \\\n",
       "0  2021-12-31 18:00:00  0 days 01:00:00       0      0.0      51.6       -0.2   \n",
       "1  2021-12-31 18:00:00  0 days 12:00:00       0      0.0      51.6       -0.2   \n",
       "2  2021-12-31 18:00:00  0 days 11:00:00       0      0.0      51.6       -0.2   \n",
       "3  2021-12-31 18:00:00  0 days 10:00:00       0      0.0      51.6       -0.2   \n",
       "4  2021-12-31 18:00:00  0 days 08:00:00       0      0.0      51.6       -0.2   \n",
       "\n",
       "            valid_time            tp  actual_latitude  actual_longitude  \\\n",
       "0  2021-12-31 19:00:00           NaN             51.6              -0.2   \n",
       "1  2022-01-01 06:00:00  2.868473e-07             51.6              -0.2   \n",
       "2  2022-01-01 05:00:00  0.000000e+00             51.6              -0.2   \n",
       "3  2022-01-01 04:00:00  0.000000e+00             51.6              -0.2   \n",
       "4  2022-01-01 02:00:00  0.000000e+00             51.6              -0.2   \n",
       "\n",
       "   target_latitude  target_longitude  \n",
       "0          51.5074           -0.1278  \n",
       "1          51.5074           -0.1278  \n",
       "2          51.5074           -0.1278  \n",
       "3          51.5074           -0.1278  \n",
       "4          51.5074           -0.1278  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_csv(\"./output/london_weather_data.csv\")\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad349eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2021, 2022, 2023, 2024])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data['year'] = df_data['datetime'].dt.year\n",
    "df_data['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0fe86e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year\n",
       "2021    9.536743e-07\n",
       "2022    6.779676e-01\n",
       "2023    7.970974e-01\n",
       "2024    8.464056e-01\n",
       "Name: tp, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_data\n",
    " .groupby('year', observed=True)['tp']\n",
    " .sum()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9649922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>step</th>\n",
       "      <th>number</th>\n",
       "      <th>surface</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>valid_time</th>\n",
       "      <th>tp</th>\n",
       "      <th>actual_latitude</th>\n",
       "      <th>actual_longitude</th>\n",
       "      <th>target_latitude</th>\n",
       "      <th>target_longitude</th>\n",
       "      <th>datetime</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 01:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>2021-12-31 19:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>-0.1278</td>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 02:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>2021-12-31 20:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>-0.1278</td>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 03:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>2021-12-31 21:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>-0.1278</td>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 04:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>2021-12-31 22:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>-0.1278</td>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 05:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>2021-12-31 23:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>-0.1278</td>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 06:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>2022-01-01 00:00:00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>-0.1278</td>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 07:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>2022-01-01 01:00:00</td>\n",
       "      <td>6.668270e-07</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>-0.1278</td>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 08:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>2022-01-01 02:00:00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>-0.1278</td>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 09:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>2022-01-01 03:00:00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>-0.1278</td>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 10:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>2022-01-01 04:00:00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>-0.1278</td>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 11:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>2022-01-01 05:00:00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>-0.1278</td>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 12:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>2022-01-01 06:00:00</td>\n",
       "      <td>2.868473e-07</td>\n",
       "      <td>51.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>-0.1278</td>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   time             step  number  surface  latitude  \\\n",
       "0   2021-12-31 18:00:00  0 days 01:00:00       0      0.0      51.6   \n",
       "1   2021-12-31 18:00:00  0 days 02:00:00       0      0.0      51.6   \n",
       "2   2021-12-31 18:00:00  0 days 03:00:00       0      0.0      51.6   \n",
       "3   2021-12-31 18:00:00  0 days 04:00:00       0      0.0      51.6   \n",
       "4   2021-12-31 18:00:00  0 days 05:00:00       0      0.0      51.6   \n",
       "5   2021-12-31 18:00:00  0 days 06:00:00       0      0.0      51.6   \n",
       "6   2021-12-31 18:00:00  0 days 07:00:00       0      0.0      51.6   \n",
       "7   2021-12-31 18:00:00  0 days 08:00:00       0      0.0      51.6   \n",
       "8   2021-12-31 18:00:00  0 days 09:00:00       0      0.0      51.6   \n",
       "9   2021-12-31 18:00:00  0 days 10:00:00       0      0.0      51.6   \n",
       "10  2021-12-31 18:00:00  0 days 11:00:00       0      0.0      51.6   \n",
       "11  2021-12-31 18:00:00  0 days 12:00:00       0      0.0      51.6   \n",
       "\n",
       "    longitude          valid_time            tp  actual_latitude  \\\n",
       "0        -0.2 2021-12-31 19:00:00           NaN             51.6   \n",
       "1        -0.2 2021-12-31 20:00:00           NaN             51.6   \n",
       "2        -0.2 2021-12-31 21:00:00           NaN             51.6   \n",
       "3        -0.2 2021-12-31 22:00:00           NaN             51.6   \n",
       "4        -0.2 2021-12-31 23:00:00           NaN             51.6   \n",
       "5        -0.2 2022-01-01 00:00:00  0.000000e+00             51.6   \n",
       "6        -0.2 2022-01-01 01:00:00  6.668270e-07             51.6   \n",
       "7        -0.2 2022-01-01 02:00:00  0.000000e+00             51.6   \n",
       "8        -0.2 2022-01-01 03:00:00  0.000000e+00             51.6   \n",
       "9        -0.2 2022-01-01 04:00:00  0.000000e+00             51.6   \n",
       "10       -0.2 2022-01-01 05:00:00  0.000000e+00             51.6   \n",
       "11       -0.2 2022-01-01 06:00:00  2.868473e-07             51.6   \n",
       "\n",
       "    actual_longitude  target_latitude  target_longitude            datetime  \\\n",
       "0               -0.2          51.5074           -0.1278 2021-12-31 18:00:00   \n",
       "1               -0.2          51.5074           -0.1278 2021-12-31 18:00:00   \n",
       "2               -0.2          51.5074           -0.1278 2021-12-31 18:00:00   \n",
       "3               -0.2          51.5074           -0.1278 2021-12-31 18:00:00   \n",
       "4               -0.2          51.5074           -0.1278 2021-12-31 18:00:00   \n",
       "5               -0.2          51.5074           -0.1278 2021-12-31 18:00:00   \n",
       "6               -0.2          51.5074           -0.1278 2021-12-31 18:00:00   \n",
       "7               -0.2          51.5074           -0.1278 2021-12-31 18:00:00   \n",
       "8               -0.2          51.5074           -0.1278 2021-12-31 18:00:00   \n",
       "9               -0.2          51.5074           -0.1278 2021-12-31 18:00:00   \n",
       "10              -0.2          51.5074           -0.1278 2021-12-31 18:00:00   \n",
       "11              -0.2          51.5074           -0.1278 2021-12-31 18:00:00   \n",
       "\n",
       "    year  \n",
       "0   2021  \n",
       "1   2021  \n",
       "2   2021  \n",
       "3   2021  \n",
       "4   2021  \n",
       "5   2021  \n",
       "6   2021  \n",
       "7   2021  \n",
       "8   2021  \n",
       "9   2021  \n",
       "10  2021  \n",
       "11  2021  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data[df_data['year'] == 2021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9456cce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading NetCDF file: ./data/ERA5/rainfall_hadukgrid_uk_1km_ann-30y_196101-199012.nc\n",
      "Dataset loaded successfully!\n",
      "Dataset dimensions: {'time': 1, 'projection_y_coordinate': 1450, 'projection_x_coordinate': 900, 'bnds': 2}\n",
      "Dataset variables: ['rainfall', 'transverse_mercator', 'time_bnds', 'projection_y_coordinate_bnds', 'projection_x_coordinate_bnds']\n",
      "Dataset coordinates: ['time', 'projection_y_coordinate', 'projection_x_coordinate', 'latitude', 'longitude']\n",
      "\n",
      "Dataset overview:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pacor\\AppData\\Local\\Temp\\ipykernel_27988\\3345813786.py:29: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"Dataset dimensions: {dict(ds.dims)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 31MB\n",
      "Dimensions:                       (time: 1, projection_y_coordinate: 1450,\n",
      "                                   projection_x_coordinate: 900, bnds: 2)\n",
      "Coordinates:\n",
      "  * time                          (time) datetime64[ns] 8B 1961-07-01\n",
      "  * projection_y_coordinate       (projection_y_coordinate) float64 12kB -1.9...\n",
      "  * projection_x_coordinate       (projection_x_coordinate) float64 7kB -1.99...\n",
      "    latitude                      (projection_y_coordinate, projection_x_coordinate) float64 10MB ...\n",
      "    longitude                     (projection_y_coordinate, projection_x_coordinate) float64 10MB ...\n",
      "Dimensions without coordinates: bnds\n",
      "Data variables:\n",
      "    rainfall                      (time, projection_y_coordinate, projection_x_coordinate) float64 10MB ...\n",
      "    transverse_mercator           int32 4B ...\n",
      "    time_bnds                     (time, bnds) datetime64[ns] 16B ...\n",
      "    projection_y_coordinate_bnds  (projection_y_coordinate, bnds) float64 23kB ...\n",
      "    projection_x_coordinate_bnds  (projection_x_coordinate, bnds) float64 14kB ...\n",
      "Attributes:\n",
      "    comment:        Annual resolution gridded climate observations\n",
      "    creation_date:  2024-06-06T15:03:56\n",
      "    frequency:      ann-30y\n",
      "    institution:    Met Office\n",
      "    lta_period:     1961-1990\n",
      "    references:     doi: 10.1002/gdj3.78\n",
      "    short_name:     annual_rainfall_climatology\n",
      "    source:         HadUK-Grid_v1.3.0.0\n",
      "    title:          Gridded surface climate observations data for the UK\n",
      "    version:        v20240514\n",
      "    Conventions:    CF-1.7\n",
      "\n",
      "Identified coordinates:\n",
      "Latitude coordinate: latitude\n",
      "Longitude coordinate: longitude\n",
      "Time coordinate: time\n",
      "Latitude range: 47.82437687984449 to 61.12827676046806\n",
      "Longitude range: -13.009019292543481 to 3.545127555239017\n",
      "\n",
      "Detected projected coordinates (likely OSGB)\n",
      "You may need to convert London lat/lon to the projection coordinates\n",
      "For now, finding nearest grid point in projected space...\n",
      "Error processing NetCDF file: index 342467 is out of bounds for axis 0 with size 1450\n",
      "Make sure you have the required packages installed:\n",
      "pip install xarray netcdf4 pandas numpy\n",
      "Failed to process NetCDF file: index 342467 is out of bounds for axis 0 with size 1450\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def netcdf_to_csv_london(nc_file_path, output_csv_path=None):\n",
    "    \"\"\"\n",
    "    Read NetCDF file, extract precipitation data for London coordinates, and save as CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    nc_file_path (str): Path to the NetCDF file\n",
    "    output_csv_path (str): Path for output CSV file (optional)\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with the extracted data\n",
    "    \"\"\"\n",
    "    \n",
    "    # London coordinates\n",
    "    london_lat = 51.5074\n",
    "    london_lon = -0.1278\n",
    "    \n",
    "    print(f\"Reading NetCDF file: {nc_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Open NetCDF file with xarray\n",
    "        ds = xr.open_dataset(nc_file_path)\n",
    "        \n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        print(f\"Dataset dimensions: {dict(ds.dims)}\")\n",
    "        print(f\"Dataset variables: {list(ds.data_vars)}\")\n",
    "        print(f\"Dataset coordinates: {list(ds.coords)}\")\n",
    "        \n",
    "        # Display dataset info\n",
    "        print(\"\\nDataset overview:\")\n",
    "        print(ds)\n",
    "        \n",
    "        # Check coordinate names (common variations)\n",
    "        lat_names = ['lat', 'latitude', 'y', 'projection_y_coordinate']\n",
    "        lon_names = ['lon', 'longitude', 'x', 'projection_x_coordinate']\n",
    "        time_names = ['time', 'time_bnds', 'yyyymm']\n",
    "        \n",
    "        # Find actual coordinate names in the dataset\n",
    "        lat_coord = None\n",
    "        lon_coord = None\n",
    "        time_coord = None\n",
    "        \n",
    "        for name in lat_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                lat_coord = name\n",
    "                break\n",
    "                \n",
    "        for name in lon_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                lon_coord = name\n",
    "                break\n",
    "                \n",
    "        for name in time_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                time_coord = name\n",
    "                break\n",
    "        \n",
    "        print(f\"\\nIdentified coordinates:\")\n",
    "        print(f\"Latitude coordinate: {lat_coord}\")\n",
    "        print(f\"Longitude coordinate: {lon_coord}\")\n",
    "        print(f\"Time coordinate: {time_coord}\")\n",
    "        \n",
    "        if lat_coord is None or lon_coord is None:\n",
    "            print(\"Warning: Could not identify latitude/longitude coordinates\")\n",
    "            print(\"Available coordinates:\", list(ds.coords))\n",
    "            return None\n",
    "        \n",
    "        # Check coordinate ranges\n",
    "        if lat_coord in ds.coords:\n",
    "            lat_min = ds.coords[lat_coord].min().values\n",
    "            lat_max = ds.coords[lat_coord].max().values\n",
    "            print(f\"Latitude range: {lat_min} to {lat_max}\")\n",
    "        \n",
    "        if lon_coord in ds.coords:\n",
    "            lon_min = ds.coords[lon_coord].min().values\n",
    "            lon_max = ds.coords[lon_coord].max().values\n",
    "            print(f\"Longitude range: {lon_min} to {lon_max}\")\n",
    "        \n",
    "        # Handle projected coordinates (if using OSGB or similar)\n",
    "        if 'projection' in str(ds.coords) or lat_coord in ['y', 'projection_y_coordinate']:\n",
    "            print(\"\\nDetected projected coordinates (likely OSGB)\")\n",
    "            print(\"You may need to convert London lat/lon to the projection coordinates\")\n",
    "            print(\"For now, finding nearest grid point in projected space...\")\n",
    "        \n",
    "        # Find nearest grid point to London\n",
    "        if lat_coord in ds.coords and lon_coord in ds.coords:\n",
    "            lat_idx = np.abs(ds.coords[lat_coord] - london_lat).argmin()\n",
    "            lon_idx = np.abs(ds.coords[lon_coord] - london_lon).argmin()\n",
    "            \n",
    "            actual_lat = ds.coords[lat_coord][lat_idx].values\n",
    "            actual_lon = ds.coords[lon_coord][lon_idx].values\n",
    "            \n",
    "            print(f\"\\nTarget coordinates: ({london_lat}, {london_lon})\")\n",
    "            print(f\"Nearest grid point: ({actual_lat}, {actual_lon})\")\n",
    "        else:\n",
    "            print(\"Could not find coordinate values for nearest point calculation\")\n",
    "            return None\n",
    "        \n",
    "        # Extract data for London coordinates\n",
    "        selection_dict = {lat_coord: lat_idx, lon_coord: lon_idx}\n",
    "        london_data = ds.isel(selection_dict)\n",
    "        \n",
    "        print(f\"\\nExtracted data variables: {list(london_data.data_vars)}\")\n",
    "        \n",
    "        # Convert to pandas DataFrame\n",
    "        df_list = []\n",
    "        \n",
    "        for var_name in london_data.data_vars:\n",
    "            var_data = london_data[var_name]\n",
    "            print(f\"\\nProcessing variable: {var_name}\")\n",
    "            print(f\"Variable dimensions: {var_data.dims}\")\n",
    "            print(f\"Variable shape: {var_data.shape}\")\n",
    "            \n",
    "            # Get variable attributes\n",
    "            if hasattr(var_data, 'attrs'):\n",
    "                print(f\"Variable attributes: {var_data.attrs}\")\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            if time_coord and time_coord in var_data.dims:\n",
    "                # Time series data\n",
    "                df_var = var_data.to_dataframe().reset_index()\n",
    "                \n",
    "                # Rename the variable column to be more descriptive\n",
    "                if var_name in df_var.columns:\n",
    "                    # Keep original variable name but add units if available\n",
    "                    units = var_data.attrs.get('units', '')\n",
    "                    long_name = var_data.attrs.get('long_name', var_name)\n",
    "                    \n",
    "                    col_name = var_name\n",
    "                    if units:\n",
    "                        col_name += f\"_{units}\".replace(\" \", \"_\")\n",
    "                    \n",
    "                    df_var = df_var.rename(columns={var_name: col_name})\n",
    "                    \n",
    "                    # Add metadata as separate columns\n",
    "                    df_var[f'{var_name}_long_name'] = long_name\n",
    "                    df_var[f'{var_name}_units'] = units\n",
    "                \n",
    "                df_list.append(df_var)\n",
    "            else:\n",
    "                # Single value or no time dimension\n",
    "                df_var = pd.DataFrame({\n",
    "                    var_name: [var_data.values],\n",
    "                    'latitude': [actual_lat],\n",
    "                    'longitude': [actual_lon]\n",
    "                })\n",
    "                \n",
    "                # Add metadata\n",
    "                if hasattr(var_data, 'attrs'):\n",
    "                    for attr_name, attr_value in var_data.attrs.items():\n",
    "                        df_var[f'{var_name}_{attr_name}'] = str(attr_value)\n",
    "                \n",
    "                df_list.append(df_var)\n",
    "        \n",
    "        # Combine all variables into one DataFrame\n",
    "        if len(df_list) > 1:\n",
    "            # Find common columns for merging\n",
    "            df = df_list[0]\n",
    "            for df_var in df_list[1:]:\n",
    "                common_cols = set(df.columns) & set(df_var.columns)\n",
    "                if common_cols and len(common_cols) > 1:  # More than just the variable column\n",
    "                    df = pd.merge(df, df_var, on=list(common_cols), how='outer')\n",
    "                else:\n",
    "                    # Concatenate side by side if same length\n",
    "                    if len(df) == len(df_var):\n",
    "                        df = pd.concat([df, df_var], axis=1)\n",
    "                    else:\n",
    "                        print(f\"Warning: Cannot merge {df_var.columns} - different lengths\")\n",
    "        else:\n",
    "            df = df_list[0] if df_list else pd.DataFrame()\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"Warning: No data extracted\")\n",
    "            return None\n",
    "        \n",
    "        # Add location metadata\n",
    "        df['actual_latitude'] = actual_lat\n",
    "        df['actual_longitude'] = actual_lon\n",
    "        df['target_latitude'] = london_lat\n",
    "        df['target_longitude'] = london_lon\n",
    "        df['coordinate_system'] = f\"{lat_coord}, {lon_coord}\"\n",
    "        \n",
    "        # Sort by time if time column exists\n",
    "        time_cols = [col for col in df.columns if any(t in col.lower() for t in ['time', 'date', 'year'])]\n",
    "        if time_cols:\n",
    "            df = df.sort_values(time_cols[0])\n",
    "            print(f\"Sorted by time column: {time_cols[0]}\")\n",
    "        \n",
    "        print(f\"\\nFinal DataFrame shape: {df.shape}\")\n",
    "        print(f\"DataFrame columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Show data types\n",
    "        print(f\"\\nData types:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"  {col}: {df[col].dtype}\")\n",
    "        \n",
    "        print(f\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        if len(df) > 5:\n",
    "            print(f\"\\nLast few rows:\")\n",
    "            print(df.tail())\n",
    "        \n",
    "        # Save to CSV\n",
    "        if output_csv_path is None:\n",
    "            output_csv_path = Path(nc_file_path).stem + \"_london_precipitation.csv\"\n",
    "        \n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData saved to: {output_csv_path}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing NetCDF file: {e}\")\n",
    "        print(\"Make sure you have the required packages installed:\")\n",
    "        print(\"pip install xarray netcdf4 pandas numpy\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # Your specific file\n",
    "    nc_file = \"./data/ERA5/rainfall_hadukgrid_uk_1km_ann-30y_196101-199012.nc\"\n",
    "    output_csv = \"london_rainfall_1961_1990.csv\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not Path(nc_file).exists():\n",
    "        print(f\"NetCDF file not found: {nc_file}\")\n",
    "        print(\"Please make sure the file is in the current directory or update the path.\")\n",
    "        return\n",
    "    \n",
    "    # Process the file\n",
    "    try:\n",
    "        df = netcdf_to_csv_london(nc_file, output_csv)\n",
    "        \n",
    "        if df is not None:\n",
    "            # Display summary statistics\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"SUMMARY\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"Total records: {len(df)}\")\n",
    "            \n",
    "            # Show time range if available\n",
    "            time_cols = [col for col in df.columns if any(t in col.lower() for t in ['time', 'date', 'year'])]\n",
    "            if time_cols:\n",
    "                time_col = time_cols[0]\n",
    "                print(f\"Time range: {df[time_col].min()} to {df[time_col].max()}\")\n",
    "            \n",
    "            # Show precipitation statistics\n",
    "            precip_cols = [col for col in df.columns if any(p in col.lower() for p in ['rain', 'precip', 'precipitation'])]\n",
    "            if precip_cols:\n",
    "                print(f\"\\nPrecipitation statistics for {precip_cols[0]}:\")\n",
    "                print(df[precip_cols[0]].describe())\n",
    "            \n",
    "            # Load the CSV back into pandas to verify\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"VERIFICATION - Loading CSV back into pandas:\")\n",
    "            print(\"=\"*60)\n",
    "            df_loaded = pd.read_csv(output_csv)\n",
    "            print(f\"Loaded DataFrame shape: {df_loaded.shape}\")\n",
    "            print(\"Sample of loaded data:\")\n",
    "            print(df_loaded.head(3))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process NetCDF file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d75c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f35646c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac3ca4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47bcf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygrib\n",
    "\n",
    "# Open GRIB file\n",
    "grbs = pygrib.open('./data/era5_monthly_averaged_data.grib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f302de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grib_file = './data/era5_monthly_averaged_data.grib'\n",
    "# grbs2 = xr.open_dataset(grib_file, engine='cfgrib') #, backend_kwargs=backend_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e323972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xarray as xr\n",
    "\n",
    "# grib_file = './data/era5_monthly_averaged_data.grib'\n",
    "\n",
    "# # Try the first grid resolution\n",
    "# try:\n",
    "#     grbs2 = xr.open_dataset(grib_file, \n",
    "#                            engine='cfgrib',\n",
    "#                            backend_kwargs={'filter_by_keys': {'numberOfPoints': 826}})\n",
    "#     print(\"Successfully opened with 826 grid points\")\n",
    "#     print(f\"Variables: {list(grbs2.data_vars)}\")\n",
    "#     print(f\"Shape: {grbs2.dims}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Failed with 826 points: {e}\")\n",
    "\n",
    "# # Try the second grid resolution\n",
    "# try:\n",
    "#     grbs2_alt = xr.open_dataset(grib_file, \n",
    "#                                engine='cfgrib',\n",
    "#                                backend_kwargs={'filter_by_keys': {'numberOfPoints': 207}})\n",
    "#     print(\"Successfully opened with 207 grid points\")\n",
    "#     print(f\"Variables: {list(grbs2_alt.data_vars)}\")\n",
    "#     print(f\"Shape: {grbs2_alt.dims}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Failed with 207 points: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641296a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grib_file = './data/era5_monthly_averaged_data.grib'\n",
    "\n",
    "# # Define all combinations to try\n",
    "# filter_combinations = [\n",
    "#     {'stepType': 'avgad', 'numberOfPoints': 826},\n",
    "#     {'stepType': 'avgas', 'numberOfPoints': 826},\n",
    "#     {'stepType': 'avgad', 'numberOfPoints': 207},\n",
    "#     {'stepType': 'avgas', 'numberOfPoints': 207}\n",
    "# ]\n",
    "\n",
    "# datasets = {}\n",
    "\n",
    "# for i, filters in enumerate(filter_combinations):\n",
    "#     try:\n",
    "#         ds = xr.open_dataset(grib_file, \n",
    "#                            engine='cfgrib',\n",
    "#                            backend_kwargs={'filter_by_keys': filters})\n",
    "        \n",
    "#         dataset_name = f\"{filters['stepType']}_{filters['numberOfPoints']}pts\"\n",
    "#         datasets[dataset_name] = ds\n",
    "        \n",
    "#         print(f\"\\n✅ Successfully opened: {dataset_name}\")\n",
    "#         print(f\"   Variables: {list(ds.data_vars)}\")\n",
    "#         print(f\"   Dimensions: {ds.dims}\")\n",
    "#         print(f\"   Coordinates: {list(ds.coords)}\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Failed {filters}: {str(e)[:100]}...\")\n",
    "\n",
    "# print(f\"\\n📊 Total datasets loaded: {len(datasets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41b4e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xarray as xr\n",
    "# import pandas as pd\n",
    "\n",
    "# grib_file = './data/era5_monthly_averaged_data.grib'\n",
    "\n",
    "# def process_all_era5_data(grib_file):\n",
    "#     \"\"\"Process all combinations and return organized data\"\"\"\n",
    "    \n",
    "#     filter_combinations = [\n",
    "#         {'stepType': 'avgad', 'numberOfPoints': 826},\n",
    "#         {'stepType': 'avgas', 'numberOfPoints': 826},\n",
    "#         {'stepType': 'avgad', 'numberOfPoints': 207},\n",
    "#         {'stepType': 'avgas', 'numberOfPoints': 207}\n",
    "#     ]\n",
    "    \n",
    "#     all_data = {}\n",
    "    \n",
    "#     for filters in filter_combinations:\n",
    "#         try:\n",
    "#             # Open dataset\n",
    "#             ds = xr.open_dataset(grib_file, \n",
    "#                                engine='cfgrib',\n",
    "#                                backend_kwargs={'filter_by_keys': filters})\n",
    "            \n",
    "#             # Create descriptive name\n",
    "#             step_type = filters['stepType']\n",
    "#             grid_size = filters['numberOfPoints']\n",
    "#             dataset_name = f\"{step_type}_{grid_size}pts\"\n",
    "            \n",
    "#             # Convert to DataFrame\n",
    "#             df = ds.to_dataframe().reset_index()\n",
    "            \n",
    "#             # Add metadata\n",
    "#             df['step_type'] = step_type\n",
    "#             df['grid_points'] = grid_size\n",
    "#             df['data_type'] = 'accumulated' if step_type == 'avgad' else 'instantaneous'\n",
    "            \n",
    "#             all_data[dataset_name] = {\n",
    "#                 'dataset': ds,\n",
    "#                 'dataframe': df,\n",
    "#                 'variables': list(ds.data_vars),\n",
    "#                 'shape': df.shape\n",
    "#             }\n",
    "            \n",
    "#             print(f\"✅ {dataset_name}:\")\n",
    "#             print(f\"   Shape: {df.shape}\")\n",
    "#             print(f\"   Variables: {list(ds.data_vars)}\")\n",
    "#             print(f\"   Sample data types: {df.dtypes.to_dict()}\")\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ Failed {filters}: {e}\")\n",
    "    \n",
    "#     return all_data\n",
    "\n",
    "# # Process all data\n",
    "# all_era5_data = process_all_era5_data(grib_file)\n",
    "\n",
    "# # Save each dataset to CSV\n",
    "# for name, data_info in all_era5_data.items():\n",
    "#     df = data_info['dataframe']\n",
    "#     filename = f'era5_{name}.csv'\n",
    "#     df.to_csv(filename, index=False)\n",
    "#     print(f\"💾 Saved {name} to {filename}\")\n",
    "\n",
    "# # Display summary\n",
    "# print(f\"\\n📋 Summary:\")\n",
    "# for name, data_info in all_era5_data.items():\n",
    "#     print(f\"{name}: {data_info['shape'][0]} rows, Variables: {data_info['variables']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfede07a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826962af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef92cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bce793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5fa512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f956cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open the NetCDF file\n",
    "# ds = xr.open_dataset(\"./data/london_test_2.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee382131",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ab6f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rain = ds['tp'].sel(latitude=51.5, longitude=0.0, method='nearest')\n",
    "rain.plot()\n",
    "plt.title('Daily Precipitation in London')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b3a417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select time series at closest grid point to London\n",
    "rain_series = ds['tp'].sel(latitude=51.5, longitude=0.0, method=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb8bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame\n",
    "df = rain_series.to_dataframe().reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9e02c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RR_mm'] = df['tp'] * 1000\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff307884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_manipulation import transform_data_datetime\n",
    "\n",
    "#london_data = transform_data_datetime(df=df.rename(columns={'valid_time' : 'DATE'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c770df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# london_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80c38dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(london_data\n",
    " .groupby(['year', 'month'], observed=True)['RR_mm']\n",
    " .sum()\n",
    " .reset_index()\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efbbb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_data['RR_mm'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d072b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2efb993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74a5979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fab6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cee839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygrib\n",
    "\n",
    "# Open GRIB file\n",
    "grbs = pygrib.open('./data/era5_hourly_data_on_single_levels.grib')\n",
    "\n",
    "# # List all messages/variables\n",
    "# for grb in grbs:\n",
    "#     print(grb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22c919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Store all data\n",
    "# all_data = []\n",
    "    \n",
    "#     # Process each message\n",
    "# for grb in grbs:\n",
    "#         # Filter by variable names if specified\n",
    "#     # if variable_names and grb.shortName not in variable_names:\n",
    "#         continue\n",
    "            \n",
    "#         # Get data and coordinates\n",
    "#     data, lats, lons = grb.data()\n",
    "        \n",
    "#         # Create coordinate meshgrids\n",
    "#     lat_grid, lon_grid = np.meshgrid(lats, lons, indexing='ij')\n",
    "        \n",
    "#         # Flatten arrays\n",
    "#     data_flat = data.flatten()\n",
    "#     lat_flat = lat_grid.flatten()\n",
    "#     lon_flat = lon_grid.flatten()\n",
    "        \n",
    "#         # Get metadata\n",
    "#     try:\n",
    "#         valid_date = grb.validDate\n",
    "#     except:\n",
    "#         valid_date = None\n",
    "            \n",
    "#     try:\n",
    "#         forecast_time = grb.forecastTime\n",
    "#     except:\n",
    "#         forecast_time = 0\n",
    "            \n",
    "#         # Create temporary DataFrame for this message\n",
    "#     temp_df = pd.DataFrame({\n",
    "#         'latitude': lat_flat,\n",
    "#         'longitude': lon_flat,\n",
    "#         'value': data_flat,\n",
    "#         'variable': grb.shortName,\n",
    "#         'long_name': getattr(grb, 'name', 'Unknown'),\n",
    "#         'units': getattr(grb, 'units', 'Unknown'),\n",
    "#         'level': getattr(grb, 'level', 0),\n",
    "#         'valid_date': valid_date,\n",
    "#         'forecast_time': forecast_time\n",
    "#     })\n",
    "        \n",
    "#     all_data.append(temp_df)\n",
    "    \n",
    "# grbs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92df5416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def grib_to_dataframe_pygrib(grib_file, variable_names=None):\n",
    "    \"\"\"\n",
    "    Convert GRIB file to pandas DataFrame using pygrib\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    grib_file : str\n",
    "        Path to GRIB file\n",
    "    variable_names : list, optional\n",
    "        List of variable short names to extract (e.g., ['tp', 't2m'])\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with meteorological data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open GRIB file\n",
    "    grbs = pygrib.open(grib_file)\n",
    "    \n",
    "    # Store all data\n",
    "    all_data = []\n",
    "    \n",
    "    # Process each message\n",
    "    for grb in grbs:\n",
    "        # Filter by variable names if specified\n",
    "        if variable_names and grb.shortName not in variable_names:\n",
    "            continue\n",
    "            \n",
    "        # Get data and coordinates\n",
    "        data, lats, lons = grb.data()\n",
    "        \n",
    "        # Create coordinate meshgrids\n",
    "        lat_grid, lon_grid = np.meshgrid(lats, lons, indexing='ij')\n",
    "        \n",
    "        # Flatten arrays\n",
    "        data_flat = data.flatten()\n",
    "        lat_flat = lat_grid.flatten()\n",
    "        lon_flat = lon_grid.flatten()\n",
    "        \n",
    "        print(len(data_flat))\n",
    "        print(len(lat_flat))\n",
    "        print(len(lon_flat))\n",
    "\n",
    "        # Get metadata\n",
    "        try:\n",
    "            valid_date = grb.validDate\n",
    "        except:\n",
    "            valid_date = None\n",
    "            \n",
    "        try:\n",
    "            forecast_time = grb.forecastTime\n",
    "        except:\n",
    "            forecast_time = 0\n",
    "            \n",
    "        # Create temporary DataFrame for this message\n",
    "        print(len(valid_date))\n",
    "        print(len(forecast_time))\n",
    "\n",
    "        temp_df = pd.DataFrame({\n",
    "            'latitude': lat_flat,\n",
    "            'longitude': lon_flat,\n",
    "            'value': data_flat,\n",
    "            'variable': grb.shortName,\n",
    "            'long_name': getattr(grb, 'name', 'Unknown'),\n",
    "            'units': getattr(grb, 'units', 'Unknown'),\n",
    "            'level': getattr(grb, 'level', 0),\n",
    "            'valid_date': valid_date,\n",
    "            'forecast_time': forecast_time\n",
    "        })\n",
    "        \n",
    "        all_data.append(temp_df)\n",
    "    \n",
    "    grbs.close()\n",
    "    \n",
    "    # Combine all data\n",
    "    if all_data:\n",
    "        df = pd.concat(all_data, ignore_index=True)\n",
    "        \n",
    "        # Remove invalid data points\n",
    "        df = df.dropna(subset=['value'])\n",
    "        \n",
    "        print(f\"Dataset shape: {df.shape}\")\n",
    "        print(f\"Variables found: {df['variable'].unique()}\")\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print(\"No data found in GRIB file\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Extract all variables\n",
    "    df = grib_to_dataframe_pygrib('./data/era5_hourly_data_on_single_levels.grib')\n",
    "    \n",
    "    # Extract specific variables only\n",
    "    df_specific = grib_to_dataframe_pygrib('./data/era5_hourly_data_on_single_levels.grib', \n",
    "                                          variable_names=['tp', 't2m', 'r'])\n",
    "    \n",
    "    if not df.empty:\n",
    "        print(\"\\nFirst 5 rows:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Pivot to have variables as columns\n",
    "        df_pivot = df.pivot_table(\n",
    "            index=['latitude', 'longitude', 'valid_date'], \n",
    "            columns='variable', \n",
    "            values='value'\n",
    "        ).reset_index()\n",
    "        \n",
    "        print(\"\\nPivoted data (variables as columns):\")\n",
    "        print(df_pivot.head())\n",
    "        \n",
    "        # Save to CSV\n",
    "        df.to_csv('weather_data_detailed.csv', index=False)\n",
    "        df_pivot.to_csv('weather_data_pivot.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999ef925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb0f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific variable\n",
    "grb = grbs.select(name='Total precipitation')[0]\n",
    "data, lats, lons = grb.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134def1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd9ee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "lons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8d3162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read GRIB file\n",
    "ds = xr.open_dataset('./data/london_data.grib', engine='cfgrib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90964df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to open only GRIB2 messages\n",
    "try:\n",
    "    ds = xr.open_dataset('./data/london_data.grib', \n",
    "                        engine='cfgrib',\n",
    "                        backend_kwargs={'filter_by_keys': {'edition': 2}})\n",
    "    print(\"Successfully opened GRIB2 data\")\n",
    "except:\n",
    "    # If that fails, try GRIB1\n",
    "    ds = xr.open_dataset('./data/london_data.grib', \n",
    "                        engine='cfgrib',\n",
    "                        backend_kwargs={'filter_by_keys': {'edition': 1}})\n",
    "    print(\"Successfully opened GRIB1 data\")\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c9e3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset\n",
    "print(\"Variables:\", list(ds.data_vars))\n",
    "print(\"Coordinates:\", list(ds.coords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccf5c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['surface'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be9f753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
