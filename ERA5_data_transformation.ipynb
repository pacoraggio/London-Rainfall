{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96b58b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.data_manipulation import transform_data_datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c5a2dc",
   "metadata": {},
   "source": [
    "# ERA5 datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76f3ab1",
   "metadata": {},
   "source": [
    "Scope of the notebook: once downloaded datasets from Copernicus website, we implement functions to:\n",
    "- Read `grib` files\n",
    "- Produce Hourly, daily, monthly, and yearly dataframes for Total Precipitation and 2 meters Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b580ae",
   "metadata": {},
   "source": [
    "## Grib Files Parameters for London and Puglia region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2897389",
   "metadata": {},
   "source": [
    "Data are downloaded from the [Copernicus Website](https://cds.climate.copernicus.eu/requests?tab=all)\n",
    "\n",
    "### London GRIB grid Parameters\n",
    "\n",
    "For London, UK, you'll want to define a geographical area that encompasses the Greater London region. Here are suitable values for the grid boundaries:\n",
    "- North: 51.7°N\n",
    "- South: 51.3°N\n",
    "- West: -0.5°W\n",
    "- East: 0.3°E\n",
    "These coordinates create a rectangular grid that covers Greater London and a small buffer around it. London's city center is approximately at 51.5°N, 0.1°W, so this grid provides good coverage while keeping the data volume manageable.\n",
    "\n",
    "If you want a smaller area focused just on central London, you could use tighter bounds:\n",
    "\n",
    "North: 51.6°N\n",
    "South: 51.4°N\n",
    "West: -0.3°W\n",
    "East: 0.1°E\n",
    "\n",
    "Or if you need a larger area covering the broader London metropolitan region, you could expand to:\n",
    "\n",
    "North: 52.0°N\n",
    "South: 51.0°N\n",
    "West: -1.0°W\n",
    "East: 1.0°E\n",
    "\n",
    "The first set of coordinates (51.7°N, 51.3°N, -0.5°W, 0.3°E) is probably your best bet for most applications - it captures London proper without being unnecessarily large.\n",
    "\n",
    "\n",
    "```\n",
    "North: 51.6\n",
    "West:  -0.2\n",
    "South: 51.4\n",
    "East:   0.1\n",
    "```\n",
    "\n",
    "### Puglia Region GRIB grid Parameters\n",
    "\n",
    "For comparing London and Puglia climate data, here's a suitable approach:\n",
    "\n",
    "- Grid Selection Strategy\n",
    "Option 1: Two separate queries (Recommended)\n",
    "\n",
    "Query London area: North 51.7°N, South 51.3°N, West -0.5°W, East 0.3°E\n",
    "Query Puglia area: North 41.9°N, South 39.8°N, West 15.5°E, East 18.5°E\n",
    "\n",
    "Option 2: Single large query covering both regions\n",
    "\n",
    "North 51.7°N, South 39.8°N, West -0.5°W, East 18.5°E\n",
    "This creates a massive grid covering most of Europe, resulting in much larger files\n",
    "\n",
    "I'd recommend Option 1 as it's more efficient and gives you focused data for each region.\n",
    "Puglia Grid Coordinates\n",
    "Puglia (Apulia) occupies the \"heel\" of Italy's boot:\n",
    "\n",
    "North: 41.9°N (around Foggia)\n",
    "South: 39.8°N (around Santa Maria di Leuca)\n",
    "West: 15.5°E (around Bari)\n",
    "East: 18.5°E (Adriatic coast)\n",
    "\n",
    "Data Extraction from GRIB Files\n",
    "Once you have the .grib files, extract data using these approaches:\n",
    "1. Point-based comparison:\n",
    "\n",
    "Extract data for specific coordinates (e.g., London center: 51.5°N, -0.1°W; Bari: 41.1°N, 16.9°E)\n",
    "Use tools like grib_get_data or Python libraries (xarray, pygrib)\n",
    "\n",
    "2. Area-averaged comparison:\n",
    "\n",
    "Calculate spatial averages for each region\n",
    "This smooths out local variations and gives regional climate signals\n",
    "\n",
    "3. Representative city comparison:\n",
    "\n",
    "London: Extract nearest grid point to 51.5°N, -0.1°W\n",
    "Puglia: Extract for major cities like Bari (41.1°N, 16.9°E) or Lecce (40.4°N, 18.2°E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4252fa3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d315208",
   "metadata": {},
   "source": [
    "> grib_file = \"./data/ERA5/ERA5_hourly_data_on_single_levels_from_1940_to_present.grib\"  # Replace with your GRIB file path\n",
    "- output_csv = \"./output/ERA5_hourly_data_on_single_levels_from_1940_to_present.csv\"  # Replace with desired output path\n",
    "- hourly data from 2022 to 2024\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c9142c",
   "metadata": {},
   "source": [
    "### GRIB to csv conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aec339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2: first working version\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import cfgrib\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def grib_to_csv(grib_file_path,\n",
    "                city_lat,\n",
    "                city_lon,\n",
    "                output_csv_path='./output/',\n",
    "                city_name = 'london',\n",
    "                print_debug=False,\n",
    "                ):\n",
    "    \"\"\"\n",
    "    Read GRIB file, extract data for London coordinates, and save as CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    grib_file_path (str): Path to the GRIB file\n",
    "    output_csv_path (str): Path for output CSV file (optional)\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with the extracted data\n",
    "    \"\"\"\n",
    "    if print_debug:\n",
    "        print(f\"Reading GRIB file: {grib_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Open GRIB file with xarray\n",
    "        print('breakpoint 0') \n",
    "        datasets = cfgrib.open_datasets(grib_file_path)\n",
    "        for i, ds in enumerate(datasets):\n",
    "            print(f\"Dataset {i}:\")\n",
    "            print(f\"Variables: {list(ds.data_vars.keys())}\")\n",
    "            print(f\"Time range: {ds.time.min().values} to {ds.time.max().values}\")\n",
    "            print(\"---\")\n",
    "        \n",
    "        ds = xr.open_dataset(grib_file_path, engine='cfgrib')\n",
    "        print('breakpoint 1')        \n",
    "        # Check longitude format (0-360 vs -180-180)\n",
    "        city_min = ds.longitude.min().values\n",
    "        city_max = ds.longitude.max().values\n",
    "        if print_debug:\n",
    "            print(f\"Longitude range: {city_min} to {city_max}\")\n",
    "        \n",
    "        target_city = city_lon\n",
    "        if print_debug:\n",
    "            print(f\"Using -180-180 longitude format: {target_city}\")\n",
    "        \n",
    "        # Find nearest grid point to London\n",
    "        lat_idx = np.abs(ds.latitude - city_lat).argmin()\n",
    "        lon_idx = np.abs(ds.longitude - target_city).argmin()\n",
    "        \n",
    "        actual_lat = ds.latitude[lat_idx].values\n",
    "        actual_lon = ds.longitude[lon_idx].values\n",
    "        \n",
    "        if print_debug:\n",
    "            print(f\"Target coordinates: ({city_lat}, {target_city})\")\n",
    "            print(f\"Nearest grid point: ({actual_lat}, {actual_lon})\")\n",
    "        \n",
    "        # Extract data for London coordinates\n",
    "        city_data = ds.isel(latitude=lat_idx, longitude=lon_idx)\n",
    "        \n",
    "        # Convert to pandas DataFrame\n",
    "        df_list = []\n",
    "        \n",
    "        for var_name in city_data.data_vars:\n",
    "            var_data = city_data[var_name]\n",
    "            \n",
    "            # Handle different time dimensions\n",
    "            if 'time' in var_data.dims:\n",
    "                # Create DataFrame with time index\n",
    "                df_var = var_data.to_dataframe().reset_index()\n",
    "                df_var = df_var.rename(columns={var_name: var_name})\n",
    "                df_list.append(df_var)\n",
    "            elif 'valid_time' in var_data.dims:\n",
    "                # Handle valid_time dimension\n",
    "                df_var = var_data.to_dataframe().reset_index()\n",
    "                df_var = df_var.rename(columns={var_name: var_name})\n",
    "                df_list.append(df_var)\n",
    "            else:\n",
    "                # Single value variables\n",
    "                df_var = pd.DataFrame({\n",
    "                    var_name: [var_data.values],\n",
    "                    'latitude': [actual_lat],\n",
    "                    'longitude': [actual_lon]\n",
    "                })\n",
    "                df_list.append(df_var)\n",
    "        \n",
    "        # Combine all variables into one DataFrame\n",
    "        if len(df_list) > 1:\n",
    "            # Merge on common columns (time, latitude, longitude)\n",
    "            df = df_list[0]\n",
    "            for df_var in df_list[1:]:\n",
    "                common_cols = set(df.columns) & set(df_var.columns)\n",
    "                if common_cols:\n",
    "                    df = pd.merge(df, df_var, on=list(common_cols), how='outer')\n",
    "                else:\n",
    "                    # If no common columns, concatenate side by side\n",
    "                    df = pd.concat([df, df_var], axis=1)\n",
    "        else:\n",
    "            df = df_list[0]\n",
    "        \n",
    "        df['target_latitude'] = city_lat\n",
    "        df['target_longitude'] = target_city\n",
    "        \n",
    "        # Sort by time if time column exists\n",
    "        time_cols = [col for col in df.columns if 'time' in col.lower()]\n",
    "        if time_cols:\n",
    "            time_col = time_cols[0]\n",
    "            df = df.sort_values(time_cols[0])\n",
    "            if print_debug:\n",
    "                print(f\"****\\n - inside csv Time range: {df[time_col].min()} to {df[time_col].max()}\")\n",
    "            min_string = df[time_col].min().strftime('%Y%m%d')\n",
    "            max_string = df[time_col].max().strftime('%Y%m%d')\n",
    "            if print_debug:\n",
    "                print(f\" - min time as string: {min_string}\")\n",
    "                print(f\" - max time as string: {max_string}\")\n",
    "            output_csv_name = list(ds.data_vars)[0] + '_' + min_string + '_' + max_string\n",
    "            if print_debug:\n",
    "                print(output_csv_name)\n",
    "        \n",
    "        if print_debug:\n",
    "            print(f\"DataFrame shape: {df.shape}\")\n",
    "            print(f\"DataFrame columns: {list(df.columns)}\")\n",
    "            print(\"grib to csv\")\n",
    "            print(f\"ds.data_vars dtype: {type(ds.data_vars)}, df.data_vars name: {list(ds.data_vars)[0]}\")\n",
    "        # print(\"\\nFirst few rows:\")\n",
    "        # print(df.head())\n",
    "        \n",
    "        # Save to CSV\n",
    "        if output_csv_path is None:\n",
    "            output_csv_path = Path(grib_file_path).stem + \"_city_data.csv\"\n",
    "        \n",
    "        output_csv_path = output_csv_path + city_name + '_' + output_csv_name + '.csv'\n",
    "        if print_debug:\n",
    "            print(f\"Saving DataFrame to: {output_csv_path}\")\n",
    "        \n",
    "\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData saved to: {output_csv_path}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing GRIB file: {e}\")\n",
    "        print(\"Make sure you have the required packages installed:\")\n",
    "        print(\"pip install xarray cfgrib pandas numpy\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "581adb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_grib_file(grib_file_path,\n",
    "                      output_csv_path='./output/',\n",
    "                      city_name='london',\n",
    "                      city_lat=51.5074,  # Default to London latitude\n",
    "                      city_lon=-0.1278,\n",
    "                      print_debug = False):  # Default to London longitude\n",
    "    \n",
    "    grib_file = grib_file_path  # Replace with your GRIB file path\n",
    "    output_csv_city = city_name  # Replace with desired output path\n",
    "    output_csv_path = output_csv_path  # Directory to save the output CSV\n",
    "\n",
    "    # Check if file exists\n",
    "    if not Path(grib_file).exists():\n",
    "        print(f\"GRIB file not found: {grib_file}\")\n",
    "        print(\"Please update the 'grib_file' variable with the correct path to your GRIB file.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        df = grib_to_csv(grib_file,\n",
    "                         city_lat=city_lat,\n",
    "                         city_lon=-city_lon, \n",
    "                         output_csv_path=output_csv_path, \n",
    "                         city_name=output_csv_city, \n",
    "                         print_debug=print_debug)\n",
    "        \n",
    "        # Display summary statistics\n",
    "        if print_debug:\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"SUMMARY\")\n",
    "            print(\"=\"*50)\n",
    "            print(f\"Total records: {len(df)}\")\n",
    "        \n",
    "        # Show time range if available\n",
    "        time_cols = [col for col in df.columns if 'time' in col.lower()]\n",
    "        if time_cols:\n",
    "            time_col = time_cols[0]\n",
    "            if print_debug:\n",
    "                print(f\"Time range: {df[time_col].min()} to {df[time_col].max()}\")\n",
    "        \n",
    "        if print_debug:\n",
    "            print(f\"Variables extracted: {[col for col in df.columns if col not in ['latitude', 'longitude', 'actual_latitude', 'actual_longitude', 'target_latitude', 'target_longitude'] + time_cols]}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process GRIB file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38e3e5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breakpoint 0\n",
      "Dataset 0:\n",
      "Variables: ['t2m']\n",
      "Time range: 2005-01-01T00:00:00.000000000 to 2010-12-31T23:00:00.000000000\n",
      "---\n",
      "Dataset 1:\n",
      "Variables: ['tp']\n",
      "Time range: 2004-12-31T18:00:00.000000000 to 2010-12-31T18:00:00.000000000\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skipping variable: paramId==228 shortName='tp'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\dataset.py\", line 725, in build_dataset_components\n",
      "    dict_merge(variables, coord_vars)\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cfgrib\\dataset.py\", line 641, in dict_merge\n",
      "    raise DatasetBuildError(\n",
      "cfgrib.dataset.DatasetBuildError: key present and new value is different: key='time' value=Variable(dimensions=('time',), data=array([1104537600, 1104541200, 1104544800, ..., 1293829200, 1293832800,\n",
      "       1293836400])) new_value=Variable(dimensions=('time',), data=array([1104516000, 1104559200, 1104602400, ..., 1293732000, 1293775200,\n",
      "       1293818400]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breakpoint 1\n",
      "\n",
      "Data saved to: ./output/london_t2m_20050101_20101231.csv\n"
     ]
    }
   ],
   "source": [
    "# Process the file\n",
    "# Bari 41.14043890868558, 16.86619063277282\n",
    "# Lecce 40.35654102220363, 18.17311913754415\n",
    "# London 51.5074, -0.1278\n",
    "\n",
    "file_path = './data/ERA5/data_london_2mt_2005_2010.grib'\n",
    "output_path = './output/'\n",
    "city_name = 'london'\n",
    "city_lat = 51.5074  # London latitude\n",
    "city_lon = -0.1278  # London longitude\n",
    "\n",
    "df_test = process_grib_file(grib_file_path = file_path,\n",
    "                            output_csv_path=output_path,\n",
    "                            city_name=city_name,\n",
    "                            city_lat=city_lat,\n",
    "                            city_lon=city_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2276e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>number</th>\n",
       "      <th>step</th>\n",
       "      <th>surface</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>valid_time</th>\n",
       "      <th>t2m</th>\n",
       "      <th>target_latitude</th>\n",
       "      <th>target_longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.55</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2005-01-01 00:00:00</td>\n",
       "      <td>279.231567</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2005-01-01 01:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.55</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2005-01-01 01:00:00</td>\n",
       "      <td>279.666870</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005-01-01 02:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.55</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2005-01-01 02:00:00</td>\n",
       "      <td>279.765686</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2005-01-01 03:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.55</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2005-01-01 03:00:00</td>\n",
       "      <td>279.600403</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2005-01-01 04:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.55</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2005-01-01 04:00:00</td>\n",
       "      <td>279.259583</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time  number   step  surface  latitude  longitude  \\\n",
       "0 2005-01-01 00:00:00       0 0 days      0.0     51.55       0.25   \n",
       "1 2005-01-01 01:00:00       0 0 days      0.0     51.55       0.25   \n",
       "2 2005-01-01 02:00:00       0 0 days      0.0     51.55       0.25   \n",
       "3 2005-01-01 03:00:00       0 0 days      0.0     51.55       0.25   \n",
       "4 2005-01-01 04:00:00       0 0 days      0.0     51.55       0.25   \n",
       "\n",
       "           valid_time         t2m  target_latitude  target_longitude  \n",
       "0 2005-01-01 00:00:00  279.231567          51.5074            0.1278  \n",
       "1 2005-01-01 01:00:00  279.666870          51.5074            0.1278  \n",
       "2 2005-01-01 02:00:00  279.765686          51.5074            0.1278  \n",
       "3 2005-01-01 03:00:00  279.600403          51.5074            0.1278  \n",
       "4 2005-01-01 04:00:00  279.259583          51.5074            0.1278  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb6f4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the file\n",
    "# Bari 41.14043890868558, 16.86619063277282\n",
    "# Lecce 40.35654102220363, 18.17311913754415\n",
    "# London 51.5074, -0.1278\n",
    "\n",
    "\n",
    "file_path = './data/ERA5/data_london_2mt_2005_2010.grib'\n",
    "output_path = './output/'\n",
    "city_name = 'london'\n",
    "city_lat = 51.5074  # London latitude\n",
    "city_lon = -0.1278  # London longitude\n",
    "\n",
    "df_test= process_grib_file(grib_file_path = file_path,\n",
    "                            output_csv_path=output_path,\n",
    "                            city_name=city_name,\n",
    "                            city_lat=city_lat,\n",
    "                            city_lon=city_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c6fb9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/ERA5/london_tp_2014_2021.grib\n",
      "./data/ERA5/london_tp_2022_2024.grib\n",
      "breakpoint 0\n",
      "Dataset 0:\n",
      "Variables: ['tp']\n",
      "Time range: 2013-12-31T18:00:00.000000000 to 2021-12-31T18:00:00.000000000\n",
      "---\n",
      "breakpoint 1\n",
      "\n",
      "Data saved to: ./output/london_tp_20131231_20211231.csv\n",
      "(70140, 10)\n",
      "breakpoint 0\n",
      "Dataset 0:\n",
      "Variables: ['tp']\n",
      "Time range: 2021-12-31T18:00:00.000000000 to 2024-12-31T18:00:00.000000000\n",
      "---\n",
      "breakpoint 1\n",
      "\n",
      "Data saved to: ./output/london_tp_20211231_20241231.csv\n",
      "(26316, 10)\n",
      "(96456, 10)\n"
     ]
    }
   ],
   "source": [
    "input_path = './data/ERA5/'\n",
    "file_names = ['london_tp_2014_2021.grib','london_tp_2022_2024.grib']\n",
    "\n",
    "for file_name in file_names:\n",
    "    print(input_path + file_name)\n",
    "\n",
    "df_data = []\n",
    "for file_name in file_names:\n",
    "    df = process_grib_file(grib_file_path=input_path + file_name,\n",
    "                           output_csv_path=output_path,\n",
    "                           city_name=city_name,\n",
    "                           city_lat=city_lat,\n",
    "                           city_lon=city_lon)\n",
    "    print(df.shape)\n",
    "    df_data.append(df)\n",
    "# Concatenate all DataFrames\n",
    "df_combined = pd.concat(df_data, ignore_index=True)\n",
    "print(df_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58fdd2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.sort_values(by='valid_time', inplace=True)\n",
    "df_combined.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72521b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>step</th>\n",
       "      <th>number</th>\n",
       "      <th>surface</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>valid_time</th>\n",
       "      <th>tp</th>\n",
       "      <th>target_latitude</th>\n",
       "      <th>target_longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-12-31 18:00:00</td>\n",
       "      <td>0 days 01:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.4</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2013-12-31 19:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-12-31 18:00:00</td>\n",
       "      <td>0 days 02:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.4</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2013-12-31 20:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-12-31 18:00:00</td>\n",
       "      <td>0 days 03:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.4</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2013-12-31 21:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-12-31 18:00:00</td>\n",
       "      <td>0 days 04:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.4</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2013-12-31 22:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-12-31 18:00:00</td>\n",
       "      <td>0 days 05:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.4</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2013-12-31 23:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70128</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 01:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2021-12-31 19:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70130</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 02:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2021-12-31 20:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70132</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 03:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2021-12-31 21:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70134</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 04:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2021-12-31 22:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70136</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 05:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2021-12-31 23:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70139</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 06:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.4</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2022-01-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70141</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 07:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.4</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2022-01-01 01:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70143</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 08:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.4</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2022-01-01 02:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70145</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 09:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.4</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2022-01-01 03:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70147</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 10:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.4</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2022-01-01 04:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70149</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 11:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.4</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2022-01-01 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70150</th>\n",
       "      <td>2021-12-31 18:00:00</td>\n",
       "      <td>0 days 12:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.4</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2022-01-01 06:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96449</th>\n",
       "      <td>2024-12-31 18:00:00</td>\n",
       "      <td>0 days 06:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2025-01-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96450</th>\n",
       "      <td>2024-12-31 18:00:00</td>\n",
       "      <td>0 days 07:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2025-01-01 01:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96451</th>\n",
       "      <td>2024-12-31 18:00:00</td>\n",
       "      <td>0 days 08:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2025-01-01 02:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96452</th>\n",
       "      <td>2024-12-31 18:00:00</td>\n",
       "      <td>0 days 09:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2025-01-01 03:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96453</th>\n",
       "      <td>2024-12-31 18:00:00</td>\n",
       "      <td>0 days 10:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2025-01-01 04:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96454</th>\n",
       "      <td>2024-12-31 18:00:00</td>\n",
       "      <td>0 days 11:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2025-01-01 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96455</th>\n",
       "      <td>2024-12-31 18:00:00</td>\n",
       "      <td>0 days 12:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2025-01-01 06:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     time            step  number  surface  latitude  \\\n",
       "0     2013-12-31 18:00:00 0 days 01:00:00       0      0.0      51.4   \n",
       "1     2013-12-31 18:00:00 0 days 02:00:00       0      0.0      51.4   \n",
       "2     2013-12-31 18:00:00 0 days 03:00:00       0      0.0      51.4   \n",
       "3     2013-12-31 18:00:00 0 days 04:00:00       0      0.0      51.4   \n",
       "4     2013-12-31 18:00:00 0 days 05:00:00       0      0.0      51.4   \n",
       "70128 2021-12-31 18:00:00 0 days 01:00:00       0      0.0      51.6   \n",
       "70130 2021-12-31 18:00:00 0 days 02:00:00       0      0.0      51.6   \n",
       "70132 2021-12-31 18:00:00 0 days 03:00:00       0      0.0      51.6   \n",
       "70134 2021-12-31 18:00:00 0 days 04:00:00       0      0.0      51.6   \n",
       "70136 2021-12-31 18:00:00 0 days 05:00:00       0      0.0      51.6   \n",
       "70139 2021-12-31 18:00:00 0 days 06:00:00       0      0.0      51.4   \n",
       "70141 2021-12-31 18:00:00 0 days 07:00:00       0      0.0      51.4   \n",
       "70143 2021-12-31 18:00:00 0 days 08:00:00       0      0.0      51.4   \n",
       "70145 2021-12-31 18:00:00 0 days 09:00:00       0      0.0      51.4   \n",
       "70147 2021-12-31 18:00:00 0 days 10:00:00       0      0.0      51.4   \n",
       "70149 2021-12-31 18:00:00 0 days 11:00:00       0      0.0      51.4   \n",
       "70150 2021-12-31 18:00:00 0 days 12:00:00       0      0.0      51.4   \n",
       "96449 2024-12-31 18:00:00 0 days 06:00:00       0      0.0      51.6   \n",
       "96450 2024-12-31 18:00:00 0 days 07:00:00       0      0.0      51.6   \n",
       "96451 2024-12-31 18:00:00 0 days 08:00:00       0      0.0      51.6   \n",
       "96452 2024-12-31 18:00:00 0 days 09:00:00       0      0.0      51.6   \n",
       "96453 2024-12-31 18:00:00 0 days 10:00:00       0      0.0      51.6   \n",
       "96454 2024-12-31 18:00:00 0 days 11:00:00       0      0.0      51.6   \n",
       "96455 2024-12-31 18:00:00 0 days 12:00:00       0      0.0      51.6   \n",
       "\n",
       "       longitude          valid_time  tp  target_latitude  target_longitude  \n",
       "0           0.05 2013-12-31 19:00:00 NaN          51.5074            0.1278  \n",
       "1           0.05 2013-12-31 20:00:00 NaN          51.5074            0.1278  \n",
       "2           0.05 2013-12-31 21:00:00 NaN          51.5074            0.1278  \n",
       "3           0.05 2013-12-31 22:00:00 NaN          51.5074            0.1278  \n",
       "4           0.05 2013-12-31 23:00:00 NaN          51.5074            0.1278  \n",
       "70128       0.05 2021-12-31 19:00:00 NaN          51.5074            0.1278  \n",
       "70130       0.05 2021-12-31 20:00:00 NaN          51.5074            0.1278  \n",
       "70132       0.05 2021-12-31 21:00:00 NaN          51.5074            0.1278  \n",
       "70134       0.05 2021-12-31 22:00:00 NaN          51.5074            0.1278  \n",
       "70136       0.05 2021-12-31 23:00:00 NaN          51.5074            0.1278  \n",
       "70139       0.05 2022-01-01 00:00:00 NaN          51.5074            0.1278  \n",
       "70141       0.05 2022-01-01 01:00:00 NaN          51.5074            0.1278  \n",
       "70143       0.05 2022-01-01 02:00:00 NaN          51.5074            0.1278  \n",
       "70145       0.05 2022-01-01 03:00:00 NaN          51.5074            0.1278  \n",
       "70147       0.05 2022-01-01 04:00:00 NaN          51.5074            0.1278  \n",
       "70149       0.05 2022-01-01 05:00:00 NaN          51.5074            0.1278  \n",
       "70150       0.05 2022-01-01 06:00:00 NaN          51.5074            0.1278  \n",
       "96449       0.05 2025-01-01 00:00:00 NaN          51.5074            0.1278  \n",
       "96450       0.05 2025-01-01 01:00:00 NaN          51.5074            0.1278  \n",
       "96451       0.05 2025-01-01 02:00:00 NaN          51.5074            0.1278  \n",
       "96452       0.05 2025-01-01 03:00:00 NaN          51.5074            0.1278  \n",
       "96453       0.05 2025-01-01 04:00:00 NaN          51.5074            0.1278  \n",
       "96454       0.05 2025-01-01 05:00:00 NaN          51.5074            0.1278  \n",
       "96455       0.05 2025-01-01 06:00:00 NaN          51.5074            0.1278  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined[df_combined.tp.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a0dac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1b1af54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>step</th>\n",
       "      <th>number</th>\n",
       "      <th>surface</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>valid_time</th>\n",
       "      <th>tp</th>\n",
       "      <th>target_latitude</th>\n",
       "      <th>target_longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-12-31 18:00:00</td>\n",
       "      <td>0 days 01:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.4</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2013-12-31 19:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-12-31 18:00:00</td>\n",
       "      <td>0 days 02:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.4</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2013-12-31 20:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-12-31 18:00:00</td>\n",
       "      <td>0 days 03:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.4</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2013-12-31 21:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-12-31 18:00:00</td>\n",
       "      <td>0 days 04:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.4</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2013-12-31 22:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-12-31 18:00:00</td>\n",
       "      <td>0 days 05:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.4</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2013-12-31 23:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.5074</td>\n",
       "      <td>0.1278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time            step  number  surface  latitude  longitude  \\\n",
       "0 2013-12-31 18:00:00 0 days 01:00:00       0      0.0      51.4       0.05   \n",
       "1 2013-12-31 18:00:00 0 days 02:00:00       0      0.0      51.4       0.05   \n",
       "2 2013-12-31 18:00:00 0 days 03:00:00       0      0.0      51.4       0.05   \n",
       "3 2013-12-31 18:00:00 0 days 04:00:00       0      0.0      51.4       0.05   \n",
       "4 2013-12-31 18:00:00 0 days 05:00:00       0      0.0      51.4       0.05   \n",
       "\n",
       "           valid_time  tp  target_latitude  target_longitude  \n",
       "0 2013-12-31 19:00:00 NaN          51.5074            0.1278  \n",
       "1 2013-12-31 20:00:00 NaN          51.5074            0.1278  \n",
       "2 2013-12-31 21:00:00 NaN          51.5074            0.1278  \n",
       "3 2013-12-31 22:00:00 NaN          51.5074            0.1278  \n",
       "4 2013-12-31 23:00:00 NaN          51.5074            0.1278  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf08005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0c5739c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2025-01-01 06:00:00')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.valid_time.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319297b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20dfd42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30ca0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.valid_time.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47427e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.valid_time.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f81987",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './data/ERA5/data_puglia_tp_2025.grib'\n",
    "output_path = './output/'\n",
    "city_name = 'bari'\n",
    "city_lat = 41.14043890868558  # Bari latitude\n",
    "city_lon = 16.86619063277282  # Bari longitude\n",
    "\n",
    "df_test_bari = process_grib_file(grib_file_path = file_path,\n",
    "                            output_csv_path=output_path,\n",
    "                            city_name=city_name,\n",
    "                            city_lat=city_lat,\n",
    "                            city_lon=city_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176228a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecce 40.35654102220363, 18.17311913754415\n",
    "\n",
    "file_path = './data/ERA5/data_puglia_tp_2025.grib'\n",
    "output_path = './output/'\n",
    "city_name = 'lecce'\n",
    "city_lat = 40.35654102220363  # Lecce latitude\n",
    "city_lon = 18.17311913754415  # Lecce longitude\n",
    "\n",
    "df_test_lecce = process_grib_file(grib_file_path = file_path,\n",
    "                                  output_csv_path=output_path,\n",
    "                                  city_name=city_name,\n",
    "                                  city_lat=city_lat,\n",
    "                                  city_lon=city_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aed3f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb5becd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5e1ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working for London coordinates\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def grib_to_csv_london(grib_file_path, output_csv_path=None):\n",
    "    \"\"\"\n",
    "    Read GRIB file, extract data for London coordinates, and save as CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    grib_file_path (str): Path to the GRIB file\n",
    "    output_csv_path (str): Path for output CSV file (optional)\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with the extracted data\n",
    "    \"\"\"\n",
    "    \n",
    "    # London coordinates\n",
    "    london_lat = 51.5074\n",
    "    london_lon = -0.1278  # Note: GRIB files often use 0-360 longitude\n",
    "    london_lon_360 = 359.8722  # London longitude in 0-360 format\n",
    "    \n",
    "    print(f\"Reading GRIB file: {grib_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Open GRIB file with xarray\n",
    "        ds = xr.open_dataset(grib_file_path, engine='cfgrib')\n",
    "        \n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        print(f\"Dataset dimensions: {dict(ds.dims)}\")\n",
    "        print(f\"Dataset variables: {list(ds.data_vars)}\")\n",
    "        print(f\"Dataset coordinates: {list(ds.coords)}\")\n",
    "        \n",
    "        # Check longitude format (0-360 vs -180-180)\n",
    "        lon_min = ds.longitude.min().values\n",
    "        lon_max = ds.longitude.max().values\n",
    "        print(f\"Longitude range: {lon_min} to {lon_max}\")\n",
    "        \n",
    "        # Use appropriate London longitude based on the dataset's longitude format\n",
    "        if lon_max > 180:\n",
    "            target_lon = london_lon_360\n",
    "            print(f\"Using 0-360 longitude format: {target_lon}\")\n",
    "        else:\n",
    "            target_lon = london_lon\n",
    "            print(f\"Using -180-180 longitude format: {target_lon}\")\n",
    "        \n",
    "        # Find nearest grid point to London\n",
    "        lat_idx = np.abs(ds.latitude - london_lat).argmin()\n",
    "        lon_idx = np.abs(ds.longitude - target_lon).argmin()\n",
    "        \n",
    "        actual_lat = ds.latitude[lat_idx].values\n",
    "        actual_lon = ds.longitude[lon_idx].values\n",
    "        \n",
    "        print(f\"Target coordinates: ({london_lat}, {target_lon})\")\n",
    "        print(f\"Nearest grid point: ({actual_lat}, {actual_lon})\")\n",
    "        \n",
    "        # Extract data for London coordinates\n",
    "        london_data = ds.isel(latitude=lat_idx, longitude=lon_idx)\n",
    "        \n",
    "        # Convert to pandas DataFrame\n",
    "        df_list = []\n",
    "        \n",
    "        for var_name in london_data.data_vars:\n",
    "            var_data = london_data[var_name]\n",
    "            \n",
    "            # Handle different time dimensions\n",
    "            if 'time' in var_data.dims:\n",
    "                # Create DataFrame with time index\n",
    "                df_var = var_data.to_dataframe().reset_index()\n",
    "                df_var = df_var.rename(columns={var_name: var_name})\n",
    "                df_list.append(df_var)\n",
    "            elif 'valid_time' in var_data.dims:\n",
    "                # Handle valid_time dimension\n",
    "                df_var = var_data.to_dataframe().reset_index()\n",
    "                df_var = df_var.rename(columns={var_name: var_name})\n",
    "                df_list.append(df_var)\n",
    "            else:\n",
    "                # Single value variables\n",
    "                df_var = pd.DataFrame({\n",
    "                    var_name: [var_data.values],\n",
    "                    'latitude': [actual_lat],\n",
    "                    'longitude': [actual_lon]\n",
    "                })\n",
    "                df_list.append(df_var)\n",
    "        \n",
    "        # Combine all variables into one DataFrame\n",
    "        if len(df_list) > 1:\n",
    "            # Merge on common columns (time, latitude, longitude)\n",
    "            df = df_list[0]\n",
    "            for df_var in df_list[1:]:\n",
    "                common_cols = set(df.columns) & set(df_var.columns)\n",
    "                if common_cols:\n",
    "                    df = pd.merge(df, df_var, on=list(common_cols), how='outer')\n",
    "                else:\n",
    "                    # If no common columns, concatenate side by side\n",
    "                    df = pd.concat([df, df_var], axis=1)\n",
    "        else:\n",
    "            df = df_list[0]\n",
    "        \n",
    "        # Add metadata columns\n",
    "        df['actual_latitude'] = actual_lat\n",
    "        df['actual_longitude'] = actual_lon\n",
    "        df['target_latitude'] = london_lat\n",
    "        df['target_longitude'] = target_lon\n",
    "        \n",
    "        # Sort by time if time column exists\n",
    "        time_cols = [col for col in df.columns if 'time' in col.lower()]\n",
    "        if time_cols:\n",
    "            df = df.sort_values(time_cols[0])\n",
    "        \n",
    "        print(f\"DataFrame shape: {df.shape}\")\n",
    "        print(f\"DataFrame columns: {list(df.columns)}\")\n",
    "        print(\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Save to CSV\n",
    "        if output_csv_path is None:\n",
    "            output_csv_path = Path(grib_file_path).stem + \"_london_data.csv\"\n",
    "        \n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData saved to: {output_csv_path}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing GRIB file: {e}\")\n",
    "        print(\"Make sure you have the required packages installed:\")\n",
    "        print(\"pip install xarray cfgrib pandas numpy\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "    grib_file = \"./data/ERA5/ERA5_hourly_data_on_single_levels_from_2014_to_2021.grib\"  # Replace with your GRIB file path\n",
    "    output_csv = \"./output/ERA5_hourly_data_on_single_levels_from_2014_to_2021.csv\"  # Replace with desired output path\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not Path(grib_file).exists():\n",
    "        print(f\"GRIB file not found: {grib_file}\")\n",
    "        print(\"Please update the 'grib_file' variable with the correct path to your GRIB file.\")\n",
    "        return\n",
    "    \n",
    "    # Process the file\n",
    "    try:\n",
    "        df = grib_to_csv_london(grib_file, output_csv)\n",
    "        \n",
    "        # Display summary statistics\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Total records: {len(df)}\")\n",
    "        \n",
    "        # Show time range if available\n",
    "        time_cols = [col for col in df.columns if 'time' in col.lower()]\n",
    "        if time_cols:\n",
    "            time_col = time_cols[0]\n",
    "            print(f\"Time range: {df[time_col].min()} to {df[time_col].max()}\")\n",
    "        \n",
    "        print(f\"Variables extracted: {[col for col in df.columns if col not in ['latitude', 'longitude', 'actual_latitude', 'actual_longitude', 'target_latitude', 'target_longitude'] + time_cols]}\")\n",
    "        \n",
    "        # Load the CSV back into pandas to verify\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"VERIFICATION - Loading CSV back into pandas:\")\n",
    "        print(\"=\"*50)\n",
    "        df_loaded = pd.read_csv(output_csv)\n",
    "        print(f\"Loaded DataFrame shape: {df_loaded.shape}\")\n",
    "        print(\"DataFrame info:\")\n",
    "        print(df_loaded.info())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process GRIB file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cece2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(\"./output/ERA5_hourly_data_on_single_levels_from_2014_to_2021.csv\")\n",
    "print(df_data.shape)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f5bc01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8217b669",
   "metadata": {},
   "outputs": [],
   "source": [
    "140-80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd85655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2817333",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(\"./output/ERA5_hourly_data_on_single_levels_from_1940_to_present.csv\")\n",
    "df_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731bd700",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde7b429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbada62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['year'] = pd.to_datetime(df_data['time'], errors='coerce').dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cded2ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_data\n",
    " .groupby('year', observed=True)['tp']\n",
    " .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625f3ac5",
   "metadata": {},
   "source": [
    "### NC files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b7dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V4 \n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def netcdf_to_csv_london(nc_file_path, output_csv_path=None):\n",
    "    \"\"\"\n",
    "    Read NetCDF file, extract precipitation data for London coordinates, and save as CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    nc_file_path (str): Path to the NetCDF file\n",
    "    output_csv_path (str): Path for output CSV file (optional)\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with the extracted data\n",
    "    \"\"\"\n",
    "    \n",
    "    # London coordinates\n",
    "    london_lat = 51.5074\n",
    "    london_lon = -0.1278\n",
    "    \n",
    "    print(f\"Reading NetCDF file: {nc_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Open NetCDF file with xarray\n",
    "        ds = xr.open_dataset(nc_file_path)\n",
    "        \n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        print(f\"Dataset dimensions: {dict(ds.dims)}\")\n",
    "        print(f\"Dataset variables: {list(ds.data_vars)}\")\n",
    "        print(f\"Dataset coordinates: {list(ds.coords)}\")\n",
    "        \n",
    "        # Display dataset info\n",
    "        print(\"\\nDataset overview:\")\n",
    "        print(ds)\n",
    "        \n",
    "        # Check coordinate names (common variations)\n",
    "        lat_names = ['lat', 'latitude', 'y', 'projection_y_coordinate']\n",
    "        lon_names = ['lon', 'longitude', 'x', 'projection_x_coordinate']\n",
    "        time_names = ['time', 'time_bnds', 'yyyymm']\n",
    "        \n",
    "        # Find actual coordinate names in the dataset\n",
    "        lat_coord = None\n",
    "        lon_coord = None\n",
    "        time_coord = None\n",
    "        \n",
    "        for name in lat_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                lat_coord = name\n",
    "                break\n",
    "                \n",
    "        for name in lon_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                lon_coord = name\n",
    "                break\n",
    "                \n",
    "        for name in time_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                time_coord = name\n",
    "                break\n",
    "        \n",
    "        print(f\"\\nIdentified coordinates:\")\n",
    "        print(f\"Latitude coordinate: {lat_coord}\")\n",
    "        print(f\"Longitude coordinate: {lon_coord}\")\n",
    "        print(f\"Time coordinate: {time_coord}\")\n",
    "        \n",
    "        if lat_coord is None or lon_coord is None:\n",
    "            print(\"Warning: Could not identify latitude/longitude coordinates\")\n",
    "            print(\"Available coordinates:\", list(ds.coords))\n",
    "            return None\n",
    "        \n",
    "        # Check coordinate ranges\n",
    "        if lat_coord in ds.coords:\n",
    "            lat_min = ds.coords[lat_coord].min().values\n",
    "            lat_max = ds.coords[lat_coord].max().values\n",
    "            print(f\"Latitude range: {lat_min} to {lat_max}\")\n",
    "        \n",
    "        if lon_coord in ds.coords:\n",
    "            lon_min = ds.coords[lon_coord].min().values\n",
    "            lon_max = ds.coords[lon_coord].max().values\n",
    "            print(f\"Longitude range: {lon_min} to {lon_max}\")\n",
    "        \n",
    "        # Handle projected coordinates (if using OSGB or similar)\n",
    "        if 'projection' in str(ds.coords) or lat_coord in ['y', 'projection_y_coordinate']:\n",
    "            print(\"\\nDetected projected coordinates (likely OSGB)\")\n",
    "            print(\"You may need to convert London lat/lon to the projection coordinates\")\n",
    "            print(\"For now, finding nearest grid point in projected space...\")\n",
    "        \n",
    "        # Define region around London (in degrees)\n",
    "        region_size = 0.5  # degrees (approximately 50km at London's latitude)\n",
    "        \n",
    "        # Define bounding box around London\n",
    "        lat_min = london_lat - region_size\n",
    "        lat_max = london_lat + region_size\n",
    "        lon_min = london_lon - region_size\n",
    "        lon_max = london_lon + region_size\n",
    "        \n",
    "        print(f\"\\nExtracting region around London:\")\n",
    "        print(f\"Latitude range: {lat_min:.3f} to {lat_max:.3f}\")\n",
    "        print(f\"Longitude range: {lon_min:.3f} to {lon_max:.3f}\")\n",
    "        \n",
    "        # Find grid indices for the region\n",
    "        if lat_coord in ds.coords and lon_coord in ds.coords:\n",
    "            # Handle 2D coordinate arrays (common in projected datasets)\n",
    "            lat_data = ds.coords[lat_coord]\n",
    "            lon_data = ds.coords[lon_coord]\n",
    "            \n",
    "            if lat_data.ndim == 2 and lon_data.ndim == 2:\n",
    "                # 2D coordinate arrays - find region\n",
    "                print(\"Using 2D coordinate arrays to find region\")\n",
    "                \n",
    "                # Find points within the bounding box\n",
    "                within_region = ((lat_data >= lat_min) & (lat_data <= lat_max) & \n",
    "                               (lon_data >= lon_min) & (lon_data <= lon_max))\n",
    "                \n",
    "                # Find the bounding indices of the region\n",
    "                y_indices, x_indices = np.where(within_region)\n",
    "                \n",
    "                if len(y_indices) == 0:\n",
    "                    print(\"No grid points found in the specified region!\")\n",
    "                    return None\n",
    "                \n",
    "                y_min_idx = y_indices.min()\n",
    "                y_max_idx = y_indices.max()\n",
    "                x_min_idx = x_indices.min()\n",
    "                x_max_idx = x_indices.max()\n",
    "                \n",
    "                print(f\"Found {len(y_indices)} grid points in region\")\n",
    "                print(f\"Grid index ranges: y={y_min_idx}-{y_max_idx}, x={x_min_idx}-{x_max_idx}\")\n",
    "                \n",
    "                # Store the slice ranges for extraction\n",
    "                y_slice = slice(y_min_idx, y_max_idx + 1)\n",
    "                x_slice = slice(x_min_idx, x_max_idx + 1)\n",
    "                \n",
    "                # Get actual coordinate bounds of extracted region\n",
    "                region_lat_min = lat_data[y_min_idx:y_max_idx+1, x_min_idx:x_max_idx+1].min().values\n",
    "                region_lat_max = lat_data[y_min_idx:y_max_idx+1, x_min_idx:x_max_idx+1].max().values\n",
    "                region_lon_min = lon_data[y_min_idx:y_max_idx+1, x_min_idx:x_max_idx+1].min().values\n",
    "                region_lon_max = lon_data[y_min_idx:y_max_idx+1, x_min_idx:x_max_idx+1].max().values\n",
    "                \n",
    "                print(f\"Actual extracted region:\")\n",
    "                print(f\"  Latitude: {region_lat_min:.3f} to {region_lat_max:.3f}\")\n",
    "                print(f\"  Longitude: {region_lon_min:.3f} to {region_lon_max:.3f}\")\n",
    "                \n",
    "            else:\n",
    "                # 1D coordinate arrays\n",
    "                lat_mask = (lat_data >= lat_min) & (lat_data <= lat_max)\n",
    "                lon_mask = (lon_data >= lon_min) & (lon_data <= lon_max)\n",
    "                \n",
    "                y_indices = np.where(lat_mask)[0]\n",
    "                x_indices = np.where(lon_mask)[0]\n",
    "                \n",
    "                if len(y_indices) == 0 or len(x_indices) == 0:\n",
    "                    print(\"No grid points found in the specified region!\")\n",
    "                    return None\n",
    "                \n",
    "                y_slice = slice(y_indices.min(), y_indices.max() + 1)\n",
    "                x_slice = slice(x_indices.min(), x_indices.max() + 1)\n",
    "                \n",
    "                print(f\"Grid slices: y={y_slice}, x={x_slice}\")\n",
    "        else:\n",
    "            print(\"Could not find coordinate values for region extraction\")\n",
    "            return None\n",
    "        \n",
    "        # Extract data for London region\n",
    "        # Use the projection coordinate names for selection\n",
    "        proj_y_coord = 'projection_y_coordinate'\n",
    "        proj_x_coord = 'projection_x_coordinate'\n",
    "        \n",
    "        selection_dict = {proj_y_coord: y_slice, proj_x_coord: x_slice}\n",
    "        london_region_data = ds.isel(selection_dict)\n",
    "        \n",
    "        print(f\"\\nExtracted region data shape: {dict(london_region_data.sizes)}\")\n",
    "        \n",
    "        print(f\"\\nExtracted data variables: {list(london_region_data.data_vars)}\")\n",
    "        \n",
    "        # Convert to pandas DataFrame - simplified approach\n",
    "        print(f\"\\nExtracting data for London...\")\n",
    "        \n",
    "        # Create a simple DataFrame with the main data\n",
    "        data_dict = {}\n",
    "        \n",
    "        # Extract main variables (skip coordinate and bounds variables)\n",
    "        main_vars = [var for var in london_region_data.data_vars \n",
    "                    if not any(skip in var for skip in ['_bnds', 'transverse_mercator', 'bounds'])]\n",
    "        \n",
    "        print(f\"Main data variables to extract: {main_vars}\")\n",
    "        \n",
    "        for var_name in main_vars:\n",
    "            var_data = london_region_data[var_name]\n",
    "            print(f\"\\nProcessing variable: {var_name}\")\n",
    "            print(f\"Variable dimensions: {var_data.dims}\")\n",
    "            print(f\"Variable shape: {var_data.shape}\")\n",
    "            \n",
    "            # Get variable attributes\n",
    "            if hasattr(var_data, 'attrs'):\n",
    "                print(f\"Variable attributes: {var_data.attrs}\")\n",
    "            \n",
    "            # Extract the actual values\n",
    "            if var_data.size == 1:\n",
    "                # Single value\n",
    "                data_dict[var_name] = [float(var_data.values)]\n",
    "            elif time_coord and time_coord in var_data.dims:\n",
    "                # Time series data\n",
    "                values = var_data.values\n",
    "                if values.ndim > 1:\n",
    "                    values = values.flatten()\n",
    "                data_dict[var_name] = values.tolist()\n",
    "            else:\n",
    "                # Other cases\n",
    "                values = var_data.values\n",
    "                if hasattr(values, 'flatten'):\n",
    "                    values = values.flatten()\n",
    "                data_dict[var_name] = [float(values)] if np.isscalar(values) else values.tolist()\n",
    "            \n",
    "            # Add metadata\n",
    "            if hasattr(var_data, 'attrs'):\n",
    "                units = var_data.attrs.get('units', 'unknown')\n",
    "                long_name = var_data.attrs.get('long_name', var_name)\n",
    "                data_dict[f'{var_name}_units'] = [units] * len(data_dict[var_name])\n",
    "                data_dict[f'{var_name}_long_name'] = [long_name] * len(data_dict[var_name])\n",
    "        \n",
    "        # Add time information if available\n",
    "        if time_coord and time_coord in london_region_data.coords:\n",
    "            time_values = london_region_data.coords[time_coord].values\n",
    "            if hasattr(time_values, 'flatten'):\n",
    "                time_values = time_values.flatten()\n",
    "            data_dict[time_coord] = time_values.tolist() if hasattr(time_values, 'tolist') else [time_values]\n",
    "        \n",
    "        # Create DataFrame\n",
    "        # First, make sure all lists have the same length\n",
    "        max_length = max(len(v) if isinstance(v, list) else 1 for v in data_dict.values())\n",
    "        \n",
    "        for key, value in data_dict.items():\n",
    "            if isinstance(value, list) and len(value) == 1 and max_length > 1:\n",
    "                data_dict[key] = value * max_length\n",
    "        \n",
    "        df = pd.DataFrame(data_dict)\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"Warning: No data extracted\")\n",
    "            return None\n",
    "        \n",
    "        # Add location metadata\n",
    "        df['target_latitude'] = london_lat\n",
    "        df['target_longitude'] = london_lon\n",
    "        df['coordinate_system'] = f\"{lat_coord}, {lon_coord}\"\n",
    "        \n",
    "        # Sort by time if time column exists\n",
    "        time_cols = [col for col in df.columns if any(t in col.lower() for t in ['time', 'date', 'year'])]\n",
    "        if time_cols:\n",
    "            df = df.sort_values(time_cols[0])\n",
    "            print(f\"Sorted by time column: {time_cols[0]}\")\n",
    "        \n",
    "        print(f\"\\nFinal DataFrame shape: {df.shape}\")\n",
    "        print(f\"DataFrame columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Show data types\n",
    "        print(f\"\\nData types:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"  {col}: {df[col].dtype}\")\n",
    "        \n",
    "        print(f\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        if len(df) > 5:\n",
    "            print(f\"\\nLast few rows:\")\n",
    "            print(df.tail())\n",
    "        \n",
    "        # Save to CSV\n",
    "        if output_csv_path is None:\n",
    "            output_csv_path = Path(nc_file_path).stem + \"_london_region_precipitation.csv\"\n",
    "        \n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData saved to: {output_csv_path}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing NetCDF file: {e}\")\n",
    "        print(\"Make sure you have the required packages installed:\")\n",
    "        print(\"pip install xarray netcdf4 pandas numpy\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # Your specific file\n",
    "    nc_file = \"./data/ERA5/rainfall_hadukgrid_uk_1km_ann-30y_196101-199012.nc\"\n",
    "    output_csv = \"./output/london_region_rainfall_1961_1990_V4.csv\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not Path(nc_file).exists():\n",
    "        print(f\"NetCDF file not found: {nc_file}\")\n",
    "        print(\"Please make sure the file is in the current directory or update the path.\")\n",
    "        return\n",
    "    \n",
    "    # Process the file\n",
    "    try:\n",
    "        df = netcdf_to_csv_london(nc_file, output_csv)\n",
    "        \n",
    "        if df is not None:\n",
    "            # Display summary statistics\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"SUMMARY\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"Total grid points extracted: {len(df)}\")\n",
    "            \n",
    "            # Show spatial extent\n",
    "            if 'latitude' in df.columns and 'longitude' in df.columns:\n",
    "                print(f\"Latitude range: {df['latitude'].min():.3f} to {df['latitude'].max():.3f}\")\n",
    "                print(f\"Longitude range: {df['longitude'].min():.3f} to {df['longitude'].max():.3f}\")\n",
    "            \n",
    "            # Show time range if available\n",
    "            time_cols = [col for col in df.columns if any(t in col.lower() for t in ['time', 'date', 'year'])]\n",
    "            if time_cols:\n",
    "                time_col = time_cols[0]\n",
    "                print(f\"Time range: {df[time_col].min()} to {df[time_col].max()}\")\n",
    "            \n",
    "            # Show precipitation statistics\n",
    "            precip_cols = [col for col in df.columns if any(p in col.lower() for p in ['rain', 'precip', 'precipitation'])]\n",
    "            if precip_cols:\n",
    "                print(f\"\\nRegional precipitation statistics for {precip_cols[0]}:\")\n",
    "                print(df[precip_cols[0]].describe())\n",
    "                print(f\"Grid points with data: {df[precip_cols[0]].notna().sum()}/{len(df)}\")\n",
    "            \n",
    "            # Load the CSV back into pandas to verify\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"VERIFICATION - Loading CSV back into pandas:\")\n",
    "            print(\"=\"*60)\n",
    "            df_loaded = pd.read_csv(output_csv)\n",
    "            print(f\"Loaded DataFrame shape: {df_loaded.shape}\")\n",
    "            print(\"Sample of loaded data:\")\n",
    "            print(df_loaded.head(3))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process NetCDF file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e38b32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c110fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c425bce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V3\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def netcdf_to_csv_london(nc_file_path, output_csv_path=None):\n",
    "    \"\"\"\n",
    "    Read NetCDF file, extract precipitation data for London coordinates, and save as CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    nc_file_path (str): Path to the NetCDF file\n",
    "    output_csv_path (str): Path for output CSV file (optional)\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with the extracted data\n",
    "    \"\"\"\n",
    "    \n",
    "    # London coordinates\n",
    "    london_lat = 51.5074\n",
    "    london_lon = -0.1278\n",
    "    \n",
    "    print(f\"Reading NetCDF file: {nc_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Open NetCDF file with xarray\n",
    "        ds = xr.open_dataset(nc_file_path)\n",
    "        \n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        print(f\"Dataset dimensions: {dict(ds.dims)}\")\n",
    "        print(f\"Dataset variables: {list(ds.data_vars)}\")\n",
    "        print(f\"Dataset coordinates: {list(ds.coords)}\")\n",
    "        \n",
    "        # Display dataset info\n",
    "        print(\"\\nDataset overview:\")\n",
    "        print(ds)\n",
    "        \n",
    "        # Check coordinate names (common variations)\n",
    "        lat_names = ['lat', 'latitude', 'y', 'projection_y_coordinate']\n",
    "        lon_names = ['lon', 'longitude', 'x', 'projection_x_coordinate']\n",
    "        time_names = ['time', 'time_bnds', 'yyyymm']\n",
    "        \n",
    "        # Find actual coordinate names in the dataset\n",
    "        lat_coord = None\n",
    "        lon_coord = None\n",
    "        time_coord = None\n",
    "        \n",
    "        for name in lat_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                lat_coord = name\n",
    "                break\n",
    "                \n",
    "        for name in lon_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                lon_coord = name\n",
    "                break\n",
    "                \n",
    "        for name in time_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                time_coord = name\n",
    "                break\n",
    "        \n",
    "        print(f\"\\nIdentified coordinates:\")\n",
    "        print(f\"Latitude coordinate: {lat_coord}\")\n",
    "        print(f\"Longitude coordinate: {lon_coord}\")\n",
    "        print(f\"Time coordinate: {time_coord}\")\n",
    "        \n",
    "        if lat_coord is None or lon_coord is None:\n",
    "            print(\"Warning: Could not identify latitude/longitude coordinates\")\n",
    "            print(\"Available coordinates:\", list(ds.coords))\n",
    "            return None\n",
    "        \n",
    "        # Check coordinate ranges\n",
    "        if lat_coord in ds.coords:\n",
    "            lat_min = ds.coords[lat_coord].min().values\n",
    "            lat_max = ds.coords[lat_coord].max().values\n",
    "            print(f\"Latitude range: {lat_min} to {lat_max}\")\n",
    "        \n",
    "        if lon_coord in ds.coords:\n",
    "            lon_min = ds.coords[lon_coord].min().values\n",
    "            lon_max = ds.coords[lon_coord].max().values\n",
    "            print(f\"Longitude range: {lon_min} to {lon_max}\")\n",
    "        \n",
    "        # Handle projected coordinates (if using OSGB or similar)\n",
    "        if 'projection' in str(ds.coords) or lat_coord in ['y', 'projection_y_coordinate']:\n",
    "            print(\"\\nDetected projected coordinates (likely OSGB)\")\n",
    "            print(\"You may need to convert London lat/lon to the projection coordinates\")\n",
    "            print(\"For now, finding nearest grid point in projected space...\")\n",
    "        \n",
    "        # Find nearest grid point to London\n",
    "        if lat_coord in ds.coords and lon_coord in ds.coords:\n",
    "            # Handle 2D coordinate arrays (common in projected datasets)\n",
    "            lat_data = ds.coords[lat_coord]\n",
    "            lon_data = ds.coords[lon_coord]\n",
    "            \n",
    "            if lat_data.ndim == 2 and lon_data.ndim == 2:\n",
    "                # 2D coordinate arrays - find nearest point in 2D space\n",
    "                print(\"Using 2D coordinate arrays to find nearest point\")\n",
    "                \n",
    "                # Calculate distance from London to all grid points\n",
    "                lat_diff = lat_data - london_lat\n",
    "                lon_diff = lon_data - london_lon\n",
    "                distances = np.sqrt(lat_diff**2 + lon_diff**2)\n",
    "                \n",
    "                # Find indices of minimum distance\n",
    "                min_idx = np.unravel_index(distances.argmin(), distances.shape)\n",
    "                y_idx, x_idx = min_idx\n",
    "                \n",
    "                actual_lat = lat_data[y_idx, x_idx].values\n",
    "                actual_lon = lon_data[y_idx, x_idx].values\n",
    "                \n",
    "                print(f\"\\nTarget coordinates: ({london_lat}, {london_lon})\")\n",
    "                print(f\"Nearest grid point: ({actual_lat}, {actual_lon})\")\n",
    "                print(f\"Grid indices: y={y_idx}, x={x_idx}\")\n",
    "                \n",
    "                # Use the projection coordinate indices for selection\n",
    "                lat_idx = y_idx  # This corresponds to projection_y_coordinate index\n",
    "                lon_idx = x_idx  # This corresponds to projection_x_coordinate index\n",
    "                \n",
    "            else:\n",
    "                # 1D coordinate arrays\n",
    "                lat_idx = np.abs(lat_data - london_lat).argmin()\n",
    "                lon_idx = np.abs(lon_data - london_lon).argmin()\n",
    "                \n",
    "                actual_lat = lat_data[lat_idx].values\n",
    "                actual_lon = lon_data[lon_idx].values\n",
    "                \n",
    "                print(f\"\\nTarget coordinates: ({london_lat}, {london_lon})\")\n",
    "                print(f\"Nearest grid point: ({actual_lat}, {actual_lon})\")\n",
    "        else:\n",
    "            print(\"Could not find coordinate values for nearest point calculation\")\n",
    "            return None\n",
    "        \n",
    "        # Extract data for London coordinates\n",
    "        # Use the projection coordinate names for selection\n",
    "        proj_y_coord = 'projection_y_coordinate'\n",
    "        proj_x_coord = 'projection_x_coordinate'\n",
    "        \n",
    "        selection_dict = {proj_y_coord: lat_idx, proj_x_coord: lon_idx}\n",
    "        london_data = ds.isel(selection_dict)\n",
    "        \n",
    "        print(f\"\\nExtracted data variables: {list(london_data.data_vars)}\")\n",
    "        \n",
    "        # Convert to pandas DataFrame - simplified approach\n",
    "        print(f\"\\nExtracting data for London...\")\n",
    "        \n",
    "        # Create a simple DataFrame with the main data\n",
    "        data_dict = {}\n",
    "        \n",
    "        # Extract main variables (skip coordinate and bounds variables)\n",
    "        main_vars = [var for var in london_data.data_vars \n",
    "                    if not any(skip in var for skip in ['_bnds', 'transverse_mercator', 'bounds'])]\n",
    "        \n",
    "        print(f\"Main data variables to extract: {main_vars}\")\n",
    "        \n",
    "        for var_name in main_vars:\n",
    "            var_data = london_data[var_name]\n",
    "            print(f\"\\nProcessing variable: {var_name}\")\n",
    "            print(f\"Variable dimensions: {var_data.dims}\")\n",
    "            print(f\"Variable shape: {var_data.shape}\")\n",
    "            \n",
    "            # Get variable attributes\n",
    "            if hasattr(var_data, 'attrs'):\n",
    "                print(f\"Variable attributes: {var_data.attrs}\")\n",
    "            \n",
    "            # Extract the actual values\n",
    "            if var_data.size == 1:\n",
    "                # Single value\n",
    "                data_dict[var_name] = [float(var_data.values)]\n",
    "            elif time_coord and time_coord in var_data.dims:\n",
    "                # Time series data\n",
    "                values = var_data.values\n",
    "                if values.ndim > 1:\n",
    "                    values = values.flatten()\n",
    "                data_dict[var_name] = values.tolist()\n",
    "            else:\n",
    "                # Other cases\n",
    "                values = var_data.values\n",
    "                if hasattr(values, 'flatten'):\n",
    "                    values = values.flatten()\n",
    "                data_dict[var_name] = [float(values)] if np.isscalar(values) else values.tolist()\n",
    "            \n",
    "            # Add metadata\n",
    "            if hasattr(var_data, 'attrs'):\n",
    "                units = var_data.attrs.get('units', 'unknown')\n",
    "                long_name = var_data.attrs.get('long_name', var_name)\n",
    "                data_dict[f'{var_name}_units'] = [units] * len(data_dict[var_name])\n",
    "                data_dict[f'{var_name}_long_name'] = [long_name] * len(data_dict[var_name])\n",
    "        \n",
    "        # Add time information if available\n",
    "        if time_coord and time_coord in london_data.coords:\n",
    "            time_values = london_data.coords[time_coord].values\n",
    "            if hasattr(time_values, 'flatten'):\n",
    "                time_values = time_values.flatten()\n",
    "            data_dict[time_coord] = time_values.tolist() if hasattr(time_values, 'tolist') else [time_values]\n",
    "        \n",
    "        # Create DataFrame\n",
    "        # First, make sure all lists have the same length\n",
    "        max_length = max(len(v) if isinstance(v, list) else 1 for v in data_dict.values())\n",
    "        \n",
    "        for key, value in data_dict.items():\n",
    "            if isinstance(value, list) and len(value) == 1 and max_length > 1:\n",
    "                data_dict[key] = value * max_length\n",
    "        \n",
    "        df = pd.DataFrame(data_dict)\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"Warning: No data extracted\")\n",
    "            return None\n",
    "        \n",
    "        # Add location metadata\n",
    "        df['actual_latitude'] = actual_lat\n",
    "        df['actual_longitude'] = actual_lon\n",
    "        df['target_latitude'] = london_lat\n",
    "        df['target_longitude'] = london_lon\n",
    "        df['coordinate_system'] = f\"{lat_coord}, {lon_coord}\"\n",
    "        \n",
    "        # Sort by time if time column exists\n",
    "        time_cols = [col for col in df.columns if any(t in col.lower() for t in ['time', 'date', 'year'])]\n",
    "        if time_cols:\n",
    "            df = df.sort_values(time_cols[0])\n",
    "            print(f\"Sorted by time column: {time_cols[0]}\")\n",
    "        \n",
    "        print(f\"\\nFinal DataFrame shape: {df.shape}\")\n",
    "        print(f\"DataFrame columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Show data types\n",
    "        print(f\"\\nData types:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"  {col}: {df[col].dtype}\")\n",
    "        \n",
    "        print(f\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        if len(df) > 5:\n",
    "            print(f\"\\nLast few rows:\")\n",
    "            print(df.tail())\n",
    "        \n",
    "        # Save to CSV\n",
    "        if output_csv_path is None:\n",
    "            output_csv_path = Path(nc_file_path).stem + \"_london_precipitation.csv\"\n",
    "        \n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData saved to: {output_csv_path}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing NetCDF file: {e}\")\n",
    "        print(\"Make sure you have the required packages installed:\")\n",
    "        print(\"pip install xarray netcdf4 pandas numpy\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # Your specific file\n",
    "    nc_file = \"./data/ERA5/rainfall_hadukgrid_uk_1km_ann-30y_196101-199012.nc\"\n",
    "    output_csv = \"./output/london_rainfall_1961_1990_V3.csv\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not Path(nc_file).exists():\n",
    "        print(f\"NetCDF file not found: {nc_file}\")\n",
    "        print(\"Please make sure the file is in the current directory or update the path.\")\n",
    "        return\n",
    "    \n",
    "    # Process the file\n",
    "    try:\n",
    "        df = netcdf_to_csv_london(nc_file, output_csv)\n",
    "        \n",
    "        if df is not None:\n",
    "            # Display summary statistics\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"SUMMARY\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"Total records: {len(df)}\")\n",
    "            \n",
    "            # Show time range if available\n",
    "            time_cols = [col for col in df.columns if any(t in col.lower() for t in ['time', 'date', 'year'])]\n",
    "            if time_cols:\n",
    "                time_col = time_cols[0]\n",
    "                print(f\"Time range: {df[time_col].min()} to {df[time_col].max()}\")\n",
    "            \n",
    "            # Show precipitation statistics\n",
    "            precip_cols = [col for col in df.columns if any(p in col.lower() for p in ['rain', 'precip', 'precipitation'])]\n",
    "            if precip_cols:\n",
    "                print(f\"\\nPrecipitation statistics for {precip_cols[0]}:\")\n",
    "                print(df[precip_cols[0]].describe())\n",
    "            \n",
    "            # Load the CSV back into pandas to verify\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"VERIFICATION - Loading CSV back into pandas:\")\n",
    "            print(\"=\"*60)\n",
    "            df_loaded = pd.read_csv(output_csv)\n",
    "            print(f\"Loaded DataFrame shape: {df_loaded.shape}\")\n",
    "            print(\"Sample of loaded data:\")\n",
    "            print(df_loaded.head(3))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process NetCDF file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c4e0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca1f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not working for datatype mismatch issues\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def netcdf_to_csv_london(nc_file_path, output_csv_path=None):\n",
    "    \"\"\"\n",
    "    Read NetCDF file, extract precipitation data for London coordinates, and save as CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    nc_file_path (str): Path to the NetCDF file\n",
    "    output_csv_path (str): Path for output CSV file (optional)\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with the extracted data\n",
    "    \"\"\"\n",
    "    \n",
    "    # London coordinates\n",
    "    london_lat = 51.5074\n",
    "    london_lon = -0.1278\n",
    "    \n",
    "    print(f\"Reading NetCDF file: {nc_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Open NetCDF file with xarray\n",
    "        ds = xr.open_dataset(nc_file_path)\n",
    "        \n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        print(f\"Dataset dimensions: {dict(ds.dims)}\")\n",
    "        print(f\"Dataset variables: {list(ds.data_vars)}\")\n",
    "        print(f\"Dataset coordinates: {list(ds.coords)}\")\n",
    "        \n",
    "        # Display dataset info\n",
    "        print(\"\\nDataset overview:\")\n",
    "        print(ds)\n",
    "        \n",
    "        # Check coordinate names (common variations)\n",
    "        lat_names = ['lat', 'latitude', 'y', 'projection_y_coordinate']\n",
    "        lon_names = ['lon', 'longitude', 'x', 'projection_x_coordinate']\n",
    "        time_names = ['time', 'time_bnds', 'yyyymm']\n",
    "        \n",
    "        # Find actual coordinate names in the dataset\n",
    "        lat_coord = None\n",
    "        lon_coord = None\n",
    "        time_coord = None\n",
    "        \n",
    "        for name in lat_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                lat_coord = name\n",
    "                break\n",
    "                \n",
    "        for name in lon_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                lon_coord = name\n",
    "                break\n",
    "                \n",
    "        for name in time_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                time_coord = name\n",
    "                break\n",
    "        \n",
    "        print(f\"\\nIdentified coordinates:\")\n",
    "        print(f\"Latitude coordinate: {lat_coord}\")\n",
    "        print(f\"Longitude coordinate: {lon_coord}\")\n",
    "        print(f\"Time coordinate: {time_coord}\")\n",
    "        \n",
    "        if lat_coord is None or lon_coord is None:\n",
    "            print(\"Warning: Could not identify latitude/longitude coordinates\")\n",
    "            print(\"Available coordinates:\", list(ds.coords))\n",
    "            return None\n",
    "        \n",
    "        # Check coordinate ranges\n",
    "        if lat_coord in ds.coords:\n",
    "            lat_min = ds.coords[lat_coord].min().values\n",
    "            lat_max = ds.coords[lat_coord].max().values\n",
    "            print(f\"Latitude range: {lat_min} to {lat_max}\")\n",
    "        \n",
    "        if lon_coord in ds.coords:\n",
    "            lon_min = ds.coords[lon_coord].min().values\n",
    "            lon_max = ds.coords[lon_coord].max().values\n",
    "            print(f\"Longitude range: {lon_min} to {lon_max}\")\n",
    "        \n",
    "        # Handle projected coordinates (if using OSGB or similar)\n",
    "        if 'projection' in str(ds.coords) or lat_coord in ['y', 'projection_y_coordinate']:\n",
    "            print(\"\\nDetected projected coordinates (likely OSGB)\")\n",
    "            print(\"You may need to convert London lat/lon to the projection coordinates\")\n",
    "            print(\"For now, finding nearest grid point in projected space...\")\n",
    "        \n",
    "        # Find nearest grid point to London\n",
    "        if lat_coord in ds.coords and lon_coord in ds.coords:\n",
    "            # Handle 2D coordinate arrays (common in projected datasets)\n",
    "            lat_data = ds.coords[lat_coord]\n",
    "            lon_data = ds.coords[lon_coord]\n",
    "            \n",
    "            if lat_data.ndim == 2 and lon_data.ndim == 2:\n",
    "                # 2D coordinate arrays - find nearest point in 2D space\n",
    "                print(\"Using 2D coordinate arrays to find nearest point\")\n",
    "                \n",
    "                # Calculate distance from London to all grid points\n",
    "                lat_diff = lat_data - london_lat\n",
    "                lon_diff = lon_data - london_lon\n",
    "                distances = np.sqrt(lat_diff**2 + lon_diff**2)\n",
    "                \n",
    "                # Find indices of minimum distance\n",
    "                min_idx = np.unravel_index(distances.argmin(), distances.shape)\n",
    "                y_idx, x_idx = min_idx\n",
    "                \n",
    "                actual_lat = lat_data[y_idx, x_idx].values\n",
    "                actual_lon = lon_data[y_idx, x_idx].values\n",
    "                \n",
    "                print(f\"\\nTarget coordinates: ({london_lat}, {london_lon})\")\n",
    "                print(f\"Nearest grid point: ({actual_lat}, {actual_lon})\")\n",
    "                print(f\"Grid indices: y={y_idx}, x={x_idx}\")\n",
    "                \n",
    "                # Use the projection coordinate indices for selection\n",
    "                lat_idx = y_idx  # This corresponds to projection_y_coordinate index\n",
    "                lon_idx = x_idx  # This corresponds to projection_x_coordinate index\n",
    "                \n",
    "            else:\n",
    "                # 1D coordinate arrays\n",
    "                lat_idx = np.abs(lat_data - london_lat).argmin()\n",
    "                lon_idx = np.abs(lon_data - london_lon).argmin()\n",
    "                \n",
    "                actual_lat = lat_data[lat_idx].values\n",
    "                actual_lon = lon_data[lon_idx].values\n",
    "                \n",
    "                print(f\"\\nTarget coordinates: ({london_lat}, {london_lon})\")\n",
    "                print(f\"Nearest grid point: ({actual_lat}, {actual_lon})\")\n",
    "        else:\n",
    "            print(\"Could not find coordinate values for nearest point calculation\")\n",
    "            return None\n",
    "        \n",
    "        # Extract data for London coordinates\n",
    "        # Use the projection coordinate names for selection\n",
    "        proj_y_coord = 'projection_y_coordinate'\n",
    "        proj_x_coord = 'projection_x_coordinate'\n",
    "        \n",
    "        selection_dict = {proj_y_coord: lat_idx, proj_x_coord: lon_idx}\n",
    "        london_data = ds.isel(selection_dict)\n",
    "        \n",
    "        print(f\"\\nExtracted data variables: {list(london_data.data_vars)}\")\n",
    "        \n",
    "        # Convert to pandas DataFrame\n",
    "        df_list = []\n",
    "        \n",
    "        for var_name in london_data.data_vars:\n",
    "            var_data = london_data[var_name]\n",
    "            print(f\"\\nProcessing variable: {var_name}\")\n",
    "            print(f\"Variable dimensions: {var_data.dims}\")\n",
    "            print(f\"Variable shape: {var_data.shape}\")\n",
    "            \n",
    "            # Get variable attributes\n",
    "            if hasattr(var_data, 'attrs'):\n",
    "                print(f\"Variable attributes: {var_data.attrs}\")\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            if time_coord and time_coord in var_data.dims:\n",
    "                # Time series data\n",
    "                df_var = var_data.to_dataframe().reset_index()\n",
    "                \n",
    "                # Rename the variable column to be more descriptive\n",
    "                if var_name in df_var.columns:\n",
    "                    # Keep original variable name but add units if available\n",
    "                    units = var_data.attrs.get('units', '')\n",
    "                    long_name = var_data.attrs.get('long_name', var_name)\n",
    "                    \n",
    "                    col_name = var_name\n",
    "                    if units:\n",
    "                        col_name += f\"_{units}\".replace(\" \", \"_\")\n",
    "                    \n",
    "                    df_var = df_var.rename(columns={var_name: col_name})\n",
    "                    \n",
    "                    # Add metadata as separate columns\n",
    "                    df_var[f'{var_name}_long_name'] = long_name\n",
    "                    df_var[f'{var_name}_units'] = units\n",
    "                \n",
    "                df_list.append(df_var)\n",
    "            else:\n",
    "                # Single value or no time dimension\n",
    "                df_var = pd.DataFrame({\n",
    "                    var_name: [var_data.values],\n",
    "                    'latitude': [actual_lat],\n",
    "                    'longitude': [actual_lon]\n",
    "                })\n",
    "                \n",
    "                # Add metadata\n",
    "                if hasattr(var_data, 'attrs'):\n",
    "                    for attr_name, attr_value in var_data.attrs.items():\n",
    "                        df_var[f'{var_name}_{attr_name}'] = str(attr_value)\n",
    "                \n",
    "                df_list.append(df_var)\n",
    "        \n",
    "        # Combine all variables into one DataFrame\n",
    "        if len(df_list) > 1:\n",
    "            # Find common columns for merging\n",
    "            df = df_list[0]\n",
    "            for df_var in df_list[1:]:\n",
    "                common_cols = set(df.columns) & set(df_var.columns)\n",
    "                if common_cols and len(common_cols) > 1:  # More than just the variable column\n",
    "                    df = pd.merge(df, df_var, on=list(common_cols), how='outer')\n",
    "                else:\n",
    "                    # Concatenate side by side if same length\n",
    "                    if len(df) == len(df_var):\n",
    "                        df = pd.concat([df, df_var], axis=1)\n",
    "                    else:\n",
    "                        print(f\"Warning: Cannot merge {df_var.columns} - different lengths\")\n",
    "        else:\n",
    "            df = df_list[0] if df_list else pd.DataFrame()\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"Warning: No data extracted\")\n",
    "            return None\n",
    "        \n",
    "        # Add location metadata\n",
    "        df['actual_latitude'] = actual_lat\n",
    "        df['actual_longitude'] = actual_lon\n",
    "        df['target_latitude'] = london_lat\n",
    "        df['target_longitude'] = london_lon\n",
    "        df['coordinate_system'] = f\"{lat_coord}, {lon_coord}\"\n",
    "        \n",
    "        # Sort by time if time column exists\n",
    "        time_cols = [col for col in df.columns if any(t in col.lower() for t in ['time', 'date', 'year'])]\n",
    "        if time_cols:\n",
    "            df = df.sort_values(time_cols[0])\n",
    "            print(f\"Sorted by time column: {time_cols[0]}\")\n",
    "        \n",
    "        print(f\"\\nFinal DataFrame shape: {df.shape}\")\n",
    "        print(f\"DataFrame columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Show data types\n",
    "        print(f\"\\nData types:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"  {col}: {df[col].dtype}\")\n",
    "        \n",
    "        print(f\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        if len(df) > 5:\n",
    "            print(f\"\\nLast few rows:\")\n",
    "            print(df.tail())\n",
    "        \n",
    "        # Save to CSV\n",
    "        if output_csv_path is None:\n",
    "            output_csv_path = Path(nc_file_path).stem + \"_london_precipitation.csv\"\n",
    "        \n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData saved to: {output_csv_path}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing NetCDF file: {e}\")\n",
    "        print(\"Make sure you have the required packages installed:\")\n",
    "        print(\"pip install xarray netcdf4 pandas numpy\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # Your specific file\n",
    "    nc_file = \"./data/ERA5/rainfall_hadukgrid_uk_1km_ann-30y_196101-199012.nc\"\n",
    "    output_csv = \"./output/london_rainfall_1961_1990_V2.csv\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not Path(nc_file).exists():\n",
    "        print(f\"NetCDF file not found: {nc_file}\")\n",
    "        print(\"Please make sure the file is in the current directory or update the path.\")\n",
    "        return\n",
    "    \n",
    "    # Process the file\n",
    "    try:\n",
    "        df = netcdf_to_csv_london(nc_file, output_csv)\n",
    "        \n",
    "        if df is not None:\n",
    "            # Display summary statistics\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"SUMMARY\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"Total records: {len(df)}\")\n",
    "            \n",
    "            # Show time range if available\n",
    "            time_cols = [col for col in df.columns if any(t in col.lower() for t in ['time', 'date', 'year'])]\n",
    "            if time_cols:\n",
    "                time_col = time_cols[0]\n",
    "                print(f\"Time range: {df[time_col].min()} to {df[time_col].max()}\")\n",
    "            \n",
    "            # Show precipitation statistics\n",
    "            precip_cols = [col for col in df.columns if any(p in col.lower() for p in ['rain', 'precip', 'precipitation'])]\n",
    "            if precip_cols:\n",
    "                print(f\"\\nPrecipitation statistics for {precip_cols[0]}:\")\n",
    "                print(df[precip_cols[0]].describe())\n",
    "            \n",
    "            # Load the CSV back into pandas to verify\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"VERIFICATION - Loading CSV back into pandas:\")\n",
    "            print(\"=\"*60)\n",
    "            df_loaded = pd.read_csv(output_csv)\n",
    "            print(f\"Loaded DataFrame shape: {df_loaded.shape}\")\n",
    "            print(\"Sample of loaded data:\")\n",
    "            print(df_loaded.head(3))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process NetCDF file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0546e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d259e374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def grib_to_csv_london(grib_file_path, output_csv_path=None):\n",
    "    \"\"\"\n",
    "    Read GRIB file, extract data for London coordinates, and save as CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    grib_file_path (str): Path to the GRIB file\n",
    "    output_csv_path (str): Path for output CSV file (optional)\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with the extracted data\n",
    "    \"\"\"\n",
    "    \n",
    "    # London coordinates\n",
    "    london_lat = 51.5074\n",
    "    london_lon = -0.1278  # Note: GRIB files often use 0-360 longitude\n",
    "    london_lon_360 = 359.8722  # London longitude in 0-360 format\n",
    "    \n",
    "    print(f\"Reading GRIB file: {grib_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Open GRIB file with xarray\n",
    "        ds = xr.open_dataset(grib_file_path, engine='cfgrib')\n",
    "        \n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        print(f\"Dataset dimensions: {dict(ds.dims)}\")\n",
    "        print(f\"Dataset variables: {list(ds.data_vars)}\")\n",
    "        print(f\"Dataset coordinates: {list(ds.coords)}\")\n",
    "        \n",
    "        # Check longitude format (0-360 vs -180-180)\n",
    "        lon_min = ds.longitude.min().values\n",
    "        lon_max = ds.longitude.max().values\n",
    "        print(f\"Longitude range: {lon_min} to {lon_max}\")\n",
    "        \n",
    "        # Use appropriate London longitude based on the dataset's longitude format\n",
    "        if lon_max > 180:\n",
    "            target_lon = london_lon_360\n",
    "            print(f\"Using 0-360 longitude format: {target_lon}\")\n",
    "        else:\n",
    "            target_lon = london_lon\n",
    "            print(f\"Using -180-180 longitude format: {target_lon}\")\n",
    "        \n",
    "        # Find nearest grid point to London\n",
    "        lat_idx = np.abs(ds.latitude - london_lat).argmin()\n",
    "        lon_idx = np.abs(ds.longitude - target_lon).argmin()\n",
    "        \n",
    "        actual_lat = ds.latitude[lat_idx].values\n",
    "        actual_lon = ds.longitude[lon_idx].values\n",
    "        \n",
    "        print(f\"Target coordinates: ({london_lat}, {target_lon})\")\n",
    "        print(f\"Nearest grid point: ({actual_lat}, {actual_lon})\")\n",
    "        \n",
    "        # Extract data for London coordinates\n",
    "        london_data = ds.isel(latitude=lat_idx, longitude=lon_idx)\n",
    "        \n",
    "        # Convert to pandas DataFrame\n",
    "        df_list = []\n",
    "        \n",
    "        for var_name in london_data.data_vars:\n",
    "            var_data = london_data[var_name]\n",
    "            \n",
    "            # Handle different time dimensions\n",
    "            if 'time' in var_data.dims:\n",
    "                # Create DataFrame with time index\n",
    "                df_var = var_data.to_dataframe().reset_index()\n",
    "                df_var = df_var.rename(columns={var_name: var_name})\n",
    "                df_list.append(df_var)\n",
    "            elif 'valid_time' in var_data.dims:\n",
    "                # Handle valid_time dimension\n",
    "                df_var = var_data.to_dataframe().reset_index()\n",
    "                df_var = df_var.rename(columns={var_name: var_name})\n",
    "                df_list.append(df_var)\n",
    "            else:\n",
    "                # Single value variables\n",
    "                df_var = pd.DataFrame({\n",
    "                    var_name: [var_data.values],\n",
    "                    'latitude': [actual_lat],\n",
    "                    'longitude': [actual_lon]\n",
    "                })\n",
    "                df_list.append(df_var)\n",
    "        \n",
    "        # Combine all variables into one DataFrame\n",
    "        if len(df_list) > 1:\n",
    "            # Merge on common columns (time, latitude, longitude)\n",
    "            df = df_list[0]\n",
    "            for df_var in df_list[1:]:\n",
    "                common_cols = set(df.columns) & set(df_var.columns)\n",
    "                if common_cols:\n",
    "                    df = pd.merge(df, df_var, on=list(common_cols), how='outer')\n",
    "                else:\n",
    "                    # If no common columns, concatenate side by side\n",
    "                    df = pd.concat([df, df_var], axis=1)\n",
    "        else:\n",
    "            df = df_list[0]\n",
    "        \n",
    "        # Add metadata columns\n",
    "        df['actual_latitude'] = actual_lat\n",
    "        df['actual_longitude'] = actual_lon\n",
    "        df['target_latitude'] = london_lat\n",
    "        df['target_longitude'] = target_lon\n",
    "        \n",
    "        # Sort by time if time column exists\n",
    "        time_cols = [col for col in df.columns if 'time' in col.lower()]\n",
    "        if time_cols:\n",
    "            df = df.sort_values(time_cols[0])\n",
    "        \n",
    "        print(f\"DataFrame shape: {df.shape}\")\n",
    "        print(f\"DataFrame columns: {list(df.columns)}\")\n",
    "        print(\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Save to CSV\n",
    "        if output_csv_path is None:\n",
    "            output_csv_path = Path(grib_file_path).stem + \"_london_data.csv\"\n",
    "        \n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData saved to: {output_csv_path}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing GRIB file: {e}\")\n",
    "        print(\"Make sure you have the required packages installed:\")\n",
    "        print(\"pip install xarray cfgrib pandas numpy\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "    grib_file = \"./data/ERA5/ERA5_hourly_data_on_single_levels_from_1940_to_present.grib\"  # Replace with your GRIB file path\n",
    "    output_csv = \"./output/london_weather_data.csv\"  # Replace with desired output path\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not Path(grib_file).exists():\n",
    "        print(f\"GRIB file not found: {grib_file}\")\n",
    "        print(\"Please update the 'grib_file' variable with the correct path to your GRIB file.\")\n",
    "        return\n",
    "    \n",
    "    # Process the file\n",
    "    try:\n",
    "        df = grib_to_csv_london(grib_file, output_csv)\n",
    "        \n",
    "        # Display summary statistics\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Total records: {len(df)}\")\n",
    "        \n",
    "        # Show time range if available\n",
    "        time_cols = [col for col in df.columns if 'time' in col.lower()]\n",
    "        if time_cols:\n",
    "            time_col = time_cols[0]\n",
    "            print(f\"Time range: {df[time_col].min()} to {df[time_col].max()}\")\n",
    "        \n",
    "        print(f\"Variables extracted: {[col for col in df.columns if col not in ['latitude', 'longitude', 'actual_latitude', 'actual_longitude', 'target_latitude', 'target_longitude'] + time_cols]}\")\n",
    "        \n",
    "        # Load the CSV back into pandas to verify\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"VERIFICATION - Loading CSV back into pandas:\")\n",
    "        print(\"=\"*50)\n",
    "        df_loaded = pd.read_csv(output_csv)\n",
    "        print(f\"Loaded DataFrame shape: {df_loaded.shape}\")\n",
    "        print(\"DataFrame info:\")\n",
    "        print(df_loaded.info())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process GRIB file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a783e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(\"./output/london_weather_data.csv\")\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad349eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['year'] = df_data['datetime'].dt.year\n",
    "df_data['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe86e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_data\n",
    " .groupby('year', observed=True)['tp']\n",
    " .sum()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f3a762",
   "metadata": {},
   "source": [
    "### Functions Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b672df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1: first working version\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def grib_to_csv_city(grib_file_path, \n",
    "                     city_lat, \n",
    "                     city_lon,\n",
    "                     output_csv_path='./output/',\n",
    "                     city_name = 'london', \n",
    "                     ):\n",
    "    \"\"\"\n",
    "    Read GRIB file, extract data for London coordinates, and save as CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    grib_file_path (str): Path to the GRIB file\n",
    "    output_csv_path (str): Path for output CSV file (optional)\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with the extracted data\n",
    "    \"\"\"\n",
    "    \n",
    "    # # London coordinates\n",
    "    # london_lat = 51.5074\n",
    "    # london_lon = -0.1278  # Note: GRIB files often use 0-360 longitude\n",
    "    # london_lon_360 = 359.8722  # London longitude in 0-360 format\n",
    "    \n",
    "    print(f\"Reading GRIB file: {grib_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Open GRIB file with xarray\n",
    "        ds = xr.open_dataset(grib_file_path, engine='cfgrib')\n",
    "        \n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        print(f\"Dataset dimensions: {dict(ds.dims)}\")\n",
    "        print(f\"Dataset variables: {list(ds.data_vars)}\")\n",
    "        print(f\"Dataset coordinates: {list(ds.coords)}\")\n",
    "        \n",
    "        # Check longitude format (0-360 vs -180-180)\n",
    "        city_min = ds.longitude.min().values\n",
    "        city_max = ds.longitude.max().values\n",
    "        print(f\"Longitude range: {city_min} to {city_max}\")\n",
    "        \n",
    "        # Use appropriate London longitude based on the dataset's longitude format\n",
    "        # if city_max > 180:\n",
    "        #     target_lon = london_lon_360\n",
    "        #     print(f\"Using 0-360 longitude format: {target_lon}\")\n",
    "        # else:\n",
    "        target_city = city_lon\n",
    "        print(f\"Using -180-180 longitude format: {target_city}\")\n",
    "        \n",
    "        # Find nearest grid point to London\n",
    "        lat_idx = np.abs(ds.latitude - city_lat).argmin()\n",
    "        lon_idx = np.abs(ds.longitude - target_city).argmin()\n",
    "        \n",
    "        actual_lat = ds.latitude[lat_idx].values\n",
    "        actual_lon = ds.longitude[lon_idx].values\n",
    "        \n",
    "        print(f\"Target coordinates: ({city_lat}, {target_city})\")\n",
    "        print(f\"Nearest grid point: ({actual_lat}, {actual_lon})\")\n",
    "        \n",
    "        # Extract data for London coordinates\n",
    "        city_data = ds.isel(latitude=lat_idx, longitude=lon_idx)\n",
    "        \n",
    "        # Convert to pandas DataFrame\n",
    "        df_list = []\n",
    "        \n",
    "        for var_name in city_data.data_vars:\n",
    "            var_data = city_data[var_name]\n",
    "            \n",
    "            # Handle different time dimensions\n",
    "            if 'time' in var_data.dims:\n",
    "                # Create DataFrame with time index\n",
    "                df_var = var_data.to_dataframe().reset_index()\n",
    "                df_var = df_var.rename(columns={var_name: var_name})\n",
    "                df_list.append(df_var)\n",
    "            elif 'valid_time' in var_data.dims:\n",
    "                # Handle valid_time dimension\n",
    "                df_var = var_data.to_dataframe().reset_index()\n",
    "                df_var = df_var.rename(columns={var_name: var_name})\n",
    "                df_list.append(df_var)\n",
    "            else:\n",
    "                # Single value variables\n",
    "                df_var = pd.DataFrame({\n",
    "                    var_name: [var_data.values],\n",
    "                    'latitude': [actual_lat],\n",
    "                    'longitude': [actual_lon]\n",
    "                })\n",
    "                df_list.append(df_var)\n",
    "        \n",
    "        # Combine all variables into one DataFrame\n",
    "        if len(df_list) > 1:\n",
    "            # Merge on common columns (time, latitude, longitude)\n",
    "            df = df_list[0]\n",
    "            for df_var in df_list[1:]:\n",
    "                common_cols = set(df.columns) & set(df_var.columns)\n",
    "                if common_cols:\n",
    "                    df = pd.merge(df, df_var, on=list(common_cols), how='outer')\n",
    "                else:\n",
    "                    # If no common columns, concatenate side by side\n",
    "                    df = pd.concat([df, df_var], axis=1)\n",
    "        else:\n",
    "            df = df_list[0]\n",
    "        \n",
    "        # Add metadata columns\n",
    "        df['actual_latitude'] = actual_lat\n",
    "        df['actual_longitude'] = actual_lon\n",
    "        df['target_latitude'] = city_lat\n",
    "        df['target_longitude'] = target_city\n",
    "        \n",
    "        # Sort by time if time column exists\n",
    "        time_cols = [col for col in df.columns if 'time' in col.lower()]\n",
    "        if time_cols:\n",
    "            time_col = time_cols[0]\n",
    "            df = df.sort_values(time_cols[0])\n",
    "            print(f\"****\\n - inside csv Time range: {df[time_col].min()} to {df[time_col].max()}\")\n",
    "            min_string = df[time_col].min().strftime('%Y%m%d')\n",
    "            max_string = df[time_col].max().strftime('%Y%m%d')\n",
    "            print(f\" - min time as string: {min_string}\")\n",
    "            print(f\" - max time as string: {max_string}\")\n",
    "            output_csv_name = list(ds.data_vars)[0] + '_' + min_string + '_' + max_string\n",
    "            print(output_csv_name)\n",
    "        \n",
    "        print(f\"DataFrame shape: {df.shape}\")\n",
    "        print(f\"DataFrame columns: {list(df.columns)}\")\n",
    "        print(\"grib to csv\")\n",
    "        print(f\"ds.data_vars dtype: {type(ds.data_vars)}, df.data_vars name: {list(ds.data_vars)[0]}\")\n",
    "        # print(\"\\nFirst few rows:\")\n",
    "        # print(df.head())\n",
    "        \n",
    "        # Save to CSV\n",
    "        if output_csv_path is None:\n",
    "            output_csv_path = Path(grib_file_path).stem + \"_city_data.csv\"\n",
    "        \n",
    "        output_csv_path = output_csv_path + city_name + '_' + output_csv_name + '.csv'\n",
    "        print(f\"Saving DataFrame to: {output_csv_path}\")\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData saved to: {output_csv_path}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing GRIB file: {e}\")\n",
    "        print(\"Make sure you have the required packages installed:\")\n",
    "        print(\"pip install xarray cfgrib pandas numpy\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e44cdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1\n",
    "\n",
    "def process_grib_file(grib_file_path,\n",
    "                      output_csv_path='./output/',\n",
    "                      city_name='london',\n",
    "                      city_lat=51.5074,  # Default to London latitude\n",
    "                      city_lon=-0.1278):  # Default to London longitude\n",
    "    \n",
    "    # Example usage\n",
    "    grib_file = grib_file_path  # Replace with your GRIB file path\n",
    "    output_csv_city = city_name  # Replace with desired output path\n",
    "    output_csv_path = output_csv_path  # Directory to save the output CSV\n",
    "\n",
    "    # Check if file exists\n",
    "    if not Path(grib_file).exists():\n",
    "        print(f\"GRIB file not found: {grib_file}\")\n",
    "        print(\"Please update the 'grib_file' variable with the correct path to your GRIB file.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        df = grib_to_csv_city(grib_file, \n",
    "                              city_lat=city_lat, \n",
    "                              city_lon=-city_lon, output_csv_path=output_csv_path, city_name=output_csv_city)\n",
    "        \n",
    "        # Display summary statistics\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Total records: {len(df)}\")\n",
    "        \n",
    "        # Show time range if available\n",
    "        time_cols = [col for col in df.columns if 'time' in col.lower()]\n",
    "        if time_cols:\n",
    "            time_col = time_cols[0]\n",
    "            print(f\"Time range: {df[time_col].min()} to {df[time_col].max()}\")\n",
    "        \n",
    "        print(f\"Variables extracted: {[col for col in df.columns if col not in ['latitude', 'longitude', 'actual_latitude', 'actual_longitude', 'target_latitude', 'target_longitude'] + time_cols]}\")\n",
    "        \n",
    "        # # Load the CSV back into pandas to verify\n",
    "        # print(\"\\n\" + \"=\"*50)\n",
    "        # print(\"VERIFICATION - Loading CSV back into pandas:\")\n",
    "        # print(\"=\"*50)\n",
    "\n",
    "        # df_loaded = pd.read_csv(output_csv)\n",
    "        # print(f\"Loaded DataFrame shape: {df_loaded.shape}\")\n",
    "        # print(\"DataFrame info:\")\n",
    "        # print(df_loaded.info())\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process GRIB file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9456cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def netcdf_to_csv_london(nc_file_path, output_csv_path=None):\n",
    "    \"\"\"\n",
    "    Read NetCDF file, extract precipitation data for London coordinates, and save as CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    nc_file_path (str): Path to the NetCDF file\n",
    "    output_csv_path (str): Path for output CSV file (optional)\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with the extracted data\n",
    "    \"\"\"\n",
    "    \n",
    "    # London coordinates\n",
    "    london_lat = 51.5074\n",
    "    london_lon = -0.1278\n",
    "    \n",
    "    print(f\"Reading NetCDF file: {nc_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Open NetCDF file with xarray\n",
    "        ds = xr.open_dataset(nc_file_path)\n",
    "        \n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        print(f\"Dataset dimensions: {dict(ds.dims)}\")\n",
    "        print(f\"Dataset variables: {list(ds.data_vars)}\")\n",
    "        print(f\"Dataset coordinates: {list(ds.coords)}\")\n",
    "        \n",
    "        # Display dataset info\n",
    "        print(\"\\nDataset overview:\")\n",
    "        print(ds)\n",
    "        \n",
    "        # Check coordinate names (common variations)\n",
    "        lat_names = ['lat', 'latitude', 'y', 'projection_y_coordinate']\n",
    "        lon_names = ['lon', 'longitude', 'x', 'projection_x_coordinate']\n",
    "        time_names = ['time', 'time_bnds', 'yyyymm']\n",
    "        \n",
    "        # Find actual coordinate names in the dataset\n",
    "        lat_coord = None\n",
    "        lon_coord = None\n",
    "        time_coord = None\n",
    "        \n",
    "        for name in lat_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                lat_coord = name\n",
    "                break\n",
    "                \n",
    "        for name in lon_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                lon_coord = name\n",
    "                break\n",
    "                \n",
    "        for name in time_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                time_coord = name\n",
    "                break\n",
    "        \n",
    "        print(f\"\\nIdentified coordinates:\")\n",
    "        print(f\"Latitude coordinate: {lat_coord}\")\n",
    "        print(f\"Longitude coordinate: {lon_coord}\")\n",
    "        print(f\"Time coordinate: {time_coord}\")\n",
    "        \n",
    "        if lat_coord is None or lon_coord is None:\n",
    "            print(\"Warning: Could not identify latitude/longitude coordinates\")\n",
    "            print(\"Available coordinates:\", list(ds.coords))\n",
    "            return None\n",
    "        \n",
    "        # Check coordinate ranges\n",
    "        if lat_coord in ds.coords:\n",
    "            lat_min = ds.coords[lat_coord].min().values\n",
    "            lat_max = ds.coords[lat_coord].max().values\n",
    "            print(f\"Latitude range: {lat_min} to {lat_max}\")\n",
    "        \n",
    "        if lon_coord in ds.coords:\n",
    "            lon_min = ds.coords[lon_coord].min().values\n",
    "            lon_max = ds.coords[lon_coord].max().values\n",
    "            print(f\"Longitude range: {lon_min} to {lon_max}\")\n",
    "        \n",
    "        # Handle projected coordinates (if using OSGB or similar)\n",
    "        if 'projection' in str(ds.coords) or lat_coord in ['y', 'projection_y_coordinate']:\n",
    "            print(\"\\nDetected projected coordinates (likely OSGB)\")\n",
    "            print(\"You may need to convert London lat/lon to the projection coordinates\")\n",
    "            print(\"For now, finding nearest grid point in projected space...\")\n",
    "        \n",
    "        # Find nearest grid point to London\n",
    "        if lat_coord in ds.coords and lon_coord in ds.coords:\n",
    "            lat_idx = np.abs(ds.coords[lat_coord] - london_lat).argmin()\n",
    "            lon_idx = np.abs(ds.coords[lon_coord] - london_lon).argmin()\n",
    "            \n",
    "            actual_lat = ds.coords[lat_coord][lat_idx].values\n",
    "            actual_lon = ds.coords[lon_coord][lon_idx].values\n",
    "            \n",
    "            print(f\"\\nTarget coordinates: ({london_lat}, {london_lon})\")\n",
    "            print(f\"Nearest grid point: ({actual_lat}, {actual_lon})\")\n",
    "        else:\n",
    "            print(\"Could not find coordinate values for nearest point calculation\")\n",
    "            return None\n",
    "        \n",
    "        # Extract data for London coordinates\n",
    "        selection_dict = {lat_coord: lat_idx, lon_coord: lon_idx}\n",
    "        london_data = ds.isel(selection_dict)\n",
    "        \n",
    "        print(f\"\\nExtracted data variables: {list(london_data.data_vars)}\")\n",
    "        \n",
    "        # Convert to pandas DataFrame\n",
    "        df_list = []\n",
    "        \n",
    "        for var_name in london_data.data_vars:\n",
    "            var_data = london_data[var_name]\n",
    "            print(f\"\\nProcessing variable: {var_name}\")\n",
    "            print(f\"Variable dimensions: {var_data.dims}\")\n",
    "            print(f\"Variable shape: {var_data.shape}\")\n",
    "            \n",
    "            # Get variable attributes\n",
    "            if hasattr(var_data, 'attrs'):\n",
    "                print(f\"Variable attributes: {var_data.attrs}\")\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            if time_coord and time_coord in var_data.dims:\n",
    "                # Time series data\n",
    "                df_var = var_data.to_dataframe().reset_index()\n",
    "                \n",
    "                # Rename the variable column to be more descriptive\n",
    "                if var_name in df_var.columns:\n",
    "                    # Keep original variable name but add units if available\n",
    "                    units = var_data.attrs.get('units', '')\n",
    "                    long_name = var_data.attrs.get('long_name', var_name)\n",
    "                    \n",
    "                    col_name = var_name\n",
    "                    if units:\n",
    "                        col_name += f\"_{units}\".replace(\" \", \"_\")\n",
    "                    \n",
    "                    df_var = df_var.rename(columns={var_name: col_name})\n",
    "                    \n",
    "                    # Add metadata as separate columns\n",
    "                    df_var[f'{var_name}_long_name'] = long_name\n",
    "                    df_var[f'{var_name}_units'] = units\n",
    "                \n",
    "                df_list.append(df_var)\n",
    "            else:\n",
    "                # Single value or no time dimension\n",
    "                df_var = pd.DataFrame({\n",
    "                    var_name: [var_data.values],\n",
    "                    'latitude': [actual_lat],\n",
    "                    'longitude': [actual_lon]\n",
    "                })\n",
    "                \n",
    "                # Add metadata\n",
    "                if hasattr(var_data, 'attrs'):\n",
    "                    for attr_name, attr_value in var_data.attrs.items():\n",
    "                        df_var[f'{var_name}_{attr_name}'] = str(attr_value)\n",
    "                \n",
    "                df_list.append(df_var)\n",
    "        \n",
    "        # Combine all variables into one DataFrame\n",
    "        if len(df_list) > 1:\n",
    "            # Find common columns for merging\n",
    "            df = df_list[0]\n",
    "            for df_var in df_list[1:]:\n",
    "                common_cols = set(df.columns) & set(df_var.columns)\n",
    "                if common_cols and len(common_cols) > 1:  # More than just the variable column\n",
    "                    df = pd.merge(df, df_var, on=list(common_cols), how='outer')\n",
    "                else:\n",
    "                    # Concatenate side by side if same length\n",
    "                    if len(df) == len(df_var):\n",
    "                        df = pd.concat([df, df_var], axis=1)\n",
    "                    else:\n",
    "                        print(f\"Warning: Cannot merge {df_var.columns} - different lengths\")\n",
    "        else:\n",
    "            df = df_list[0] if df_list else pd.DataFrame()\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"Warning: No data extracted\")\n",
    "            return None\n",
    "        \n",
    "        # Add location metadata\n",
    "        df['actual_latitude'] = actual_lat\n",
    "        df['actual_longitude'] = actual_lon\n",
    "        df['target_latitude'] = london_lat\n",
    "        df['target_longitude'] = london_lon\n",
    "        df['coordinate_system'] = f\"{lat_coord}, {lon_coord}\"\n",
    "        \n",
    "        # Sort by time if time column exists\n",
    "        time_cols = [col for col in df.columns if any(t in col.lower() for t in ['time', 'date', 'year'])]\n",
    "        if time_cols:\n",
    "            df = df.sort_values(time_cols[0])\n",
    "            print(f\"Sorted by time column: {time_cols[0]}\")\n",
    "        \n",
    "        print(f\"\\nFinal DataFrame shape: {df.shape}\")\n",
    "        print(f\"DataFrame columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Show data types\n",
    "        print(f\"\\nData types:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"  {col}: {df[col].dtype}\")\n",
    "        \n",
    "        print(f\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        if len(df) > 5:\n",
    "            print(f\"\\nLast few rows:\")\n",
    "            print(df.tail())\n",
    "        \n",
    "        # Save to CSV\n",
    "        if output_csv_path is None:\n",
    "            output_csv_path = Path(nc_file_path).stem + \"_london_precipitation.csv\"\n",
    "        \n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData saved to: {output_csv_path}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing NetCDF file: {e}\")\n",
    "        print(\"Make sure you have the required packages installed:\")\n",
    "        print(\"pip install xarray netcdf4 pandas numpy\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # Your specific file\n",
    "    nc_file = \"./data/ERA5/rainfall_hadukgrid_uk_1km_ann-30y_196101-199012.nc\"\n",
    "    output_csv = \"london_rainfall_1961_1990.csv\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not Path(nc_file).exists():\n",
    "        print(f\"NetCDF file not found: {nc_file}\")\n",
    "        print(\"Please make sure the file is in the current directory or update the path.\")\n",
    "        return\n",
    "    \n",
    "    # Process the file\n",
    "    try:\n",
    "        df = netcdf_to_csv_london(nc_file, output_csv)\n",
    "        \n",
    "        if df is not None:\n",
    "            # Display summary statistics\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"SUMMARY\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"Total records: {len(df)}\")\n",
    "            \n",
    "            # Show time range if available\n",
    "            time_cols = [col for col in df.columns if any(t in col.lower() for t in ['time', 'date', 'year'])]\n",
    "            if time_cols:\n",
    "                time_col = time_cols[0]\n",
    "                print(f\"Time range: {df[time_col].min()} to {df[time_col].max()}\")\n",
    "            \n",
    "            # Show precipitation statistics\n",
    "            precip_cols = [col for col in df.columns if any(p in col.lower() for p in ['rain', 'precip', 'precipitation'])]\n",
    "            if precip_cols:\n",
    "                print(f\"\\nPrecipitation statistics for {precip_cols[0]}:\")\n",
    "                print(df[precip_cols[0]].describe())\n",
    "            \n",
    "            # Load the CSV back into pandas to verify\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"VERIFICATION - Loading CSV back into pandas:\")\n",
    "            print(\"=\"*60)\n",
    "            df_loaded = pd.read_csv(output_csv)\n",
    "            print(f\"Loaded DataFrame shape: {df_loaded.shape}\")\n",
    "            print(\"Sample of loaded data:\")\n",
    "            print(df_loaded.head(3))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process NetCDF file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d75c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f35646c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac3ca4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47bcf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygrib\n",
    "\n",
    "# Open GRIB file\n",
    "grbs = pygrib.open('./data/era5_monthly_averaged_data.grib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f302de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grib_file = './data/era5_monthly_averaged_data.grib'\n",
    "# grbs2 = xr.open_dataset(grib_file, engine='cfgrib') #, backend_kwargs=backend_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e323972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xarray as xr\n",
    "\n",
    "# grib_file = './data/era5_monthly_averaged_data.grib'\n",
    "\n",
    "# # Try the first grid resolution\n",
    "# try:\n",
    "#     grbs2 = xr.open_dataset(grib_file, \n",
    "#                            engine='cfgrib',\n",
    "#                            backend_kwargs={'filter_by_keys': {'numberOfPoints': 826}})\n",
    "#     print(\"Successfully opened with 826 grid points\")\n",
    "#     print(f\"Variables: {list(grbs2.data_vars)}\")\n",
    "#     print(f\"Shape: {grbs2.dims}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Failed with 826 points: {e}\")\n",
    "\n",
    "# # Try the second grid resolution\n",
    "# try:\n",
    "#     grbs2_alt = xr.open_dataset(grib_file, \n",
    "#                                engine='cfgrib',\n",
    "#                                backend_kwargs={'filter_by_keys': {'numberOfPoints': 207}})\n",
    "#     print(\"Successfully opened with 207 grid points\")\n",
    "#     print(f\"Variables: {list(grbs2_alt.data_vars)}\")\n",
    "#     print(f\"Shape: {grbs2_alt.dims}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Failed with 207 points: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641296a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grib_file = './data/era5_monthly_averaged_data.grib'\n",
    "\n",
    "# # Define all combinations to try\n",
    "# filter_combinations = [\n",
    "#     {'stepType': 'avgad', 'numberOfPoints': 826},\n",
    "#     {'stepType': 'avgas', 'numberOfPoints': 826},\n",
    "#     {'stepType': 'avgad', 'numberOfPoints': 207},\n",
    "#     {'stepType': 'avgas', 'numberOfPoints': 207}\n",
    "# ]\n",
    "\n",
    "# datasets = {}\n",
    "\n",
    "# for i, filters in enumerate(filter_combinations):\n",
    "#     try:\n",
    "#         ds = xr.open_dataset(grib_file, \n",
    "#                            engine='cfgrib',\n",
    "#                            backend_kwargs={'filter_by_keys': filters})\n",
    "        \n",
    "#         dataset_name = f\"{filters['stepType']}_{filters['numberOfPoints']}pts\"\n",
    "#         datasets[dataset_name] = ds\n",
    "        \n",
    "#         print(f\"\\n✅ Successfully opened: {dataset_name}\")\n",
    "#         print(f\"   Variables: {list(ds.data_vars)}\")\n",
    "#         print(f\"   Dimensions: {ds.dims}\")\n",
    "#         print(f\"   Coordinates: {list(ds.coords)}\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Failed {filters}: {str(e)[:100]}...\")\n",
    "\n",
    "# print(f\"\\n📊 Total datasets loaded: {len(datasets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41b4e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xarray as xr\n",
    "# import pandas as pd\n",
    "\n",
    "# grib_file = './data/era5_monthly_averaged_data.grib'\n",
    "\n",
    "# def process_all_era5_data(grib_file):\n",
    "#     \"\"\"Process all combinations and return organized data\"\"\"\n",
    "    \n",
    "#     filter_combinations = [\n",
    "#         {'stepType': 'avgad', 'numberOfPoints': 826},\n",
    "#         {'stepType': 'avgas', 'numberOfPoints': 826},\n",
    "#         {'stepType': 'avgad', 'numberOfPoints': 207},\n",
    "#         {'stepType': 'avgas', 'numberOfPoints': 207}\n",
    "#     ]\n",
    "    \n",
    "#     all_data = {}\n",
    "    \n",
    "#     for filters in filter_combinations:\n",
    "#         try:\n",
    "#             # Open dataset\n",
    "#             ds = xr.open_dataset(grib_file, \n",
    "#                                engine='cfgrib',\n",
    "#                                backend_kwargs={'filter_by_keys': filters})\n",
    "            \n",
    "#             # Create descriptive name\n",
    "#             step_type = filters['stepType']\n",
    "#             grid_size = filters['numberOfPoints']\n",
    "#             dataset_name = f\"{step_type}_{grid_size}pts\"\n",
    "            \n",
    "#             # Convert to DataFrame\n",
    "#             df = ds.to_dataframe().reset_index()\n",
    "            \n",
    "#             # Add metadata\n",
    "#             df['step_type'] = step_type\n",
    "#             df['grid_points'] = grid_size\n",
    "#             df['data_type'] = 'accumulated' if step_type == 'avgad' else 'instantaneous'\n",
    "            \n",
    "#             all_data[dataset_name] = {\n",
    "#                 'dataset': ds,\n",
    "#                 'dataframe': df,\n",
    "#                 'variables': list(ds.data_vars),\n",
    "#                 'shape': df.shape\n",
    "#             }\n",
    "            \n",
    "#             print(f\"✅ {dataset_name}:\")\n",
    "#             print(f\"   Shape: {df.shape}\")\n",
    "#             print(f\"   Variables: {list(ds.data_vars)}\")\n",
    "#             print(f\"   Sample data types: {df.dtypes.to_dict()}\")\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ Failed {filters}: {e}\")\n",
    "    \n",
    "#     return all_data\n",
    "\n",
    "# # Process all data\n",
    "# all_era5_data = process_all_era5_data(grib_file)\n",
    "\n",
    "# # Save each dataset to CSV\n",
    "# for name, data_info in all_era5_data.items():\n",
    "#     df = data_info['dataframe']\n",
    "#     filename = f'era5_{name}.csv'\n",
    "#     df.to_csv(filename, index=False)\n",
    "#     print(f\"💾 Saved {name} to {filename}\")\n",
    "\n",
    "# # Display summary\n",
    "# print(f\"\\n📋 Summary:\")\n",
    "# for name, data_info in all_era5_data.items():\n",
    "#     print(f\"{name}: {data_info['shape'][0]} rows, Variables: {data_info['variables']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfede07a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826962af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef92cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bce793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5fa512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f956cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open the NetCDF file\n",
    "# ds = xr.open_dataset(\"./data/london_test_2.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee382131",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ab6f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rain = ds['tp'].sel(latitude=51.5, longitude=0.0, method='nearest')\n",
    "rain.plot()\n",
    "plt.title('Daily Precipitation in London')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b3a417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select time series at closest grid point to London\n",
    "rain_series = ds['tp'].sel(latitude=51.5, longitude=0.0, method=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb8bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame\n",
    "df = rain_series.to_dataframe().reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9e02c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RR_mm'] = df['tp'] * 1000\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff307884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_manipulation import transform_data_datetime\n",
    "\n",
    "#london_data = transform_data_datetime(df=df.rename(columns={'valid_time' : 'DATE'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c770df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# london_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d072b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2efb993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74a5979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fab6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999ef925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb0f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific variable\n",
    "grb = grbs.select(name='Total precipitation')[0]\n",
    "data, lats, lons = grb.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134def1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd9ee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "lons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8d3162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read GRIB file\n",
    "ds = xr.open_dataset('./data/london_data.grib', engine='cfgrib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90964df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to open only GRIB2 messages\n",
    "try:\n",
    "    ds = xr.open_dataset('./data/london_data.grib', \n",
    "                        engine='cfgrib',\n",
    "                        backend_kwargs={'filter_by_keys': {'edition': 2}})\n",
    "    print(\"Successfully opened GRIB2 data\")\n",
    "except:\n",
    "    # If that fails, try GRIB1\n",
    "    ds = xr.open_dataset('./data/london_data.grib', \n",
    "                        engine='cfgrib',\n",
    "                        backend_kwargs={'filter_by_keys': {'edition': 1}})\n",
    "    print(\"Successfully opened GRIB1 data\")\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c9e3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset\n",
    "print(\"Variables:\", list(ds.data_vars))\n",
    "print(\"Coordinates:\", list(ds.coords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccf5c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['surface'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be9f753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
