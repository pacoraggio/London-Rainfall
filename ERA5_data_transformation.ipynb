{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96b58b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.data_manipulation import transform_data_datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c5a2dc",
   "metadata": {},
   "source": [
    "# ERA5 datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76f3ab1",
   "metadata": {},
   "source": [
    "Scope of the notebook: once downloaded datasets from Copernicus website, we implement functions to:\n",
    "- Read `grib` files\n",
    "- Produce Hourly, daily, monthly, and yearly dataframes for Total Precipitation and 2 meters Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3ab7cd",
   "metadata": {},
   "source": [
    "## Reading `grib` files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b580ae",
   "metadata": {},
   "source": [
    "### Grib Files Parameters for London and Puglia region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2897389",
   "metadata": {},
   "source": [
    "Data are downloaded from the [Copernicus Website](https://cds.climate.copernicus.eu/requests?tab=all)\n",
    "- We can can double check London Data using the [Weather and Climate website ](https://weatherandclimate.com/london/july-2013)\n",
    "\n",
    "#### London GRIB grid Parameters\n",
    "\n",
    "For London, UK, you'll want to define a geographical area that encompasses the Greater London region. Here are suitable values for the grid boundaries:\n",
    "- North: 51.7°N\n",
    "- South: 51.3°N\n",
    "- West: -0.5°W\n",
    "- East: 0.3°E\n",
    "These coordinates create a rectangular grid that covers Greater London and a small buffer around it. London's city center is approximately at 51.5°N, 0.1°W, so this grid provides good coverage while keeping the data volume manageable.\n",
    "\n",
    "If you want a smaller area focused just on central London, you could use tighter bounds:\n",
    "\n",
    "North: 51.6°N\n",
    "South: 51.4°N\n",
    "West: -0.3°W\n",
    "East: 0.1°E\n",
    "\n",
    "Or if you need a larger area covering the broader London metropolitan region, you could expand to:\n",
    "\n",
    "North: 52.0°N\n",
    "South: 51.0°N\n",
    "West: -1.0°W\n",
    "East: 1.0°E\n",
    "\n",
    "The first set of coordinates (51.7°N, 51.3°N, -0.5°W, 0.3°E) is probably your best bet for most applications - it captures London proper without being unnecessarily large.\n",
    "\n",
    "\n",
    "```\n",
    "North: 51.6\n",
    "West:  -0.2\n",
    "South: 51.4\n",
    "East:   0.1\n",
    "```\n",
    "\n",
    "#### Puglia Region GRIB grid Parameters\n",
    "\n",
    "For comparing London and Puglia climate data, here's a suitable approach:\n",
    "\n",
    "- Grid Selection Strategy\n",
    "Option 1: Two separate queries (Recommended)\n",
    "\n",
    "Query London area: North 51.7°N, South 51.3°N, West -0.5°W, East 0.3°E\n",
    "Query Puglia area: North 41.9°N, South 39.8°N, West 15.5°E, East 18.5°E\n",
    "\n",
    "Option 2: Single large query covering both regions\n",
    "\n",
    "North 51.7°N, South 39.8°N, West -0.5°W, East 18.5°E\n",
    "This creates a massive grid covering most of Europe, resulting in much larger files\n",
    "\n",
    "I'd recommend Option 1 as it's more efficient and gives you focused data for each region.\n",
    "Puglia Grid Coordinates\n",
    "Puglia (Apulia) occupies the \"heel\" of Italy's boot:\n",
    "\n",
    "North: 41.9°N (around Foggia)\n",
    "South: 39.8°N (around Santa Maria di Leuca)\n",
    "West: 15.5°E (around Bari)\n",
    "East: 18.5°E (Adriatic coast)\n",
    "\n",
    "Data Extraction from GRIB Files\n",
    "Once you have the .grib files, extract data using these approaches:\n",
    "1. Point-based comparison:\n",
    "\n",
    "Extract data for specific coordinates (e.g., London center: 51.5°N, -0.1°W; Bari: 41.1°N, 16.9°E)\n",
    "Use tools like grib_get_data or Python libraries (xarray, pygrib)\n",
    "\n",
    "2. Area-averaged comparison:\n",
    "\n",
    "Calculate spatial averages for each region\n",
    "This smooths out local variations and gives regional climate signals\n",
    "\n",
    "3. Representative city comparison:\n",
    "\n",
    "London: Extract nearest grid point to 51.5°N, -0.1°W\n",
    "Puglia: Extract for major cities like Bari (41.1°N, 16.9°E) or Lecce (40.4°N, 18.2°E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d315208",
   "metadata": {},
   "source": [
    "> grib_file = \"./data/ERA5/ERA5_hourly_data_on_single_levels_from_1940_to_present.grib\"  # Replace with your GRIB file path\n",
    "- output_csv = \"./output/ERA5_hourly_data_on_single_levels_from_1940_to_present.csv\"  # Replace with desired output path\n",
    "- hourly data from 2022 to 2024\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c9142c",
   "metadata": {},
   "source": [
    "### GRIB to csv conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aec339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2: first working version\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import cfgrib\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def grib_to_csv(grib_file_path,\n",
    "                city_lat,\n",
    "                city_lon,\n",
    "                output_variable='tp', # output variable 'tp' for total precipitation, 't2m' for temperature at 2m\n",
    "                city_name = 'london', # city name for output file 'london', 'puglia'\n",
    "                print_debug=True,\n",
    "                ):\n",
    "    \"\"\"\n",
    "    Read GRIB file, extract data for London coordinates, and save as CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    grib_file_path (str): Path to the GRIB file\n",
    "    output_csv_path (str): Path for output CSV file (optional)\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with the extracted data\n",
    "    \"\"\"\n",
    "    if print_debug:\n",
    "        print(f\"Reading GRIB file: {grib_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Open GRIB file with xarray\n",
    "        print('breakpoint 0') \n",
    "        datasets = cfgrib.open_datasets(grib_file_path)\n",
    "        for i, ds in enumerate(datasets):\n",
    "            print(f\"Dataset {i}:\")\n",
    "            print(f\"Variables: {list(ds.data_vars.keys())}\")\n",
    "            print(f\"Time range: {ds.time.min().values} to {ds.time.max().values}\")\n",
    "            print(\"---\")\n",
    "        \n",
    "        ds = xr.open_dataset(grib_file_path, engine='cfgrib')\n",
    "        print('breakpoint 1')        \n",
    "        # Check longitude format (0-360 vs -180-180)\n",
    "        city_min = ds.longitude.min().values\n",
    "        city_max = ds.longitude.max().values\n",
    "        if print_debug:\n",
    "            print(f\"Longitude range: {city_min} to {city_max}\")\n",
    "        \n",
    "        target_city = city_lon\n",
    "        if print_debug:\n",
    "            print(f\"Using -180-180 longitude format: {target_city}\")\n",
    "        \n",
    "        # Find nearest grid point to London\n",
    "        lat_idx = np.abs(ds.latitude - city_lat).argmin()\n",
    "        lon_idx = np.abs(ds.longitude - target_city).argmin()\n",
    "        \n",
    "        actual_lat = ds.latitude[lat_idx].values\n",
    "        actual_lon = ds.longitude[lon_idx].values\n",
    "        \n",
    "        if print_debug:\n",
    "            print(f\"Target coordinates: ({city_lat}, {target_city})\")\n",
    "            print(f\"Nearest grid point: ({actual_lat}, {actual_lon})\")\n",
    "        \n",
    "        # Extract data for London coordinates\n",
    "        city_data = ds.isel(latitude=lat_idx, longitude=lon_idx)\n",
    "        \n",
    "        # Convert to pandas DataFrame\n",
    "        df_list = []\n",
    "        \n",
    "        for var_name in city_data.data_vars:\n",
    "            var_data = city_data[var_name]\n",
    "            \n",
    "            # Handle different time dimensions\n",
    "            if 'time' in var_data.dims:\n",
    "                # Create DataFrame with time index\n",
    "                df_var = var_data.to_dataframe().reset_index()\n",
    "                df_var = df_var.rename(columns={var_name: var_name})\n",
    "                df_list.append(df_var)\n",
    "            elif 'valid_time' in var_data.dims:\n",
    "                # Handle valid_time dimension\n",
    "                df_var = var_data.to_dataframe().reset_index()\n",
    "                df_var = df_var.rename(columns={var_name: var_name})\n",
    "                df_list.append(df_var)\n",
    "            else:\n",
    "                # Single value variables\n",
    "                df_var = pd.DataFrame({\n",
    "                    var_name: [var_data.values],\n",
    "                    'latitude': [actual_lat],\n",
    "                    'longitude': [actual_lon]\n",
    "                })\n",
    "                df_list.append(df_var)\n",
    "        \n",
    "        # Combine all variables into one DataFrame\n",
    "        if len(df_list) > 1:\n",
    "            # Merge on common columns (time, latitude, longitude)\n",
    "            df = df_list[0]\n",
    "            for df_var in df_list[1:]:\n",
    "                common_cols = set(df.columns) & set(df_var.columns)\n",
    "                if common_cols:\n",
    "                    df = pd.merge(df, df_var, on=list(common_cols), how='outer')\n",
    "                else:\n",
    "                    # If no common columns, concatenate side by side\n",
    "                    df = pd.concat([df, df_var], axis=1)\n",
    "        else:\n",
    "            df = df_list[0]\n",
    "        \n",
    "        df['target_latitude'] = city_lat\n",
    "        df['target_longitude'] = target_city\n",
    "        \n",
    "        # Sort by time if time column exists\n",
    "        time_cols = [col for col in df.columns if 'time' in col.lower()]\n",
    "        if time_cols:\n",
    "            time_col = time_cols[0]\n",
    "            df = df.sort_values(time_cols[0])\n",
    "            if print_debug:\n",
    "                print(f\"****\\n - inside csv Time range: {df[time_col].min()} to {df[time_col].max()}\")\n",
    "            min_string = df[time_col].min().strftime('%Y%m%d')\n",
    "            max_string = df[time_col].max().strftime('%Y%m%d')\n",
    "            if print_debug:\n",
    "                print(f\" - min time as string: {min_string}\")\n",
    "                print(f\" - max time as string: {max_string}\")\n",
    "            output_csv_name = list(ds.data_vars)[0] + '_' + min_string + '_' + max_string\n",
    "            if print_debug:\n",
    "                print(output_csv_name)\n",
    "        \n",
    "        output_csv_name = city_name + '_' + output_csv_name\n",
    "        if print_debug:\n",
    "            print(f\"DataFrame shape: {df.shape}\")\n",
    "            print(f\"DataFrame columns: {list(df.columns)}\")\n",
    "            print(\"grib to csv\")\n",
    "            print(f\"ds.data_vars dtype: {type(ds.data_vars)}, df.data_vars name: {list(ds.data_vars)[0]}\")\n",
    "            output_csv_path = './output/' + output_variable + '//' + city_name + '//' + output_csv_name + '.csv'\n",
    "            print(output_csv_path)\n",
    "        # print(\"\\nFirst few rows:\")\n",
    "        # print(df.head())\n",
    "        \n",
    "        # Save to CSV\n",
    "        if output_csv_path is None:\n",
    "            output_csv_path = Path(grib_file_path).stem + \"_city_data.csv\"\n",
    "        \n",
    "        output_csv_path = './output/' + output_variable + '//' + city_name + '//' + output_csv_name + '.csv'\n",
    "        print(output_csv_path)\n",
    "        if print_debug:\n",
    "            print(f\"Saving DataFrame to: {output_csv_path}\")\n",
    "        \n",
    "\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData saved to: {output_csv_path}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing GRIB file: {e}\")\n",
    "        print(\"Make sure you have the required packages installed:\")\n",
    "        print(\"pip install xarray cfgrib pandas numpy\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581adb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def grib_to_csv(grib_file_path,\n",
    "#                 city_lat,\n",
    "#                 city_lon,\n",
    "#                 output_variable='tp', # output variable 'tp' for total precipitation, 't2m' for temperature at 2m\n",
    "#                 city_name = 'london', # city name for output file 'london', 'puglia'\n",
    "#                 print_debug=False,\n",
    "#                 ):\n",
    "\n",
    "def process_grib_file(grib_file_path,\n",
    "                      city_lat=51.5074,  # Default to London latitude\n",
    "                      city_lon=-0.1278,  # Default to London longitude\n",
    "                      output_variable='tp', # Default for total precipitation \n",
    "                      city_name='london', # Default city name for london\n",
    "                      print_debug = False):  \n",
    "    \n",
    "    grib_file = grib_file_path  # Replace with your GRIB file path\n",
    "    output_csv_city = city_name  # Replace with desired output path\n",
    "\n",
    "    # Check if file exists\n",
    "    if not Path(grib_file).exists():\n",
    "        print(f\"GRIB file not found: {grib_file}\")\n",
    "        print(\"Please update the 'grib_file' variable with the correct path to your GRIB file.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        df = grib_to_csv(grib_file,\n",
    "                         city_lat=city_lat,\n",
    "                         city_lon=-city_lon, \n",
    "                         output_variable = output_variable, \n",
    "                         city_name=city_name, \n",
    "                         print_debug=print_debug)\n",
    "        \n",
    "        # Display summary statistics\n",
    "        if print_debug:\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"SUMMARY\")\n",
    "            print(\"=\"*50)\n",
    "            print(f\"Total records: {len(df)}\")\n",
    "        \n",
    "        # Show time range if available\n",
    "        time_cols = [col for col in df.columns if 'time' in col.lower()]\n",
    "        if time_cols:\n",
    "            time_col = time_cols[0]\n",
    "            if print_debug:\n",
    "                print(f\"Time range: {df[time_col].min()} to {df[time_col].max()}\")\n",
    "        \n",
    "        if print_debug:\n",
    "            print(f\"Variables extracted: {[col for col in df.columns if col not in ['latitude', 'longitude', 'actual_latitude', 'actual_longitude', 'target_latitude', 'target_longitude'] + time_cols]}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process GRIB file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce6fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_mame = 'data_london_2mt_1999_2004'\n",
    "output_variable = file_mame.split('_')[2]\n",
    "city_name = file_mame.split('_')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037e41ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_path = \"./data/ERA5/\" + output_variable + '/' + city_name + '/' + file_mame + '.grib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e3e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the file\n",
    "# Bari 41.14043890868558, 16.86619063277282\n",
    "# Lecce 40.35654102220363, 18.17311913754415\n",
    "# London 51.5074, -0.1278\n",
    "file_mame = 'data_london_t2m_2025'\n",
    "output_variable = file_mame.split('_')[2]\n",
    "city_name = file_mame.split('_')[1]\n",
    "file_path = \"./data/ERA5/\" + output_variable + '/' + city_name + '/' + file_mame + '.grib'\n",
    "\n",
    "city_lat = 51.5074  # London latitude\n",
    "city_lon = -0.1278  # London longitude\n",
    "output_variable = output_variable\n",
    "city_name = city_name\n",
    "dubug = True\n",
    "\n",
    "df_test = process_grib_file(grib_file_path = file_path,\n",
    "                            city_lat=city_lat,\n",
    "                            city_lon=city_lon,\n",
    "                            output_variable=output_variable,\n",
    "                            city_name=city_name,\n",
    "                            print_debug=dubug\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541e5398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80849968",
   "metadata": {},
   "source": [
    "## Producing daily, monthly and yearly summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d08d7ba",
   "metadata": {},
   "source": [
    "As a result of opening and reading a `grib` file, we produce one `.csv` file containing the hourly report of Total Precipitation `tp` and another with hourly 2 meters temperature readings `t2m`. \n",
    "\n",
    "In this section, we will load the `.csv` file and produce daily, monthly and yearly aggregate summary for these measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93244dd1",
   "metadata": {},
   "source": [
    "### Total Precipitation `tp`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02b8c85",
   "metadata": {},
   "source": [
    "The folder structure to retrieve the Total Precipitation file is:\n",
    "- output\n",
    "    - tp\n",
    "        - london\n",
    "        - puglia\n",
    "\n",
    "> Strategy\n",
    "\n",
    "1. Reading all csv files\n",
    "2. Concatenating dataframes\n",
    "3. Drop `NaN`s and duplicated\n",
    "4. Producing daily, monthly and yearly aggregates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5909bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "from src.data_manipulation import transform_data_datetime\n",
    "\n",
    "def create_hourly_dataset(files_base_folder = './output',\n",
    "                        weather_var = 'tp',\n",
    "                        city = 'london'):\n",
    "    folder_path = Path(files_base_folder + '/' + weather_var + '/' + city)\n",
    "\n",
    "    csv_files = list(folder_path.glob(\"*.csv\"))\n",
    "\n",
    "    # To get just the filenames:\n",
    "    csv_filenames = [file.name for file in csv_files]\n",
    "    print(csv_filenames)\n",
    "\n",
    "    # To get full paths as strings:\n",
    "    csv_paths = [str(file) for file in csv_files]\n",
    "    print(csv_paths)\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for file in csv_paths:\n",
    "        df = pd.read_csv(file)\n",
    "        df['valid_time'] = pd.to_datetime(df['valid_time'])\n",
    "        df = (df[['valid_time', weather_var]]\n",
    "            .sort_values('valid_time')\n",
    "            .reset_index(drop=True)\n",
    "            .dropna()\n",
    "            ).copy()\n",
    "        if (weather_var == 'tp'):\n",
    "            df['tp_mm'] = df['tp'] * 1000\n",
    "        print(df.shape)\n",
    "        dfs.append(df)\n",
    "\n",
    "\n",
    "    hourly_data = pd.concat(dfs, axis = 0)\n",
    "    print(hourly_data.shape)\n",
    "    # remove nans - redundant as they have already been removed in a previous step\n",
    "    hourly_data.dropna(inplace=True)\n",
    "    print(hourly_data.shape)\n",
    "    # compute mean of duplicated records - same date and hour\n",
    "    print(f\"duplicated records: {hourly_data.duplicated().sum()}\")\n",
    "    hourly_data['date_hour'] = hourly_data['valid_time'].dt.floor('h')\n",
    "    \n",
    "    if weather_var == 'tp':\n",
    "        df_24h = (hourly_data\n",
    "                .groupby(\"date_hour\")[['tp', 'tp_mm']]\n",
    "                .mean()\n",
    "                .reset_index()\n",
    "                )\n",
    "    else:\n",
    "        df_24h = (hourly_data\n",
    "                .groupby(\"date_hour\")[weather_var]\n",
    "                .mean()\n",
    "                .reset_index()\n",
    "                )\n",
    "    \n",
    "    df_24h.rename(columns = {'date_hour' : 'valid_time'}, inplace = True)\n",
    "    print(f\"duplicated records after mean: {df_24h.duplicated().sum()}\")\n",
    "    print(f\"dataframe new shape: {df_24h.shape}\")\n",
    "\n",
    "    time_delta = (df_24h['valid_time'].max() - df_24h['valid_time'].min())\n",
    "    n_expected_records = time_delta.components.days *24 +  time_delta.components.hours + 1\n",
    "    print(f\"number of expected records: {n_expected_records}\")\n",
    "\n",
    "    df_24h['hour_of_day'] = df_24h['valid_time'].dt.hour\n",
    "    df_hourly = transform_data_datetime(df_24h, date_column='valid_time')\n",
    "    \n",
    "    \n",
    "    filename = city + \"_\" + weather_var + '_' + \"hourly_data\"\n",
    "\n",
    "    if weather_var == 'tp':\n",
    "        features_list = ['valid_time', 'year', 'month', 'day', 'tp', 'tp_mm', 'hour_of_day', 'month int']\n",
    "        df_hourly[features_list].to_pickle(str(folder_path) + '/' + filename + \".pkl\")\n",
    "    else:\n",
    "        features_list = ['valid_time', 'year', 'month', 'day', weather_var, 'hour_of_day', 'month int']\n",
    "        df_hourly[features_list].to_pickle(str(folder_path) + '/' + filename + \".pkl\")\n",
    "    return(df_hourly)\n",
    "    \n",
    "\n",
    "london_tp_hourly = create_hourly_dataset(files_base_folder='./output',\n",
    "                                       weather_var='tp',\n",
    "                                       city = 'london')\n",
    "\n",
    "london_tp_hourly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360b0f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_tp_hourly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "576c884a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valid_time</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>tp</th>\n",
       "      <th>tp_mm</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>month int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1999-01-01 00:00:00</td>\n",
       "      <td>1999</td>\n",
       "      <td>Jan</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999-01-01 01:00:00</td>\n",
       "      <td>1999</td>\n",
       "      <td>Jan</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1999-01-01 02:00:00</td>\n",
       "      <td>1999</td>\n",
       "      <td>Jan</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1999-01-01 03:00:00</td>\n",
       "      <td>1999</td>\n",
       "      <td>Jan</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1999-01-01 04:00:00</td>\n",
       "      <td>1999</td>\n",
       "      <td>Jan</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           valid_time  year month  day   tp  tp_mm  hour_of_day  month int\n",
       "0 1999-01-01 00:00:00  1999   Jan    1  0.0    0.0            0          1\n",
       "1 1999-01-01 01:00:00  1999   Jan    1  0.0    0.0            1          1\n",
       "2 1999-01-01 02:00:00  1999   Jan    1  0.0    0.0            2          1\n",
       "3 1999-01-01 03:00:00  1999   Jan    1  0.0    0.0            3          1\n",
       "4 1999-01-01 04:00:00  1999   Jan    1  0.0    0.0            4          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "london_tp_hourly = pd.read_pickle('./output/tp/london/london_tp_hourly_data.pkl')\n",
    "london_tp_hourly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a97341",
   "metadata": {},
   "source": [
    "### Daily aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aa9605",
   "metadata": {},
   "source": [
    "#### 1. Create `date` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80951c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_tp_hourly['date'] = london_tp_hourly['valid_time'].dt.normalize()\n",
    "london_tp_hourly.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27be966",
   "metadata": {},
   "source": [
    "#### 2. Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624ed59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_tp_daily = (london_tp_hourly\n",
    "                   .groupby('date')\n",
    "                   .agg(\n",
    "                        year = ('year', 'first'),\n",
    "                        month = ('month', 'first'),\n",
    "                        day = ('day', 'first'),\n",
    "                        tp_daily_sum = ('tp', 'sum'),\n",
    "                        tp_daily_mean = ('tp', 'mean'),\n",
    "                        tp_daily_std = ('tp', 'std'),\n",
    "                        tp_daily_median = ('tp_mm', 'median'),\n",
    "                        tp_daily_min = ('tp', 'min'),\n",
    "                        tp_daily_max = ('tp', 'max'),\n",
    "                        tp_daily_mm_mean = ('tp_mm', 'mean'),\n",
    "                        tp_daily_mm_std = ('tp_mm', 'std'),\n",
    "                        tp_daily_mm_median = ('tp_mm', 'median'),\n",
    "                        tp_daily_mm_min = ('tp_mm', 'min'),\n",
    "                        tp_daily_mm_max = ('tp_mm', 'max'),\n",
    " ).reset_index()\n",
    ")\n",
    "\n",
    "london_tp_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c55a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4d29e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city = london, var = tp\n",
      "(231950, 8)\n",
      "columns = Index(['valid_time', 'year', 'month', 'day', 'tp', 'tp_mm', 'hour_of_day',\n",
      "       'month int'],\n",
      "      dtype='object')\n",
      "./output/tp/london/london_tp_daily_data.pkl\n"
     ]
    }
   ],
   "source": [
    "def create_daily_aggregate(file_folder = './output/tp/london/',\n",
    "                           file_name = 'london_tp_hourly_data.pkl'):\n",
    "    city = file_name.split('_')[0]\n",
    "    weather_var = file_name.split('_')[1]\n",
    "    print(f\"city = {city}, var = {weather_var}\")\n",
    "    df = pd.read_pickle(file_folder + file_name)\n",
    "    print(df.shape)\n",
    "    print(f\"columns = {df.columns}\")\n",
    "    if 'tp' in df.columns:\n",
    "        df['date'] = df['valid_time'].dt.normalize()\n",
    "        df_daily = (df\n",
    "                   .groupby('date')\n",
    "                   .agg(\n",
    "                        year = ('year', 'first'),\n",
    "                        month = ('month', 'first'),\n",
    "                        day = ('day', 'first'),\n",
    "                        tp_daily_sum = ('tp', 'sum'),\n",
    "                        tp_daily_mean = ('tp', 'mean'),\n",
    "                        tp_daily_std = ('tp', 'std'),\n",
    "                        tp_daily_median = ('tp_mm', 'median'),\n",
    "                        tp_daily_min = ('tp', 'min'),\n",
    "                        tp_daily_mm_sum = ('tp_mm', 'sum'),\n",
    "                        tp_daily_max = ('tp', 'max'),\n",
    "                        tp_daily_mm_mean = ('tp_mm', 'mean'),\n",
    "                        tp_daily_mm_std = ('tp_mm', 'std'),\n",
    "                        tp_daily_mm_median = ('tp_mm', 'median'),\n",
    "                        tp_daily_mm_min = ('tp_mm', 'min'),\n",
    "                        tp_daily_mm_max = ('tp_mm', 'max')\n",
    "                        ).reset_index()\n",
    "                        )\n",
    "        \n",
    "        # saving result as .pkl file\n",
    "        foutput = file_name.split('_')\n",
    "        output_fname = file_folder + foutput[0] + '_' + foutput[1] + '_' + 'daily_data.pkl'\n",
    "        print(output_fname)\n",
    "        df_daily.to_pickle(output_fname)\n",
    "        \n",
    "        return(df_daily)\n",
    "        \n",
    "        \n",
    "\n",
    "test = create_daily_aggregate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07fcaf54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./output/tp/london/london_tp_daily_data.pkl'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffolder = './output/tp/london/'\n",
    "fname = 'london_tp_hourly_data.pkl'\n",
    "foutput = fname.split('_')\n",
    "output_fname = ffolder + foutput[0] + '_' + foutput[1] + '_' + 'daily_data.pkl'\n",
    "output_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "506e6d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>tp_daily_sum</th>\n",
       "      <th>tp_daily_mean</th>\n",
       "      <th>tp_daily_std</th>\n",
       "      <th>tp_daily_median</th>\n",
       "      <th>tp_daily_min</th>\n",
       "      <th>tp_daily_mm_sum</th>\n",
       "      <th>tp_daily_max</th>\n",
       "      <th>tp_daily_mm_mean</th>\n",
       "      <th>tp_daily_mm_std</th>\n",
       "      <th>tp_daily_mm_median</th>\n",
       "      <th>tp_daily_mm_min</th>\n",
       "      <th>tp_daily_mm_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1999-01-01</td>\n",
       "      <td>1999</td>\n",
       "      <td>Jan</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>4.788235e-06</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.114918</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.004788</td>\n",
       "      <td>0.010305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999-01-02</td>\n",
       "      <td>1999</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003636</td>\n",
       "      <td>1.514792e-04</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.006628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.635501</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>0.151479</td>\n",
       "      <td>0.344832</td>\n",
       "      <td>0.006628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.358417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1999-01-03</td>\n",
       "      <td>1999</td>\n",
       "      <td>Jan</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007048</td>\n",
       "      <td>2.936874e-04</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.011015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.048499</td>\n",
       "      <td>0.001751</td>\n",
       "      <td>0.293687</td>\n",
       "      <td>0.489040</td>\n",
       "      <td>0.011015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.751140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1999-01-04</td>\n",
       "      <td>1999</td>\n",
       "      <td>Jan</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>2.638104e-05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.008392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.633145</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.026381</td>\n",
       "      <td>0.043223</td>\n",
       "      <td>0.008392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1999-01-05</td>\n",
       "      <td>1999</td>\n",
       "      <td>Jan</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>9.179096e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022030</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.001569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  year month  day  tp_daily_sum  tp_daily_mean  tp_daily_std  \\\n",
       "0 1999-01-01  1999   Jan    1      0.000115   4.788235e-06      0.000010   \n",
       "1 1999-01-02  1999   Jan    2      0.003636   1.514792e-04      0.000345   \n",
       "2 1999-01-03  1999   Jan    3      0.007048   2.936874e-04      0.000489   \n",
       "3 1999-01-04  1999   Jan    4      0.000633   2.638104e-05      0.000043   \n",
       "4 1999-01-05  1999   Jan    5      0.000022   9.179096e-07      0.000002   \n",
       "\n",
       "   tp_daily_median  tp_daily_min  tp_daily_mm_sum  tp_daily_max  \\\n",
       "0         0.000000           0.0         0.114918      0.000034   \n",
       "1         0.006628           0.0         3.635501      0.001358   \n",
       "2         0.011015           0.0         7.048499      0.001751   \n",
       "3         0.008392           0.0         0.633145      0.000185   \n",
       "4         0.000000           0.0         0.022030      0.000006   \n",
       "\n",
       "   tp_daily_mm_mean  tp_daily_mm_std  tp_daily_mm_median  tp_daily_mm_min  \\\n",
       "0          0.004788         0.010305            0.000000              0.0   \n",
       "1          0.151479         0.344832            0.006628              0.0   \n",
       "2          0.293687         0.489040            0.011015              0.0   \n",
       "3          0.026381         0.043223            0.008392              0.0   \n",
       "4          0.000918         0.001569            0.000000              0.0   \n",
       "\n",
       "   tp_daily_mm_max  \n",
       "0         0.034237  \n",
       "1         1.358417  \n",
       "2         1.751140  \n",
       "3         0.185108  \n",
       "4         0.006008  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_2 = pd.read_pickle(output_fname)\n",
    "test_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "625d11a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'year', 'month', 'day', 'tp_daily_sum', 'tp_daily_mean',\n",
       "       'tp_daily_std', 'tp_daily_median', 'tp_daily_min', 'tp_daily_mm_sum',\n",
       "       'tp_daily_max', 'tp_daily_mm_mean', 'tp_daily_mm_std',\n",
       "       'tp_daily_mm_median', 'tp_daily_mm_min', 'tp_daily_mm_max'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46ad606c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_size = (15,4)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=fig_size)\n",
    "\n",
    "year = 2019\n",
    "month = 'Apr'\n",
    "feature = 'tp_daily_mm_sum'\n",
    "\n",
    "df_test_y_m = test_2[(test_2['year'] == year) & (test_2['month'] == month)]\n",
    "\n",
    "ax = sns.barplot(\n",
    "    x='day',\n",
    "    y=feature,\n",
    "    hue='year',\n",
    "    data=df_test_y_m,\n",
    "    edgecolor='black',\n",
    "    errorbar=None,\n",
    "    legend=False,\n",
    "    );\n",
    "\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1a404b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3919060d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f82f9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05313962",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[test['tp_mm'] > 13.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a905ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[(test['year'] == 2013) & (test['month int'] == 7)]['tp_mm'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9642d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_delta = (london_tp_hourly['valid_time'].max() - london_tp_hourly['valid_time'].min())\n",
    "time_delta.components.days *24 +  time_delta.components.hours + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172c5936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_manipulation import transform_data_datetime\n",
    "\n",
    "london_tp_hourly['hour_of_day'] = london_tp_hourly['valid_time'].dt.hour\n",
    "london_tp_hourly_test = transform_data_datetime(london_tp_hourly, date_column='valid_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c85fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_tp_hourly_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ec485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_tp_daily_test = (london_tp_hourly_test\n",
    "                        .groupby('valid_time')\n",
    "                        .agg(\n",
    "                            tp_sum = ('tp', 'sum'),\n",
    "                            tp_min = ('tp', 'min'),\n",
    "                            tp_max = ('tp', 'max'),\n",
    "                            tp_mm_sum = ('tp_mm', 'sum'),\n",
    "                            tp_mm_min = ('tp_mm', 'min'),\n",
    "                            tp_mm_max = ('tp_mm', 'max'),\n",
    "                            year = ('year', 'first'),\n",
    "                            month_int = ('month int', 'first'),\n",
    "                            month = ('month', 'first'),\n",
    "                            day = ('day', 'first')\n",
    "                       )\n",
    "                       .reset_index()\n",
    "                       )\n",
    "\n",
    "london_tp_daily_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ca5dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_tp_hourly['valid_time'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2cb9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_tp_hourly['valid_date'] = london_tp_hourly['valid_time'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad18dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_tp_daily = (london_tp_hourly\n",
    "                   .groupby('valid_date')[['tp', 'tp_mm']]\n",
    "                   .agg(\n",
    "                       tp_sum = ('tp', 'sum'),\n",
    "                       tp_min = ('tp', 'min'),\n",
    "                       tp_max = ('tp', 'max'),\n",
    "                       tp_mm_sum = ('tp_mm', 'sum'),\n",
    "                       tp_mm_min = ('tp_mm', 'min'),\n",
    "                       tp_mm_max = ('tp_mm', 'max')\n",
    "                       )\n",
    "                       .reset_index()\n",
    "                       )\n",
    "\n",
    "london_tp_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a9d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_tp_daily['year'] = london_tp_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4859634",
   "metadata": {},
   "outputs": [],
   "source": [
    "london_tp_daily[london_tp_daily['tp_max'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f85c1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c992ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631d97dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089151ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81964e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e1e214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5551a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df96e13e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f81987",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './data/ERA5/data_puglia_tp_2025.grib'\n",
    "output_path = './output/'\n",
    "city_name = 'bari'\n",
    "city_lat = 41.14043890868558  # Bari latitude\n",
    "city_lon = 16.86619063277282  # Bari longitude\n",
    "\n",
    "df_test_bari = process_grib_file(grib_file_path = file_path,\n",
    "                            output_csv_path=output_path,\n",
    "                            city_name=city_name,\n",
    "                            city_lat=city_lat,\n",
    "                            city_lon=city_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176228a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecce 40.35654102220363, 18.17311913754415\n",
    "\n",
    "file_path = './data/ERA5/data_puglia_tp_2025.grib'\n",
    "output_path = './output/'\n",
    "city_name = 'lecce'\n",
    "city_lat = 40.35654102220363  # Lecce latitude\n",
    "city_lon = 18.17311913754415  # Lecce longitude\n",
    "\n",
    "df_test_lecce = process_grib_file(grib_file_path = file_path,\n",
    "                                  output_csv_path=output_path,\n",
    "                                  city_name=city_name,\n",
    "                                  city_lat=city_lat,\n",
    "                                  city_lon=city_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aed3f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb5becd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5e1ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working for London coordinates\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def grib_to_csv_london(grib_file_path, output_csv_path=None):\n",
    "    \"\"\"\n",
    "    Read GRIB file, extract data for London coordinates, and save as CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    grib_file_path (str): Path to the GRIB file\n",
    "    output_csv_path (str): Path for output CSV file (optional)\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with the extracted data\n",
    "    \"\"\"\n",
    "    \n",
    "    # London coordinates\n",
    "    london_lat = 51.5074\n",
    "    london_lon = -0.1278  # Note: GRIB files often use 0-360 longitude\n",
    "    london_lon_360 = 359.8722  # London longitude in 0-360 format\n",
    "    \n",
    "    print(f\"Reading GRIB file: {grib_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Open GRIB file with xarray\n",
    "        ds = xr.open_dataset(grib_file_path, engine='cfgrib')\n",
    "        \n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        print(f\"Dataset dimensions: {dict(ds.dims)}\")\n",
    "        print(f\"Dataset variables: {list(ds.data_vars)}\")\n",
    "        print(f\"Dataset coordinates: {list(ds.coords)}\")\n",
    "        \n",
    "        # Check longitude format (0-360 vs -180-180)\n",
    "        lon_min = ds.longitude.min().values\n",
    "        lon_max = ds.longitude.max().values\n",
    "        print(f\"Longitude range: {lon_min} to {lon_max}\")\n",
    "        \n",
    "        # Use appropriate London longitude based on the dataset's longitude format\n",
    "        if lon_max > 180:\n",
    "            target_lon = london_lon_360\n",
    "            print(f\"Using 0-360 longitude format: {target_lon}\")\n",
    "        else:\n",
    "            target_lon = london_lon\n",
    "            print(f\"Using -180-180 longitude format: {target_lon}\")\n",
    "        \n",
    "        # Find nearest grid point to London\n",
    "        lat_idx = np.abs(ds.latitude - london_lat).argmin()\n",
    "        lon_idx = np.abs(ds.longitude - target_lon).argmin()\n",
    "        \n",
    "        actual_lat = ds.latitude[lat_idx].values\n",
    "        actual_lon = ds.longitude[lon_idx].values\n",
    "        \n",
    "        print(f\"Target coordinates: ({london_lat}, {target_lon})\")\n",
    "        print(f\"Nearest grid point: ({actual_lat}, {actual_lon})\")\n",
    "        \n",
    "        # Extract data for London coordinates\n",
    "        london_data = ds.isel(latitude=lat_idx, longitude=lon_idx)\n",
    "        \n",
    "        # Convert to pandas DataFrame\n",
    "        df_list = []\n",
    "        \n",
    "        for var_name in london_data.data_vars:\n",
    "            var_data = london_data[var_name]\n",
    "            \n",
    "            # Handle different time dimensions\n",
    "            if 'time' in var_data.dims:\n",
    "                # Create DataFrame with time index\n",
    "                df_var = var_data.to_dataframe().reset_index()\n",
    "                df_var = df_var.rename(columns={var_name: var_name})\n",
    "                df_list.append(df_var)\n",
    "            elif 'valid_time' in var_data.dims:\n",
    "                # Handle valid_time dimension\n",
    "                df_var = var_data.to_dataframe().reset_index()\n",
    "                df_var = df_var.rename(columns={var_name: var_name})\n",
    "                df_list.append(df_var)\n",
    "            else:\n",
    "                # Single value variables\n",
    "                df_var = pd.DataFrame({\n",
    "                    var_name: [var_data.values],\n",
    "                    'latitude': [actual_lat],\n",
    "                    'longitude': [actual_lon]\n",
    "                })\n",
    "                df_list.append(df_var)\n",
    "        \n",
    "        # Combine all variables into one DataFrame\n",
    "        if len(df_list) > 1:\n",
    "            # Merge on common columns (time, latitude, longitude)\n",
    "            df = df_list[0]\n",
    "            for df_var in df_list[1:]:\n",
    "                common_cols = set(df.columns) & set(df_var.columns)\n",
    "                if common_cols:\n",
    "                    df = pd.merge(df, df_var, on=list(common_cols), how='outer')\n",
    "                else:\n",
    "                    # If no common columns, concatenate side by side\n",
    "                    df = pd.concat([df, df_var], axis=1)\n",
    "        else:\n",
    "            df = df_list[0]\n",
    "        \n",
    "        # Add metadata columns\n",
    "        df['actual_latitude'] = actual_lat\n",
    "        df['actual_longitude'] = actual_lon\n",
    "        df['target_latitude'] = london_lat\n",
    "        df['target_longitude'] = target_lon\n",
    "        \n",
    "        # Sort by time if time column exists\n",
    "        time_cols = [col for col in df.columns if 'time' in col.lower()]\n",
    "        if time_cols:\n",
    "            df = df.sort_values(time_cols[0])\n",
    "        \n",
    "        print(f\"DataFrame shape: {df.shape}\")\n",
    "        print(f\"DataFrame columns: {list(df.columns)}\")\n",
    "        print(\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Save to CSV\n",
    "        if output_csv_path is None:\n",
    "            output_csv_path = Path(grib_file_path).stem + \"_london_data.csv\"\n",
    "        \n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData saved to: {output_csv_path}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing GRIB file: {e}\")\n",
    "        print(\"Make sure you have the required packages installed:\")\n",
    "        print(\"pip install xarray cfgrib pandas numpy\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "    grib_file = \"./data/ERA5/ERA5_hourly_data_on_single_levels_from_2014_to_2021.grib\"  # Replace with your GRIB file path\n",
    "    output_csv = \"./output/ERA5_hourly_data_on_single_levels_from_2014_to_2021.csv\"  # Replace with desired output path\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not Path(grib_file).exists():\n",
    "        print(f\"GRIB file not found: {grib_file}\")\n",
    "        print(\"Please update the 'grib_file' variable with the correct path to your GRIB file.\")\n",
    "        return\n",
    "    \n",
    "    # Process the file\n",
    "    try:\n",
    "        df = grib_to_csv_london(grib_file, output_csv)\n",
    "        \n",
    "        # Display summary statistics\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Total records: {len(df)}\")\n",
    "        \n",
    "        # Show time range if available\n",
    "        time_cols = [col for col in df.columns if 'time' in col.lower()]\n",
    "        if time_cols:\n",
    "            time_col = time_cols[0]\n",
    "            print(f\"Time range: {df[time_col].min()} to {df[time_col].max()}\")\n",
    "        \n",
    "        print(f\"Variables extracted: {[col for col in df.columns if col not in ['latitude', 'longitude', 'actual_latitude', 'actual_longitude', 'target_latitude', 'target_longitude'] + time_cols]}\")\n",
    "        \n",
    "        # Load the CSV back into pandas to verify\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"VERIFICATION - Loading CSV back into pandas:\")\n",
    "        print(\"=\"*50)\n",
    "        df_loaded = pd.read_csv(output_csv)\n",
    "        print(f\"Loaded DataFrame shape: {df_loaded.shape}\")\n",
    "        print(\"DataFrame info:\")\n",
    "        print(df_loaded.info())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process GRIB file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cece2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(\"./output/ERA5_hourly_data_on_single_levels_from_2014_to_2021.csv\")\n",
    "print(df_data.shape)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f5bc01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8217b669",
   "metadata": {},
   "outputs": [],
   "source": [
    "140-80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd85655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2817333",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(\"./output/ERA5_hourly_data_on_single_levels_from_1940_to_present.csv\")\n",
    "df_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731bd700",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde7b429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbada62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['year'] = pd.to_datetime(df_data['time'], errors='coerce').dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cded2ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_data\n",
    " .groupby('year', observed=True)['tp']\n",
    " .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625f3ac5",
   "metadata": {},
   "source": [
    "### NC files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b7dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V4 \n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def netcdf_to_csv_london(nc_file_path, output_csv_path=None):\n",
    "    \"\"\"\n",
    "    Read NetCDF file, extract precipitation data for London coordinates, and save as CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    nc_file_path (str): Path to the NetCDF file\n",
    "    output_csv_path (str): Path for output CSV file (optional)\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with the extracted data\n",
    "    \"\"\"\n",
    "    \n",
    "    # London coordinates\n",
    "    london_lat = 51.5074\n",
    "    london_lon = -0.1278\n",
    "    \n",
    "    print(f\"Reading NetCDF file: {nc_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Open NetCDF file with xarray\n",
    "        ds = xr.open_dataset(nc_file_path)\n",
    "        \n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        print(f\"Dataset dimensions: {dict(ds.dims)}\")\n",
    "        print(f\"Dataset variables: {list(ds.data_vars)}\")\n",
    "        print(f\"Dataset coordinates: {list(ds.coords)}\")\n",
    "        \n",
    "        # Display dataset info\n",
    "        print(\"\\nDataset overview:\")\n",
    "        print(ds)\n",
    "        \n",
    "        # Check coordinate names (common variations)\n",
    "        lat_names = ['lat', 'latitude', 'y', 'projection_y_coordinate']\n",
    "        lon_names = ['lon', 'longitude', 'x', 'projection_x_coordinate']\n",
    "        time_names = ['time', 'time_bnds', 'yyyymm']\n",
    "        \n",
    "        # Find actual coordinate names in the dataset\n",
    "        lat_coord = None\n",
    "        lon_coord = None\n",
    "        time_coord = None\n",
    "        \n",
    "        for name in lat_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                lat_coord = name\n",
    "                break\n",
    "                \n",
    "        for name in lon_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                lon_coord = name\n",
    "                break\n",
    "                \n",
    "        for name in time_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                time_coord = name\n",
    "                break\n",
    "        \n",
    "        print(f\"\\nIdentified coordinates:\")\n",
    "        print(f\"Latitude coordinate: {lat_coord}\")\n",
    "        print(f\"Longitude coordinate: {lon_coord}\")\n",
    "        print(f\"Time coordinate: {time_coord}\")\n",
    "        \n",
    "        if lat_coord is None or lon_coord is None:\n",
    "            print(\"Warning: Could not identify latitude/longitude coordinates\")\n",
    "            print(\"Available coordinates:\", list(ds.coords))\n",
    "            return None\n",
    "        \n",
    "        # Check coordinate ranges\n",
    "        if lat_coord in ds.coords:\n",
    "            lat_min = ds.coords[lat_coord].min().values\n",
    "            lat_max = ds.coords[lat_coord].max().values\n",
    "            print(f\"Latitude range: {lat_min} to {lat_max}\")\n",
    "        \n",
    "        if lon_coord in ds.coords:\n",
    "            lon_min = ds.coords[lon_coord].min().values\n",
    "            lon_max = ds.coords[lon_coord].max().values\n",
    "            print(f\"Longitude range: {lon_min} to {lon_max}\")\n",
    "        \n",
    "        # Handle projected coordinates (if using OSGB or similar)\n",
    "        if 'projection' in str(ds.coords) or lat_coord in ['y', 'projection_y_coordinate']:\n",
    "            print(\"\\nDetected projected coordinates (likely OSGB)\")\n",
    "            print(\"You may need to convert London lat/lon to the projection coordinates\")\n",
    "            print(\"For now, finding nearest grid point in projected space...\")\n",
    "        \n",
    "        # Define region around London (in degrees)\n",
    "        region_size = 0.5  # degrees (approximately 50km at London's latitude)\n",
    "        \n",
    "        # Define bounding box around London\n",
    "        lat_min = london_lat - region_size\n",
    "        lat_max = london_lat + region_size\n",
    "        lon_min = london_lon - region_size\n",
    "        lon_max = london_lon + region_size\n",
    "        \n",
    "        print(f\"\\nExtracting region around London:\")\n",
    "        print(f\"Latitude range: {lat_min:.3f} to {lat_max:.3f}\")\n",
    "        print(f\"Longitude range: {lon_min:.3f} to {lon_max:.3f}\")\n",
    "        \n",
    "        # Find grid indices for the region\n",
    "        if lat_coord in ds.coords and lon_coord in ds.coords:\n",
    "            # Handle 2D coordinate arrays (common in projected datasets)\n",
    "            lat_data = ds.coords[lat_coord]\n",
    "            lon_data = ds.coords[lon_coord]\n",
    "            \n",
    "            if lat_data.ndim == 2 and lon_data.ndim == 2:\n",
    "                # 2D coordinate arrays - find region\n",
    "                print(\"Using 2D coordinate arrays to find region\")\n",
    "                \n",
    "                # Find points within the bounding box\n",
    "                within_region = ((lat_data >= lat_min) & (lat_data <= lat_max) & \n",
    "                               (lon_data >= lon_min) & (lon_data <= lon_max))\n",
    "                \n",
    "                # Find the bounding indices of the region\n",
    "                y_indices, x_indices = np.where(within_region)\n",
    "                \n",
    "                if len(y_indices) == 0:\n",
    "                    print(\"No grid points found in the specified region!\")\n",
    "                    return None\n",
    "                \n",
    "                y_min_idx = y_indices.min()\n",
    "                y_max_idx = y_indices.max()\n",
    "                x_min_idx = x_indices.min()\n",
    "                x_max_idx = x_indices.max()\n",
    "                \n",
    "                print(f\"Found {len(y_indices)} grid points in region\")\n",
    "                print(f\"Grid index ranges: y={y_min_idx}-{y_max_idx}, x={x_min_idx}-{x_max_idx}\")\n",
    "                \n",
    "                # Store the slice ranges for extraction\n",
    "                y_slice = slice(y_min_idx, y_max_idx + 1)\n",
    "                x_slice = slice(x_min_idx, x_max_idx + 1)\n",
    "                \n",
    "                # Get actual coordinate bounds of extracted region\n",
    "                region_lat_min = lat_data[y_min_idx:y_max_idx+1, x_min_idx:x_max_idx+1].min().values\n",
    "                region_lat_max = lat_data[y_min_idx:y_max_idx+1, x_min_idx:x_max_idx+1].max().values\n",
    "                region_lon_min = lon_data[y_min_idx:y_max_idx+1, x_min_idx:x_max_idx+1].min().values\n",
    "                region_lon_max = lon_data[y_min_idx:y_max_idx+1, x_min_idx:x_max_idx+1].max().values\n",
    "                \n",
    "                print(f\"Actual extracted region:\")\n",
    "                print(f\"  Latitude: {region_lat_min:.3f} to {region_lat_max:.3f}\")\n",
    "                print(f\"  Longitude: {region_lon_min:.3f} to {region_lon_max:.3f}\")\n",
    "                \n",
    "            else:\n",
    "                # 1D coordinate arrays\n",
    "                lat_mask = (lat_data >= lat_min) & (lat_data <= lat_max)\n",
    "                lon_mask = (lon_data >= lon_min) & (lon_data <= lon_max)\n",
    "                \n",
    "                y_indices = np.where(lat_mask)[0]\n",
    "                x_indices = np.where(lon_mask)[0]\n",
    "                \n",
    "                if len(y_indices) == 0 or len(x_indices) == 0:\n",
    "                    print(\"No grid points found in the specified region!\")\n",
    "                    return None\n",
    "                \n",
    "                y_slice = slice(y_indices.min(), y_indices.max() + 1)\n",
    "                x_slice = slice(x_indices.min(), x_indices.max() + 1)\n",
    "                \n",
    "                print(f\"Grid slices: y={y_slice}, x={x_slice}\")\n",
    "        else:\n",
    "            print(\"Could not find coordinate values for region extraction\")\n",
    "            return None\n",
    "        \n",
    "        # Extract data for London region\n",
    "        # Use the projection coordinate names for selection\n",
    "        proj_y_coord = 'projection_y_coordinate'\n",
    "        proj_x_coord = 'projection_x_coordinate'\n",
    "        \n",
    "        selection_dict = {proj_y_coord: y_slice, proj_x_coord: x_slice}\n",
    "        london_region_data = ds.isel(selection_dict)\n",
    "        \n",
    "        print(f\"\\nExtracted region data shape: {dict(london_region_data.sizes)}\")\n",
    "        \n",
    "        print(f\"\\nExtracted data variables: {list(london_region_data.data_vars)}\")\n",
    "        \n",
    "        # Convert to pandas DataFrame - simplified approach\n",
    "        print(f\"\\nExtracting data for London...\")\n",
    "        \n",
    "        # Create a simple DataFrame with the main data\n",
    "        data_dict = {}\n",
    "        \n",
    "        # Extract main variables (skip coordinate and bounds variables)\n",
    "        main_vars = [var for var in london_region_data.data_vars \n",
    "                    if not any(skip in var for skip in ['_bnds', 'transverse_mercator', 'bounds'])]\n",
    "        \n",
    "        print(f\"Main data variables to extract: {main_vars}\")\n",
    "        \n",
    "        for var_name in main_vars:\n",
    "            var_data = london_region_data[var_name]\n",
    "            print(f\"\\nProcessing variable: {var_name}\")\n",
    "            print(f\"Variable dimensions: {var_data.dims}\")\n",
    "            print(f\"Variable shape: {var_data.shape}\")\n",
    "            \n",
    "            # Get variable attributes\n",
    "            if hasattr(var_data, 'attrs'):\n",
    "                print(f\"Variable attributes: {var_data.attrs}\")\n",
    "            \n",
    "            # Extract the actual values\n",
    "            if var_data.size == 1:\n",
    "                # Single value\n",
    "                data_dict[var_name] = [float(var_data.values)]\n",
    "            elif time_coord and time_coord in var_data.dims:\n",
    "                # Time series data\n",
    "                values = var_data.values\n",
    "                if values.ndim > 1:\n",
    "                    values = values.flatten()\n",
    "                data_dict[var_name] = values.tolist()\n",
    "            else:\n",
    "                # Other cases\n",
    "                values = var_data.values\n",
    "                if hasattr(values, 'flatten'):\n",
    "                    values = values.flatten()\n",
    "                data_dict[var_name] = [float(values)] if np.isscalar(values) else values.tolist()\n",
    "            \n",
    "            # Add metadata\n",
    "            if hasattr(var_data, 'attrs'):\n",
    "                units = var_data.attrs.get('units', 'unknown')\n",
    "                long_name = var_data.attrs.get('long_name', var_name)\n",
    "                data_dict[f'{var_name}_units'] = [units] * len(data_dict[var_name])\n",
    "                data_dict[f'{var_name}_long_name'] = [long_name] * len(data_dict[var_name])\n",
    "        \n",
    "        # Add time information if available\n",
    "        if time_coord and time_coord in london_region_data.coords:\n",
    "            time_values = london_region_data.coords[time_coord].values\n",
    "            if hasattr(time_values, 'flatten'):\n",
    "                time_values = time_values.flatten()\n",
    "            data_dict[time_coord] = time_values.tolist() if hasattr(time_values, 'tolist') else [time_values]\n",
    "        \n",
    "        # Create DataFrame\n",
    "        # First, make sure all lists have the same length\n",
    "        max_length = max(len(v) if isinstance(v, list) else 1 for v in data_dict.values())\n",
    "        \n",
    "        for key, value in data_dict.items():\n",
    "            if isinstance(value, list) and len(value) == 1 and max_length > 1:\n",
    "                data_dict[key] = value * max_length\n",
    "        \n",
    "        df = pd.DataFrame(data_dict)\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"Warning: No data extracted\")\n",
    "            return None\n",
    "        \n",
    "        # Add location metadata\n",
    "        df['target_latitude'] = london_lat\n",
    "        df['target_longitude'] = london_lon\n",
    "        df['coordinate_system'] = f\"{lat_coord}, {lon_coord}\"\n",
    "        \n",
    "        # Sort by time if time column exists\n",
    "        time_cols = [col for col in df.columns if any(t in col.lower() for t in ['time', 'date', 'year'])]\n",
    "        if time_cols:\n",
    "            df = df.sort_values(time_cols[0])\n",
    "            print(f\"Sorted by time column: {time_cols[0]}\")\n",
    "        \n",
    "        print(f\"\\nFinal DataFrame shape: {df.shape}\")\n",
    "        print(f\"DataFrame columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Show data types\n",
    "        print(f\"\\nData types:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"  {col}: {df[col].dtype}\")\n",
    "        \n",
    "        print(f\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        if len(df) > 5:\n",
    "            print(f\"\\nLast few rows:\")\n",
    "            print(df.tail())\n",
    "        \n",
    "        # Save to CSV\n",
    "        if output_csv_path is None:\n",
    "            output_csv_path = Path(nc_file_path).stem + \"_london_region_precipitation.csv\"\n",
    "        \n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData saved to: {output_csv_path}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing NetCDF file: {e}\")\n",
    "        print(\"Make sure you have the required packages installed:\")\n",
    "        print(\"pip install xarray netcdf4 pandas numpy\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # Your specific file\n",
    "    nc_file = \"./data/ERA5/rainfall_hadukgrid_uk_1km_ann-30y_196101-199012.nc\"\n",
    "    output_csv = \"./output/london_region_rainfall_1961_1990_V4.csv\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not Path(nc_file).exists():\n",
    "        print(f\"NetCDF file not found: {nc_file}\")\n",
    "        print(\"Please make sure the file is in the current directory or update the path.\")\n",
    "        return\n",
    "    \n",
    "    # Process the file\n",
    "    try:\n",
    "        df = netcdf_to_csv_london(nc_file, output_csv)\n",
    "        \n",
    "        if df is not None:\n",
    "            # Display summary statistics\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"SUMMARY\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"Total grid points extracted: {len(df)}\")\n",
    "            \n",
    "            # Show spatial extent\n",
    "            if 'latitude' in df.columns and 'longitude' in df.columns:\n",
    "                print(f\"Latitude range: {df['latitude'].min():.3f} to {df['latitude'].max():.3f}\")\n",
    "                print(f\"Longitude range: {df['longitude'].min():.3f} to {df['longitude'].max():.3f}\")\n",
    "            \n",
    "            # Show time range if available\n",
    "            time_cols = [col for col in df.columns if any(t in col.lower() for t in ['time', 'date', 'year'])]\n",
    "            if time_cols:\n",
    "                time_col = time_cols[0]\n",
    "                print(f\"Time range: {df[time_col].min()} to {df[time_col].max()}\")\n",
    "            \n",
    "            # Show precipitation statistics\n",
    "            precip_cols = [col for col in df.columns if any(p in col.lower() for p in ['rain', 'precip', 'precipitation'])]\n",
    "            if precip_cols:\n",
    "                print(f\"\\nRegional precipitation statistics for {precip_cols[0]}:\")\n",
    "                print(df[precip_cols[0]].describe())\n",
    "                print(f\"Grid points with data: {df[precip_cols[0]].notna().sum()}/{len(df)}\")\n",
    "            \n",
    "            # Load the CSV back into pandas to verify\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"VERIFICATION - Loading CSV back into pandas:\")\n",
    "            print(\"=\"*60)\n",
    "            df_loaded = pd.read_csv(output_csv)\n",
    "            print(f\"Loaded DataFrame shape: {df_loaded.shape}\")\n",
    "            print(\"Sample of loaded data:\")\n",
    "            print(df_loaded.head(3))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process NetCDF file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e38b32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c110fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c425bce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V3\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def netcdf_to_csv_london(nc_file_path, output_csv_path=None):\n",
    "    \"\"\"\n",
    "    Read NetCDF file, extract precipitation data for London coordinates, and save as CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    nc_file_path (str): Path to the NetCDF file\n",
    "    output_csv_path (str): Path for output CSV file (optional)\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with the extracted data\n",
    "    \"\"\"\n",
    "    \n",
    "    # London coordinates\n",
    "    london_lat = 51.5074\n",
    "    london_lon = -0.1278\n",
    "    \n",
    "    print(f\"Reading NetCDF file: {nc_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Open NetCDF file with xarray\n",
    "        ds = xr.open_dataset(nc_file_path)\n",
    "        \n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        print(f\"Dataset dimensions: {dict(ds.dims)}\")\n",
    "        print(f\"Dataset variables: {list(ds.data_vars)}\")\n",
    "        print(f\"Dataset coordinates: {list(ds.coords)}\")\n",
    "        \n",
    "        # Display dataset info\n",
    "        print(\"\\nDataset overview:\")\n",
    "        print(ds)\n",
    "        \n",
    "        # Check coordinate names (common variations)\n",
    "        lat_names = ['lat', 'latitude', 'y', 'projection_y_coordinate']\n",
    "        lon_names = ['lon', 'longitude', 'x', 'projection_x_coordinate']\n",
    "        time_names = ['time', 'time_bnds', 'yyyymm']\n",
    "        \n",
    "        # Find actual coordinate names in the dataset\n",
    "        lat_coord = None\n",
    "        lon_coord = None\n",
    "        time_coord = None\n",
    "        \n",
    "        for name in lat_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                lat_coord = name\n",
    "                break\n",
    "                \n",
    "        for name in lon_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                lon_coord = name\n",
    "                break\n",
    "                \n",
    "        for name in time_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                time_coord = name\n",
    "                break\n",
    "        \n",
    "        print(f\"\\nIdentified coordinates:\")\n",
    "        print(f\"Latitude coordinate: {lat_coord}\")\n",
    "        print(f\"Longitude coordinate: {lon_coord}\")\n",
    "        print(f\"Time coordinate: {time_coord}\")\n",
    "        \n",
    "        if lat_coord is None or lon_coord is None:\n",
    "            print(\"Warning: Could not identify latitude/longitude coordinates\")\n",
    "            print(\"Available coordinates:\", list(ds.coords))\n",
    "            return None\n",
    "        \n",
    "        # Check coordinate ranges\n",
    "        if lat_coord in ds.coords:\n",
    "            lat_min = ds.coords[lat_coord].min().values\n",
    "            lat_max = ds.coords[lat_coord].max().values\n",
    "            print(f\"Latitude range: {lat_min} to {lat_max}\")\n",
    "        \n",
    "        if lon_coord in ds.coords:\n",
    "            lon_min = ds.coords[lon_coord].min().values\n",
    "            lon_max = ds.coords[lon_coord].max().values\n",
    "            print(f\"Longitude range: {lon_min} to {lon_max}\")\n",
    "        \n",
    "        # Handle projected coordinates (if using OSGB or similar)\n",
    "        if 'projection' in str(ds.coords) or lat_coord in ['y', 'projection_y_coordinate']:\n",
    "            print(\"\\nDetected projected coordinates (likely OSGB)\")\n",
    "            print(\"You may need to convert London lat/lon to the projection coordinates\")\n",
    "            print(\"For now, finding nearest grid point in projected space...\")\n",
    "        \n",
    "        # Find nearest grid point to London\n",
    "        if lat_coord in ds.coords and lon_coord in ds.coords:\n",
    "            # Handle 2D coordinate arrays (common in projected datasets)\n",
    "            lat_data = ds.coords[lat_coord]\n",
    "            lon_data = ds.coords[lon_coord]\n",
    "            \n",
    "            if lat_data.ndim == 2 and lon_data.ndim == 2:\n",
    "                # 2D coordinate arrays - find nearest point in 2D space\n",
    "                print(\"Using 2D coordinate arrays to find nearest point\")\n",
    "                \n",
    "                # Calculate distance from London to all grid points\n",
    "                lat_diff = lat_data - london_lat\n",
    "                lon_diff = lon_data - london_lon\n",
    "                distances = np.sqrt(lat_diff**2 + lon_diff**2)\n",
    "                \n",
    "                # Find indices of minimum distance\n",
    "                min_idx = np.unravel_index(distances.argmin(), distances.shape)\n",
    "                y_idx, x_idx = min_idx\n",
    "                \n",
    "                actual_lat = lat_data[y_idx, x_idx].values\n",
    "                actual_lon = lon_data[y_idx, x_idx].values\n",
    "                \n",
    "                print(f\"\\nTarget coordinates: ({london_lat}, {london_lon})\")\n",
    "                print(f\"Nearest grid point: ({actual_lat}, {actual_lon})\")\n",
    "                print(f\"Grid indices: y={y_idx}, x={x_idx}\")\n",
    "                \n",
    "                # Use the projection coordinate indices for selection\n",
    "                lat_idx = y_idx  # This corresponds to projection_y_coordinate index\n",
    "                lon_idx = x_idx  # This corresponds to projection_x_coordinate index\n",
    "                \n",
    "            else:\n",
    "                # 1D coordinate arrays\n",
    "                lat_idx = np.abs(lat_data - london_lat).argmin()\n",
    "                lon_idx = np.abs(lon_data - london_lon).argmin()\n",
    "                \n",
    "                actual_lat = lat_data[lat_idx].values\n",
    "                actual_lon = lon_data[lon_idx].values\n",
    "                \n",
    "                print(f\"\\nTarget coordinates: ({london_lat}, {london_lon})\")\n",
    "                print(f\"Nearest grid point: ({actual_lat}, {actual_lon})\")\n",
    "        else:\n",
    "            print(\"Could not find coordinate values for nearest point calculation\")\n",
    "            return None\n",
    "        \n",
    "        # Extract data for London coordinates\n",
    "        # Use the projection coordinate names for selection\n",
    "        proj_y_coord = 'projection_y_coordinate'\n",
    "        proj_x_coord = 'projection_x_coordinate'\n",
    "        \n",
    "        selection_dict = {proj_y_coord: lat_idx, proj_x_coord: lon_idx}\n",
    "        london_data = ds.isel(selection_dict)\n",
    "        \n",
    "        print(f\"\\nExtracted data variables: {list(london_data.data_vars)}\")\n",
    "        \n",
    "        # Convert to pandas DataFrame - simplified approach\n",
    "        print(f\"\\nExtracting data for London...\")\n",
    "        \n",
    "        # Create a simple DataFrame with the main data\n",
    "        data_dict = {}\n",
    "        \n",
    "        # Extract main variables (skip coordinate and bounds variables)\n",
    "        main_vars = [var for var in london_data.data_vars \n",
    "                    if not any(skip in var for skip in ['_bnds', 'transverse_mercator', 'bounds'])]\n",
    "        \n",
    "        print(f\"Main data variables to extract: {main_vars}\")\n",
    "        \n",
    "        for var_name in main_vars:\n",
    "            var_data = london_data[var_name]\n",
    "            print(f\"\\nProcessing variable: {var_name}\")\n",
    "            print(f\"Variable dimensions: {var_data.dims}\")\n",
    "            print(f\"Variable shape: {var_data.shape}\")\n",
    "            \n",
    "            # Get variable attributes\n",
    "            if hasattr(var_data, 'attrs'):\n",
    "                print(f\"Variable attributes: {var_data.attrs}\")\n",
    "            \n",
    "            # Extract the actual values\n",
    "            if var_data.size == 1:\n",
    "                # Single value\n",
    "                data_dict[var_name] = [float(var_data.values)]\n",
    "            elif time_coord and time_coord in var_data.dims:\n",
    "                # Time series data\n",
    "                values = var_data.values\n",
    "                if values.ndim > 1:\n",
    "                    values = values.flatten()\n",
    "                data_dict[var_name] = values.tolist()\n",
    "            else:\n",
    "                # Other cases\n",
    "                values = var_data.values\n",
    "                if hasattr(values, 'flatten'):\n",
    "                    values = values.flatten()\n",
    "                data_dict[var_name] = [float(values)] if np.isscalar(values) else values.tolist()\n",
    "            \n",
    "            # Add metadata\n",
    "            if hasattr(var_data, 'attrs'):\n",
    "                units = var_data.attrs.get('units', 'unknown')\n",
    "                long_name = var_data.attrs.get('long_name', var_name)\n",
    "                data_dict[f'{var_name}_units'] = [units] * len(data_dict[var_name])\n",
    "                data_dict[f'{var_name}_long_name'] = [long_name] * len(data_dict[var_name])\n",
    "        \n",
    "        # Add time information if available\n",
    "        if time_coord and time_coord in london_data.coords:\n",
    "            time_values = london_data.coords[time_coord].values\n",
    "            if hasattr(time_values, 'flatten'):\n",
    "                time_values = time_values.flatten()\n",
    "            data_dict[time_coord] = time_values.tolist() if hasattr(time_values, 'tolist') else [time_values]\n",
    "        \n",
    "        # Create DataFrame\n",
    "        # First, make sure all lists have the same length\n",
    "        max_length = max(len(v) if isinstance(v, list) else 1 for v in data_dict.values())\n",
    "        \n",
    "        for key, value in data_dict.items():\n",
    "            if isinstance(value, list) and len(value) == 1 and max_length > 1:\n",
    "                data_dict[key] = value * max_length\n",
    "        \n",
    "        df = pd.DataFrame(data_dict)\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"Warning: No data extracted\")\n",
    "            return None\n",
    "        \n",
    "        # Add location metadata\n",
    "        df['actual_latitude'] = actual_lat\n",
    "        df['actual_longitude'] = actual_lon\n",
    "        df['target_latitude'] = london_lat\n",
    "        df['target_longitude'] = london_lon\n",
    "        df['coordinate_system'] = f\"{lat_coord}, {lon_coord}\"\n",
    "        \n",
    "        # Sort by time if time column exists\n",
    "        time_cols = [col for col in df.columns if any(t in col.lower() for t in ['time', 'date', 'year'])]\n",
    "        if time_cols:\n",
    "            df = df.sort_values(time_cols[0])\n",
    "            print(f\"Sorted by time column: {time_cols[0]}\")\n",
    "        \n",
    "        print(f\"\\nFinal DataFrame shape: {df.shape}\")\n",
    "        print(f\"DataFrame columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Show data types\n",
    "        print(f\"\\nData types:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"  {col}: {df[col].dtype}\")\n",
    "        \n",
    "        print(f\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        if len(df) > 5:\n",
    "            print(f\"\\nLast few rows:\")\n",
    "            print(df.tail())\n",
    "        \n",
    "        # Save to CSV\n",
    "        if output_csv_path is None:\n",
    "            output_csv_path = Path(nc_file_path).stem + \"_london_precipitation.csv\"\n",
    "        \n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData saved to: {output_csv_path}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing NetCDF file: {e}\")\n",
    "        print(\"Make sure you have the required packages installed:\")\n",
    "        print(\"pip install xarray netcdf4 pandas numpy\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # Your specific file\n",
    "    nc_file = \"./data/ERA5/rainfall_hadukgrid_uk_1km_ann-30y_196101-199012.nc\"\n",
    "    output_csv = \"./output/london_rainfall_1961_1990_V3.csv\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not Path(nc_file).exists():\n",
    "        print(f\"NetCDF file not found: {nc_file}\")\n",
    "        print(\"Please make sure the file is in the current directory or update the path.\")\n",
    "        return\n",
    "    \n",
    "    # Process the file\n",
    "    try:\n",
    "        df = netcdf_to_csv_london(nc_file, output_csv)\n",
    "        \n",
    "        if df is not None:\n",
    "            # Display summary statistics\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"SUMMARY\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"Total records: {len(df)}\")\n",
    "            \n",
    "            # Show time range if available\n",
    "            time_cols = [col for col in df.columns if any(t in col.lower() for t in ['time', 'date', 'year'])]\n",
    "            if time_cols:\n",
    "                time_col = time_cols[0]\n",
    "                print(f\"Time range: {df[time_col].min()} to {df[time_col].max()}\")\n",
    "            \n",
    "            # Show precipitation statistics\n",
    "            precip_cols = [col for col in df.columns if any(p in col.lower() for p in ['rain', 'precip', 'precipitation'])]\n",
    "            if precip_cols:\n",
    "                print(f\"\\nPrecipitation statistics for {precip_cols[0]}:\")\n",
    "                print(df[precip_cols[0]].describe())\n",
    "            \n",
    "            # Load the CSV back into pandas to verify\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"VERIFICATION - Loading CSV back into pandas:\")\n",
    "            print(\"=\"*60)\n",
    "            df_loaded = pd.read_csv(output_csv)\n",
    "            print(f\"Loaded DataFrame shape: {df_loaded.shape}\")\n",
    "            print(\"Sample of loaded data:\")\n",
    "            print(df_loaded.head(3))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process NetCDF file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c4e0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca1f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not working for datatype mismatch issues\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def netcdf_to_csv_london(nc_file_path, output_csv_path=None):\n",
    "    \"\"\"\n",
    "    Read NetCDF file, extract precipitation data for London coordinates, and save as CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    nc_file_path (str): Path to the NetCDF file\n",
    "    output_csv_path (str): Path for output CSV file (optional)\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with the extracted data\n",
    "    \"\"\"\n",
    "    \n",
    "    # London coordinates\n",
    "    london_lat = 51.5074\n",
    "    london_lon = -0.1278\n",
    "    \n",
    "    print(f\"Reading NetCDF file: {nc_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Open NetCDF file with xarray\n",
    "        ds = xr.open_dataset(nc_file_path)\n",
    "        \n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        print(f\"Dataset dimensions: {dict(ds.dims)}\")\n",
    "        print(f\"Dataset variables: {list(ds.data_vars)}\")\n",
    "        print(f\"Dataset coordinates: {list(ds.coords)}\")\n",
    "        \n",
    "        # Display dataset info\n",
    "        print(\"\\nDataset overview:\")\n",
    "        print(ds)\n",
    "        \n",
    "        # Check coordinate names (common variations)\n",
    "        lat_names = ['lat', 'latitude', 'y', 'projection_y_coordinate']\n",
    "        lon_names = ['lon', 'longitude', 'x', 'projection_x_coordinate']\n",
    "        time_names = ['time', 'time_bnds', 'yyyymm']\n",
    "        \n",
    "        # Find actual coordinate names in the dataset\n",
    "        lat_coord = None\n",
    "        lon_coord = None\n",
    "        time_coord = None\n",
    "        \n",
    "        for name in lat_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                lat_coord = name\n",
    "                break\n",
    "                \n",
    "        for name in lon_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                lon_coord = name\n",
    "                break\n",
    "                \n",
    "        for name in time_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                time_coord = name\n",
    "                break\n",
    "        \n",
    "        print(f\"\\nIdentified coordinates:\")\n",
    "        print(f\"Latitude coordinate: {lat_coord}\")\n",
    "        print(f\"Longitude coordinate: {lon_coord}\")\n",
    "        print(f\"Time coordinate: {time_coord}\")\n",
    "        \n",
    "        if lat_coord is None or lon_coord is None:\n",
    "            print(\"Warning: Could not identify latitude/longitude coordinates\")\n",
    "            print(\"Available coordinates:\", list(ds.coords))\n",
    "            return None\n",
    "        \n",
    "        # Check coordinate ranges\n",
    "        if lat_coord in ds.coords:\n",
    "            lat_min = ds.coords[lat_coord].min().values\n",
    "            lat_max = ds.coords[lat_coord].max().values\n",
    "            print(f\"Latitude range: {lat_min} to {lat_max}\")\n",
    "        \n",
    "        if lon_coord in ds.coords:\n",
    "            lon_min = ds.coords[lon_coord].min().values\n",
    "            lon_max = ds.coords[lon_coord].max().values\n",
    "            print(f\"Longitude range: {lon_min} to {lon_max}\")\n",
    "        \n",
    "        # Handle projected coordinates (if using OSGB or similar)\n",
    "        if 'projection' in str(ds.coords) or lat_coord in ['y', 'projection_y_coordinate']:\n",
    "            print(\"\\nDetected projected coordinates (likely OSGB)\")\n",
    "            print(\"You may need to convert London lat/lon to the projection coordinates\")\n",
    "            print(\"For now, finding nearest grid point in projected space...\")\n",
    "        \n",
    "        # Find nearest grid point to London\n",
    "        if lat_coord in ds.coords and lon_coord in ds.coords:\n",
    "            # Handle 2D coordinate arrays (common in projected datasets)\n",
    "            lat_data = ds.coords[lat_coord]\n",
    "            lon_data = ds.coords[lon_coord]\n",
    "            \n",
    "            if lat_data.ndim == 2 and lon_data.ndim == 2:\n",
    "                # 2D coordinate arrays - find nearest point in 2D space\n",
    "                print(\"Using 2D coordinate arrays to find nearest point\")\n",
    "                \n",
    "                # Calculate distance from London to all grid points\n",
    "                lat_diff = lat_data - london_lat\n",
    "                lon_diff = lon_data - london_lon\n",
    "                distances = np.sqrt(lat_diff**2 + lon_diff**2)\n",
    "                \n",
    "                # Find indices of minimum distance\n",
    "                min_idx = np.unravel_index(distances.argmin(), distances.shape)\n",
    "                y_idx, x_idx = min_idx\n",
    "                \n",
    "                actual_lat = lat_data[y_idx, x_idx].values\n",
    "                actual_lon = lon_data[y_idx, x_idx].values\n",
    "                \n",
    "                print(f\"\\nTarget coordinates: ({london_lat}, {london_lon})\")\n",
    "                print(f\"Nearest grid point: ({actual_lat}, {actual_lon})\")\n",
    "                print(f\"Grid indices: y={y_idx}, x={x_idx}\")\n",
    "                \n",
    "                # Use the projection coordinate indices for selection\n",
    "                lat_idx = y_idx  # This corresponds to projection_y_coordinate index\n",
    "                lon_idx = x_idx  # This corresponds to projection_x_coordinate index\n",
    "                \n",
    "            else:\n",
    "                # 1D coordinate arrays\n",
    "                lat_idx = np.abs(lat_data - london_lat).argmin()\n",
    "                lon_idx = np.abs(lon_data - london_lon).argmin()\n",
    "                \n",
    "                actual_lat = lat_data[lat_idx].values\n",
    "                actual_lon = lon_data[lon_idx].values\n",
    "                \n",
    "                print(f\"\\nTarget coordinates: ({london_lat}, {london_lon})\")\n",
    "                print(f\"Nearest grid point: ({actual_lat}, {actual_lon})\")\n",
    "        else:\n",
    "            print(\"Could not find coordinate values for nearest point calculation\")\n",
    "            return None\n",
    "        \n",
    "        # Extract data for London coordinates\n",
    "        # Use the projection coordinate names for selection\n",
    "        proj_y_coord = 'projection_y_coordinate'\n",
    "        proj_x_coord = 'projection_x_coordinate'\n",
    "        \n",
    "        selection_dict = {proj_y_coord: lat_idx, proj_x_coord: lon_idx}\n",
    "        london_data = ds.isel(selection_dict)\n",
    "        \n",
    "        print(f\"\\nExtracted data variables: {list(london_data.data_vars)}\")\n",
    "        \n",
    "        # Convert to pandas DataFrame\n",
    "        df_list = []\n",
    "        \n",
    "        for var_name in london_data.data_vars:\n",
    "            var_data = london_data[var_name]\n",
    "            print(f\"\\nProcessing variable: {var_name}\")\n",
    "            print(f\"Variable dimensions: {var_data.dims}\")\n",
    "            print(f\"Variable shape: {var_data.shape}\")\n",
    "            \n",
    "            # Get variable attributes\n",
    "            if hasattr(var_data, 'attrs'):\n",
    "                print(f\"Variable attributes: {var_data.attrs}\")\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            if time_coord and time_coord in var_data.dims:\n",
    "                # Time series data\n",
    "                df_var = var_data.to_dataframe().reset_index()\n",
    "                \n",
    "                # Rename the variable column to be more descriptive\n",
    "                if var_name in df_var.columns:\n",
    "                    # Keep original variable name but add units if available\n",
    "                    units = var_data.attrs.get('units', '')\n",
    "                    long_name = var_data.attrs.get('long_name', var_name)\n",
    "                    \n",
    "                    col_name = var_name\n",
    "                    if units:\n",
    "                        col_name += f\"_{units}\".replace(\" \", \"_\")\n",
    "                    \n",
    "                    df_var = df_var.rename(columns={var_name: col_name})\n",
    "                    \n",
    "                    # Add metadata as separate columns\n",
    "                    df_var[f'{var_name}_long_name'] = long_name\n",
    "                    df_var[f'{var_name}_units'] = units\n",
    "                \n",
    "                df_list.append(df_var)\n",
    "            else:\n",
    "                # Single value or no time dimension\n",
    "                df_var = pd.DataFrame({\n",
    "                    var_name: [var_data.values],\n",
    "                    'latitude': [actual_lat],\n",
    "                    'longitude': [actual_lon]\n",
    "                })\n",
    "                \n",
    "                # Add metadata\n",
    "                if hasattr(var_data, 'attrs'):\n",
    "                    for attr_name, attr_value in var_data.attrs.items():\n",
    "                        df_var[f'{var_name}_{attr_name}'] = str(attr_value)\n",
    "                \n",
    "                df_list.append(df_var)\n",
    "        \n",
    "        # Combine all variables into one DataFrame\n",
    "        if len(df_list) > 1:\n",
    "            # Find common columns for merging\n",
    "            df = df_list[0]\n",
    "            for df_var in df_list[1:]:\n",
    "                common_cols = set(df.columns) & set(df_var.columns)\n",
    "                if common_cols and len(common_cols) > 1:  # More than just the variable column\n",
    "                    df = pd.merge(df, df_var, on=list(common_cols), how='outer')\n",
    "                else:\n",
    "                    # Concatenate side by side if same length\n",
    "                    if len(df) == len(df_var):\n",
    "                        df = pd.concat([df, df_var], axis=1)\n",
    "                    else:\n",
    "                        print(f\"Warning: Cannot merge {df_var.columns} - different lengths\")\n",
    "        else:\n",
    "            df = df_list[0] if df_list else pd.DataFrame()\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"Warning: No data extracted\")\n",
    "            return None\n",
    "        \n",
    "        # Add location metadata\n",
    "        df['actual_latitude'] = actual_lat\n",
    "        df['actual_longitude'] = actual_lon\n",
    "        df['target_latitude'] = london_lat\n",
    "        df['target_longitude'] = london_lon\n",
    "        df['coordinate_system'] = f\"{lat_coord}, {lon_coord}\"\n",
    "        \n",
    "        # Sort by time if time column exists\n",
    "        time_cols = [col for col in df.columns if any(t in col.lower() for t in ['time', 'date', 'year'])]\n",
    "        if time_cols:\n",
    "            df = df.sort_values(time_cols[0])\n",
    "            print(f\"Sorted by time column: {time_cols[0]}\")\n",
    "        \n",
    "        print(f\"\\nFinal DataFrame shape: {df.shape}\")\n",
    "        print(f\"DataFrame columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Show data types\n",
    "        print(f\"\\nData types:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"  {col}: {df[col].dtype}\")\n",
    "        \n",
    "        print(f\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        if len(df) > 5:\n",
    "            print(f\"\\nLast few rows:\")\n",
    "            print(df.tail())\n",
    "        \n",
    "        # Save to CSV\n",
    "        if output_csv_path is None:\n",
    "            output_csv_path = Path(nc_file_path).stem + \"_london_precipitation.csv\"\n",
    "        \n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData saved to: {output_csv_path}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing NetCDF file: {e}\")\n",
    "        print(\"Make sure you have the required packages installed:\")\n",
    "        print(\"pip install xarray netcdf4 pandas numpy\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # Your specific file\n",
    "    nc_file = \"./data/ERA5/rainfall_hadukgrid_uk_1km_ann-30y_196101-199012.nc\"\n",
    "    output_csv = \"./output/london_rainfall_1961_1990_V2.csv\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not Path(nc_file).exists():\n",
    "        print(f\"NetCDF file not found: {nc_file}\")\n",
    "        print(\"Please make sure the file is in the current directory or update the path.\")\n",
    "        return\n",
    "    \n",
    "    # Process the file\n",
    "    try:\n",
    "        df = netcdf_to_csv_london(nc_file, output_csv)\n",
    "        \n",
    "        if df is not None:\n",
    "            # Display summary statistics\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"SUMMARY\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"Total records: {len(df)}\")\n",
    "            \n",
    "            # Show time range if available\n",
    "            time_cols = [col for col in df.columns if any(t in col.lower() for t in ['time', 'date', 'year'])]\n",
    "            if time_cols:\n",
    "                time_col = time_cols[0]\n",
    "                print(f\"Time range: {df[time_col].min()} to {df[time_col].max()}\")\n",
    "            \n",
    "            # Show precipitation statistics\n",
    "            precip_cols = [col for col in df.columns if any(p in col.lower() for p in ['rain', 'precip', 'precipitation'])]\n",
    "            if precip_cols:\n",
    "                print(f\"\\nPrecipitation statistics for {precip_cols[0]}:\")\n",
    "                print(df[precip_cols[0]].describe())\n",
    "            \n",
    "            # Load the CSV back into pandas to verify\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"VERIFICATION - Loading CSV back into pandas:\")\n",
    "            print(\"=\"*60)\n",
    "            df_loaded = pd.read_csv(output_csv)\n",
    "            print(f\"Loaded DataFrame shape: {df_loaded.shape}\")\n",
    "            print(\"Sample of loaded data:\")\n",
    "            print(df_loaded.head(3))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process NetCDF file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0546e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d259e374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def grib_to_csv_london(grib_file_path, output_csv_path=None):\n",
    "    \"\"\"\n",
    "    Read GRIB file, extract data for London coordinates, and save as CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    grib_file_path (str): Path to the GRIB file\n",
    "    output_csv_path (str): Path for output CSV file (optional)\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with the extracted data\n",
    "    \"\"\"\n",
    "    \n",
    "    # London coordinates\n",
    "    london_lat = 51.5074\n",
    "    london_lon = -0.1278  # Note: GRIB files often use 0-360 longitude\n",
    "    london_lon_360 = 359.8722  # London longitude in 0-360 format\n",
    "    \n",
    "    print(f\"Reading GRIB file: {grib_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Open GRIB file with xarray\n",
    "        ds = xr.open_dataset(grib_file_path, engine='cfgrib')\n",
    "        \n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        print(f\"Dataset dimensions: {dict(ds.dims)}\")\n",
    "        print(f\"Dataset variables: {list(ds.data_vars)}\")\n",
    "        print(f\"Dataset coordinates: {list(ds.coords)}\")\n",
    "        \n",
    "        # Check longitude format (0-360 vs -180-180)\n",
    "        lon_min = ds.longitude.min().values\n",
    "        lon_max = ds.longitude.max().values\n",
    "        print(f\"Longitude range: {lon_min} to {lon_max}\")\n",
    "        \n",
    "        # Use appropriate London longitude based on the dataset's longitude format\n",
    "        if lon_max > 180:\n",
    "            target_lon = london_lon_360\n",
    "            print(f\"Using 0-360 longitude format: {target_lon}\")\n",
    "        else:\n",
    "            target_lon = london_lon\n",
    "            print(f\"Using -180-180 longitude format: {target_lon}\")\n",
    "        \n",
    "        # Find nearest grid point to London\n",
    "        lat_idx = np.abs(ds.latitude - london_lat).argmin()\n",
    "        lon_idx = np.abs(ds.longitude - target_lon).argmin()\n",
    "        \n",
    "        actual_lat = ds.latitude[lat_idx].values\n",
    "        actual_lon = ds.longitude[lon_idx].values\n",
    "        \n",
    "        print(f\"Target coordinates: ({london_lat}, {target_lon})\")\n",
    "        print(f\"Nearest grid point: ({actual_lat}, {actual_lon})\")\n",
    "        \n",
    "        # Extract data for London coordinates\n",
    "        london_data = ds.isel(latitude=lat_idx, longitude=lon_idx)\n",
    "        \n",
    "        # Convert to pandas DataFrame\n",
    "        df_list = []\n",
    "        \n",
    "        for var_name in london_data.data_vars:\n",
    "            var_data = london_data[var_name]\n",
    "            \n",
    "            # Handle different time dimensions\n",
    "            if 'time' in var_data.dims:\n",
    "                # Create DataFrame with time index\n",
    "                df_var = var_data.to_dataframe().reset_index()\n",
    "                df_var = df_var.rename(columns={var_name: var_name})\n",
    "                df_list.append(df_var)\n",
    "            elif 'valid_time' in var_data.dims:\n",
    "                # Handle valid_time dimension\n",
    "                df_var = var_data.to_dataframe().reset_index()\n",
    "                df_var = df_var.rename(columns={var_name: var_name})\n",
    "                df_list.append(df_var)\n",
    "            else:\n",
    "                # Single value variables\n",
    "                df_var = pd.DataFrame({\n",
    "                    var_name: [var_data.values],\n",
    "                    'latitude': [actual_lat],\n",
    "                    'longitude': [actual_lon]\n",
    "                })\n",
    "                df_list.append(df_var)\n",
    "        \n",
    "        # Combine all variables into one DataFrame\n",
    "        if len(df_list) > 1:\n",
    "            # Merge on common columns (time, latitude, longitude)\n",
    "            df = df_list[0]\n",
    "            for df_var in df_list[1:]:\n",
    "                common_cols = set(df.columns) & set(df_var.columns)\n",
    "                if common_cols:\n",
    "                    df = pd.merge(df, df_var, on=list(common_cols), how='outer')\n",
    "                else:\n",
    "                    # If no common columns, concatenate side by side\n",
    "                    df = pd.concat([df, df_var], axis=1)\n",
    "        else:\n",
    "            df = df_list[0]\n",
    "        \n",
    "        # Add metadata columns\n",
    "        df['actual_latitude'] = actual_lat\n",
    "        df['actual_longitude'] = actual_lon\n",
    "        df['target_latitude'] = london_lat\n",
    "        df['target_longitude'] = target_lon\n",
    "        \n",
    "        # Sort by time if time column exists\n",
    "        time_cols = [col for col in df.columns if 'time' in col.lower()]\n",
    "        if time_cols:\n",
    "            df = df.sort_values(time_cols[0])\n",
    "        \n",
    "        print(f\"DataFrame shape: {df.shape}\")\n",
    "        print(f\"DataFrame columns: {list(df.columns)}\")\n",
    "        print(\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Save to CSV\n",
    "        if output_csv_path is None:\n",
    "            output_csv_path = Path(grib_file_path).stem + \"_london_data.csv\"\n",
    "        \n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData saved to: {output_csv_path}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing GRIB file: {e}\")\n",
    "        print(\"Make sure you have the required packages installed:\")\n",
    "        print(\"pip install xarray cfgrib pandas numpy\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "    grib_file = \"./data/ERA5/ERA5_hourly_data_on_single_levels_from_1940_to_present.grib\"  # Replace with your GRIB file path\n",
    "    output_csv = \"./output/london_weather_data.csv\"  # Replace with desired output path\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not Path(grib_file).exists():\n",
    "        print(f\"GRIB file not found: {grib_file}\")\n",
    "        print(\"Please update the 'grib_file' variable with the correct path to your GRIB file.\")\n",
    "        return\n",
    "    \n",
    "    # Process the file\n",
    "    try:\n",
    "        df = grib_to_csv_london(grib_file, output_csv)\n",
    "        \n",
    "        # Display summary statistics\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Total records: {len(df)}\")\n",
    "        \n",
    "        # Show time range if available\n",
    "        time_cols = [col for col in df.columns if 'time' in col.lower()]\n",
    "        if time_cols:\n",
    "            time_col = time_cols[0]\n",
    "            print(f\"Time range: {df[time_col].min()} to {df[time_col].max()}\")\n",
    "        \n",
    "        print(f\"Variables extracted: {[col for col in df.columns if col not in ['latitude', 'longitude', 'actual_latitude', 'actual_longitude', 'target_latitude', 'target_longitude'] + time_cols]}\")\n",
    "        \n",
    "        # Load the CSV back into pandas to verify\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"VERIFICATION - Loading CSV back into pandas:\")\n",
    "        print(\"=\"*50)\n",
    "        df_loaded = pd.read_csv(output_csv)\n",
    "        print(f\"Loaded DataFrame shape: {df_loaded.shape}\")\n",
    "        print(\"DataFrame info:\")\n",
    "        print(df_loaded.info())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process GRIB file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a783e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(\"./output/london_weather_data.csv\")\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad349eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['year'] = df_data['datetime'].dt.year\n",
    "df_data['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe86e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_data\n",
    " .groupby('year', observed=True)['tp']\n",
    " .sum()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f3a762",
   "metadata": {},
   "source": [
    "### Functions Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b672df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1: first working version\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def grib_to_csv_city(grib_file_path, \n",
    "                     city_lat, \n",
    "                     city_lon,\n",
    "                     output_csv_path='./output/',\n",
    "                     city_name = 'london', \n",
    "                     ):\n",
    "    \"\"\"\n",
    "    Read GRIB file, extract data for London coordinates, and save as CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    grib_file_path (str): Path to the GRIB file\n",
    "    output_csv_path (str): Path for output CSV file (optional)\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with the extracted data\n",
    "    \"\"\"\n",
    "    \n",
    "    # # London coordinates\n",
    "    # london_lat = 51.5074\n",
    "    # london_lon = -0.1278  # Note: GRIB files often use 0-360 longitude\n",
    "    # london_lon_360 = 359.8722  # London longitude in 0-360 format\n",
    "    \n",
    "    print(f\"Reading GRIB file: {grib_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Open GRIB file with xarray\n",
    "        ds = xr.open_dataset(grib_file_path, engine='cfgrib')\n",
    "        \n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        print(f\"Dataset dimensions: {dict(ds.dims)}\")\n",
    "        print(f\"Dataset variables: {list(ds.data_vars)}\")\n",
    "        print(f\"Dataset coordinates: {list(ds.coords)}\")\n",
    "        \n",
    "        # Check longitude format (0-360 vs -180-180)\n",
    "        city_min = ds.longitude.min().values\n",
    "        city_max = ds.longitude.max().values\n",
    "        print(f\"Longitude range: {city_min} to {city_max}\")\n",
    "        \n",
    "        # Use appropriate London longitude based on the dataset's longitude format\n",
    "        # if city_max > 180:\n",
    "        #     target_lon = london_lon_360\n",
    "        #     print(f\"Using 0-360 longitude format: {target_lon}\")\n",
    "        # else:\n",
    "        target_city = city_lon\n",
    "        print(f\"Using -180-180 longitude format: {target_city}\")\n",
    "        \n",
    "        # Find nearest grid point to London\n",
    "        lat_idx = np.abs(ds.latitude - city_lat).argmin()\n",
    "        lon_idx = np.abs(ds.longitude - target_city).argmin()\n",
    "        \n",
    "        actual_lat = ds.latitude[lat_idx].values\n",
    "        actual_lon = ds.longitude[lon_idx].values\n",
    "        \n",
    "        print(f\"Target coordinates: ({city_lat}, {target_city})\")\n",
    "        print(f\"Nearest grid point: ({actual_lat}, {actual_lon})\")\n",
    "        \n",
    "        # Extract data for London coordinates\n",
    "        city_data = ds.isel(latitude=lat_idx, longitude=lon_idx)\n",
    "        \n",
    "        # Convert to pandas DataFrame\n",
    "        df_list = []\n",
    "        \n",
    "        for var_name in city_data.data_vars:\n",
    "            var_data = city_data[var_name]\n",
    "            \n",
    "            # Handle different time dimensions\n",
    "            if 'time' in var_data.dims:\n",
    "                # Create DataFrame with time index\n",
    "                df_var = var_data.to_dataframe().reset_index()\n",
    "                df_var = df_var.rename(columns={var_name: var_name})\n",
    "                df_list.append(df_var)\n",
    "            elif 'valid_time' in var_data.dims:\n",
    "                # Handle valid_time dimension\n",
    "                df_var = var_data.to_dataframe().reset_index()\n",
    "                df_var = df_var.rename(columns={var_name: var_name})\n",
    "                df_list.append(df_var)\n",
    "            else:\n",
    "                # Single value variables\n",
    "                df_var = pd.DataFrame({\n",
    "                    var_name: [var_data.values],\n",
    "                    'latitude': [actual_lat],\n",
    "                    'longitude': [actual_lon]\n",
    "                })\n",
    "                df_list.append(df_var)\n",
    "        \n",
    "        # Combine all variables into one DataFrame\n",
    "        if len(df_list) > 1:\n",
    "            # Merge on common columns (time, latitude, longitude)\n",
    "            df = df_list[0]\n",
    "            for df_var in df_list[1:]:\n",
    "                common_cols = set(df.columns) & set(df_var.columns)\n",
    "                if common_cols:\n",
    "                    df = pd.merge(df, df_var, on=list(common_cols), how='outer')\n",
    "                else:\n",
    "                    # If no common columns, concatenate side by side\n",
    "                    df = pd.concat([df, df_var], axis=1)\n",
    "        else:\n",
    "            df = df_list[0]\n",
    "        \n",
    "        # Add metadata columns\n",
    "        df['actual_latitude'] = actual_lat\n",
    "        df['actual_longitude'] = actual_lon\n",
    "        df['target_latitude'] = city_lat\n",
    "        df['target_longitude'] = target_city\n",
    "        \n",
    "        # Sort by time if time column exists\n",
    "        time_cols = [col for col in df.columns if 'time' in col.lower()]\n",
    "        if time_cols:\n",
    "            time_col = time_cols[0]\n",
    "            df = df.sort_values(time_cols[0])\n",
    "            print(f\"****\\n - inside csv Time range: {df[time_col].min()} to {df[time_col].max()}\")\n",
    "            min_string = df[time_col].min().strftime('%Y%m%d')\n",
    "            max_string = df[time_col].max().strftime('%Y%m%d')\n",
    "            print(f\" - min time as string: {min_string}\")\n",
    "            print(f\" - max time as string: {max_string}\")\n",
    "            output_csv_name = list(ds.data_vars)[0] + '_' + min_string + '_' + max_string\n",
    "            print(output_csv_name)\n",
    "        \n",
    "        print(f\"DataFrame shape: {df.shape}\")\n",
    "        print(f\"DataFrame columns: {list(df.columns)}\")\n",
    "        print(\"grib to csv\")\n",
    "        print(f\"ds.data_vars dtype: {type(ds.data_vars)}, df.data_vars name: {list(ds.data_vars)[0]}\")\n",
    "        # print(\"\\nFirst few rows:\")\n",
    "        # print(df.head())\n",
    "        \n",
    "        # Save to CSV\n",
    "        if output_csv_path is None:\n",
    "            output_csv_path = Path(grib_file_path).stem + \"_city_data.csv\"\n",
    "        \n",
    "        output_csv_path = output_csv_path + city_name + '_' + output_csv_name + '.csv'\n",
    "        print(f\"Saving DataFrame to: {output_csv_path}\")\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData saved to: {output_csv_path}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing GRIB file: {e}\")\n",
    "        print(\"Make sure you have the required packages installed:\")\n",
    "        print(\"pip install xarray cfgrib pandas numpy\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e44cdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1\n",
    "\n",
    "def process_grib_file(grib_file_path,\n",
    "                      output_csv_path='./output/',\n",
    "                      city_name='london',\n",
    "                      city_lat=51.5074,  # Default to London latitude\n",
    "                      city_lon=-0.1278):  # Default to London longitude\n",
    "    \n",
    "    # Example usage\n",
    "    grib_file = grib_file_path  # Replace with your GRIB file path\n",
    "    output_csv_city = city_name  # Replace with desired output path\n",
    "    output_csv_path = output_csv_path  # Directory to save the output CSV\n",
    "\n",
    "    # Check if file exists\n",
    "    if not Path(grib_file).exists():\n",
    "        print(f\"GRIB file not found: {grib_file}\")\n",
    "        print(\"Please update the 'grib_file' variable with the correct path to your GRIB file.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        df = grib_to_csv_city(grib_file, \n",
    "                              city_lat=city_lat, \n",
    "                              city_lon=-city_lon, output_csv_path=output_csv_path, city_name=output_csv_city)\n",
    "        \n",
    "        # Display summary statistics\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Total records: {len(df)}\")\n",
    "        \n",
    "        # Show time range if available\n",
    "        time_cols = [col for col in df.columns if 'time' in col.lower()]\n",
    "        if time_cols:\n",
    "            time_col = time_cols[0]\n",
    "            print(f\"Time range: {df[time_col].min()} to {df[time_col].max()}\")\n",
    "        \n",
    "        print(f\"Variables extracted: {[col for col in df.columns if col not in ['latitude', 'longitude', 'actual_latitude', 'actual_longitude', 'target_latitude', 'target_longitude'] + time_cols]}\")\n",
    "        \n",
    "        # # Load the CSV back into pandas to verify\n",
    "        # print(\"\\n\" + \"=\"*50)\n",
    "        # print(\"VERIFICATION - Loading CSV back into pandas:\")\n",
    "        # print(\"=\"*50)\n",
    "\n",
    "        # df_loaded = pd.read_csv(output_csv)\n",
    "        # print(f\"Loaded DataFrame shape: {df_loaded.shape}\")\n",
    "        # print(\"DataFrame info:\")\n",
    "        # print(df_loaded.info())\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process GRIB file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9456cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def netcdf_to_csv_london(nc_file_path, output_csv_path=None):\n",
    "    \"\"\"\n",
    "    Read NetCDF file, extract precipitation data for London coordinates, and save as CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    nc_file_path (str): Path to the NetCDF file\n",
    "    output_csv_path (str): Path for output CSV file (optional)\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with the extracted data\n",
    "    \"\"\"\n",
    "    \n",
    "    # London coordinates\n",
    "    london_lat = 51.5074\n",
    "    london_lon = -0.1278\n",
    "    \n",
    "    print(f\"Reading NetCDF file: {nc_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Open NetCDF file with xarray\n",
    "        ds = xr.open_dataset(nc_file_path)\n",
    "        \n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        print(f\"Dataset dimensions: {dict(ds.dims)}\")\n",
    "        print(f\"Dataset variables: {list(ds.data_vars)}\")\n",
    "        print(f\"Dataset coordinates: {list(ds.coords)}\")\n",
    "        \n",
    "        # Display dataset info\n",
    "        print(\"\\nDataset overview:\")\n",
    "        print(ds)\n",
    "        \n",
    "        # Check coordinate names (common variations)\n",
    "        lat_names = ['lat', 'latitude', 'y', 'projection_y_coordinate']\n",
    "        lon_names = ['lon', 'longitude', 'x', 'projection_x_coordinate']\n",
    "        time_names = ['time', 'time_bnds', 'yyyymm']\n",
    "        \n",
    "        # Find actual coordinate names in the dataset\n",
    "        lat_coord = None\n",
    "        lon_coord = None\n",
    "        time_coord = None\n",
    "        \n",
    "        for name in lat_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                lat_coord = name\n",
    "                break\n",
    "                \n",
    "        for name in lon_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                lon_coord = name\n",
    "                break\n",
    "                \n",
    "        for name in time_names:\n",
    "            if name in ds.coords or name in ds.dims:\n",
    "                time_coord = name\n",
    "                break\n",
    "        \n",
    "        print(f\"\\nIdentified coordinates:\")\n",
    "        print(f\"Latitude coordinate: {lat_coord}\")\n",
    "        print(f\"Longitude coordinate: {lon_coord}\")\n",
    "        print(f\"Time coordinate: {time_coord}\")\n",
    "        \n",
    "        if lat_coord is None or lon_coord is None:\n",
    "            print(\"Warning: Could not identify latitude/longitude coordinates\")\n",
    "            print(\"Available coordinates:\", list(ds.coords))\n",
    "            return None\n",
    "        \n",
    "        # Check coordinate ranges\n",
    "        if lat_coord in ds.coords:\n",
    "            lat_min = ds.coords[lat_coord].min().values\n",
    "            lat_max = ds.coords[lat_coord].max().values\n",
    "            print(f\"Latitude range: {lat_min} to {lat_max}\")\n",
    "        \n",
    "        if lon_coord in ds.coords:\n",
    "            lon_min = ds.coords[lon_coord].min().values\n",
    "            lon_max = ds.coords[lon_coord].max().values\n",
    "            print(f\"Longitude range: {lon_min} to {lon_max}\")\n",
    "        \n",
    "        # Handle projected coordinates (if using OSGB or similar)\n",
    "        if 'projection' in str(ds.coords) or lat_coord in ['y', 'projection_y_coordinate']:\n",
    "            print(\"\\nDetected projected coordinates (likely OSGB)\")\n",
    "            print(\"You may need to convert London lat/lon to the projection coordinates\")\n",
    "            print(\"For now, finding nearest grid point in projected space...\")\n",
    "        \n",
    "        # Find nearest grid point to London\n",
    "        if lat_coord in ds.coords and lon_coord in ds.coords:\n",
    "            lat_idx = np.abs(ds.coords[lat_coord] - london_lat).argmin()\n",
    "            lon_idx = np.abs(ds.coords[lon_coord] - london_lon).argmin()\n",
    "            \n",
    "            actual_lat = ds.coords[lat_coord][lat_idx].values\n",
    "            actual_lon = ds.coords[lon_coord][lon_idx].values\n",
    "            \n",
    "            print(f\"\\nTarget coordinates: ({london_lat}, {london_lon})\")\n",
    "            print(f\"Nearest grid point: ({actual_lat}, {actual_lon})\")\n",
    "        else:\n",
    "            print(\"Could not find coordinate values for nearest point calculation\")\n",
    "            return None\n",
    "        \n",
    "        # Extract data for London coordinates\n",
    "        selection_dict = {lat_coord: lat_idx, lon_coord: lon_idx}\n",
    "        london_data = ds.isel(selection_dict)\n",
    "        \n",
    "        print(f\"\\nExtracted data variables: {list(london_data.data_vars)}\")\n",
    "        \n",
    "        # Convert to pandas DataFrame\n",
    "        df_list = []\n",
    "        \n",
    "        for var_name in london_data.data_vars:\n",
    "            var_data = london_data[var_name]\n",
    "            print(f\"\\nProcessing variable: {var_name}\")\n",
    "            print(f\"Variable dimensions: {var_data.dims}\")\n",
    "            print(f\"Variable shape: {var_data.shape}\")\n",
    "            \n",
    "            # Get variable attributes\n",
    "            if hasattr(var_data, 'attrs'):\n",
    "                print(f\"Variable attributes: {var_data.attrs}\")\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            if time_coord and time_coord in var_data.dims:\n",
    "                # Time series data\n",
    "                df_var = var_data.to_dataframe().reset_index()\n",
    "                \n",
    "                # Rename the variable column to be more descriptive\n",
    "                if var_name in df_var.columns:\n",
    "                    # Keep original variable name but add units if available\n",
    "                    units = var_data.attrs.get('units', '')\n",
    "                    long_name = var_data.attrs.get('long_name', var_name)\n",
    "                    \n",
    "                    col_name = var_name\n",
    "                    if units:\n",
    "                        col_name += f\"_{units}\".replace(\" \", \"_\")\n",
    "                    \n",
    "                    df_var = df_var.rename(columns={var_name: col_name})\n",
    "                    \n",
    "                    # Add metadata as separate columns\n",
    "                    df_var[f'{var_name}_long_name'] = long_name\n",
    "                    df_var[f'{var_name}_units'] = units\n",
    "                \n",
    "                df_list.append(df_var)\n",
    "            else:\n",
    "                # Single value or no time dimension\n",
    "                df_var = pd.DataFrame({\n",
    "                    var_name: [var_data.values],\n",
    "                    'latitude': [actual_lat],\n",
    "                    'longitude': [actual_lon]\n",
    "                })\n",
    "                \n",
    "                # Add metadata\n",
    "                if hasattr(var_data, 'attrs'):\n",
    "                    for attr_name, attr_value in var_data.attrs.items():\n",
    "                        df_var[f'{var_name}_{attr_name}'] = str(attr_value)\n",
    "                \n",
    "                df_list.append(df_var)\n",
    "        \n",
    "        # Combine all variables into one DataFrame\n",
    "        if len(df_list) > 1:\n",
    "            # Find common columns for merging\n",
    "            df = df_list[0]\n",
    "            for df_var in df_list[1:]:\n",
    "                common_cols = set(df.columns) & set(df_var.columns)\n",
    "                if common_cols and len(common_cols) > 1:  # More than just the variable column\n",
    "                    df = pd.merge(df, df_var, on=list(common_cols), how='outer')\n",
    "                else:\n",
    "                    # Concatenate side by side if same length\n",
    "                    if len(df) == len(df_var):\n",
    "                        df = pd.concat([df, df_var], axis=1)\n",
    "                    else:\n",
    "                        print(f\"Warning: Cannot merge {df_var.columns} - different lengths\")\n",
    "        else:\n",
    "            df = df_list[0] if df_list else pd.DataFrame()\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"Warning: No data extracted\")\n",
    "            return None\n",
    "        \n",
    "        # Add location metadata\n",
    "        df['actual_latitude'] = actual_lat\n",
    "        df['actual_longitude'] = actual_lon\n",
    "        df['target_latitude'] = london_lat\n",
    "        df['target_longitude'] = london_lon\n",
    "        df['coordinate_system'] = f\"{lat_coord}, {lon_coord}\"\n",
    "        \n",
    "        # Sort by time if time column exists\n",
    "        time_cols = [col for col in df.columns if any(t in col.lower() for t in ['time', 'date', 'year'])]\n",
    "        if time_cols:\n",
    "            df = df.sort_values(time_cols[0])\n",
    "            print(f\"Sorted by time column: {time_cols[0]}\")\n",
    "        \n",
    "        print(f\"\\nFinal DataFrame shape: {df.shape}\")\n",
    "        print(f\"DataFrame columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Show data types\n",
    "        print(f\"\\nData types:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"  {col}: {df[col].dtype}\")\n",
    "        \n",
    "        print(f\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        if len(df) > 5:\n",
    "            print(f\"\\nLast few rows:\")\n",
    "            print(df.tail())\n",
    "        \n",
    "        # Save to CSV\n",
    "        if output_csv_path is None:\n",
    "            output_csv_path = Path(nc_file_path).stem + \"_london_precipitation.csv\"\n",
    "        \n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nData saved to: {output_csv_path}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing NetCDF file: {e}\")\n",
    "        print(\"Make sure you have the required packages installed:\")\n",
    "        print(\"pip install xarray netcdf4 pandas numpy\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # Your specific file\n",
    "    nc_file = \"./data/ERA5/rainfall_hadukgrid_uk_1km_ann-30y_196101-199012.nc\"\n",
    "    output_csv = \"london_rainfall_1961_1990.csv\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not Path(nc_file).exists():\n",
    "        print(f\"NetCDF file not found: {nc_file}\")\n",
    "        print(\"Please make sure the file is in the current directory or update the path.\")\n",
    "        return\n",
    "    \n",
    "    # Process the file\n",
    "    try:\n",
    "        df = netcdf_to_csv_london(nc_file, output_csv)\n",
    "        \n",
    "        if df is not None:\n",
    "            # Display summary statistics\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"SUMMARY\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"Total records: {len(df)}\")\n",
    "            \n",
    "            # Show time range if available\n",
    "            time_cols = [col for col in df.columns if any(t in col.lower() for t in ['time', 'date', 'year'])]\n",
    "            if time_cols:\n",
    "                time_col = time_cols[0]\n",
    "                print(f\"Time range: {df[time_col].min()} to {df[time_col].max()}\")\n",
    "            \n",
    "            # Show precipitation statistics\n",
    "            precip_cols = [col for col in df.columns if any(p in col.lower() for p in ['rain', 'precip', 'precipitation'])]\n",
    "            if precip_cols:\n",
    "                print(f\"\\nPrecipitation statistics for {precip_cols[0]}:\")\n",
    "                print(df[precip_cols[0]].describe())\n",
    "            \n",
    "            # Load the CSV back into pandas to verify\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"VERIFICATION - Loading CSV back into pandas:\")\n",
    "            print(\"=\"*60)\n",
    "            df_loaded = pd.read_csv(output_csv)\n",
    "            print(f\"Loaded DataFrame shape: {df_loaded.shape}\")\n",
    "            print(\"Sample of loaded data:\")\n",
    "            print(df_loaded.head(3))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process NetCDF file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d75c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f35646c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac3ca4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47bcf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygrib\n",
    "\n",
    "# Open GRIB file\n",
    "grbs = pygrib.open('./data/era5_monthly_averaged_data.grib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f302de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grib_file = './data/era5_monthly_averaged_data.grib'\n",
    "# grbs2 = xr.open_dataset(grib_file, engine='cfgrib') #, backend_kwargs=backend_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e323972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xarray as xr\n",
    "\n",
    "# grib_file = './data/era5_monthly_averaged_data.grib'\n",
    "\n",
    "# # Try the first grid resolution\n",
    "# try:\n",
    "#     grbs2 = xr.open_dataset(grib_file, \n",
    "#                            engine='cfgrib',\n",
    "#                            backend_kwargs={'filter_by_keys': {'numberOfPoints': 826}})\n",
    "#     print(\"Successfully opened with 826 grid points\")\n",
    "#     print(f\"Variables: {list(grbs2.data_vars)}\")\n",
    "#     print(f\"Shape: {grbs2.dims}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Failed with 826 points: {e}\")\n",
    "\n",
    "# # Try the second grid resolution\n",
    "# try:\n",
    "#     grbs2_alt = xr.open_dataset(grib_file, \n",
    "#                                engine='cfgrib',\n",
    "#                                backend_kwargs={'filter_by_keys': {'numberOfPoints': 207}})\n",
    "#     print(\"Successfully opened with 207 grid points\")\n",
    "#     print(f\"Variables: {list(grbs2_alt.data_vars)}\")\n",
    "#     print(f\"Shape: {grbs2_alt.dims}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Failed with 207 points: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641296a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grib_file = './data/era5_monthly_averaged_data.grib'\n",
    "\n",
    "# # Define all combinations to try\n",
    "# filter_combinations = [\n",
    "#     {'stepType': 'avgad', 'numberOfPoints': 826},\n",
    "#     {'stepType': 'avgas', 'numberOfPoints': 826},\n",
    "#     {'stepType': 'avgad', 'numberOfPoints': 207},\n",
    "#     {'stepType': 'avgas', 'numberOfPoints': 207}\n",
    "# ]\n",
    "\n",
    "# datasets = {}\n",
    "\n",
    "# for i, filters in enumerate(filter_combinations):\n",
    "#     try:\n",
    "#         ds = xr.open_dataset(grib_file, \n",
    "#                            engine='cfgrib',\n",
    "#                            backend_kwargs={'filter_by_keys': filters})\n",
    "        \n",
    "#         dataset_name = f\"{filters['stepType']}_{filters['numberOfPoints']}pts\"\n",
    "#         datasets[dataset_name] = ds\n",
    "        \n",
    "#         print(f\"\\n✅ Successfully opened: {dataset_name}\")\n",
    "#         print(f\"   Variables: {list(ds.data_vars)}\")\n",
    "#         print(f\"   Dimensions: {ds.dims}\")\n",
    "#         print(f\"   Coordinates: {list(ds.coords)}\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Failed {filters}: {str(e)[:100]}...\")\n",
    "\n",
    "# print(f\"\\n📊 Total datasets loaded: {len(datasets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41b4e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xarray as xr\n",
    "# import pandas as pd\n",
    "\n",
    "# grib_file = './data/era5_monthly_averaged_data.grib'\n",
    "\n",
    "# def process_all_era5_data(grib_file):\n",
    "#     \"\"\"Process all combinations and return organized data\"\"\"\n",
    "    \n",
    "#     filter_combinations = [\n",
    "#         {'stepType': 'avgad', 'numberOfPoints': 826},\n",
    "#         {'stepType': 'avgas', 'numberOfPoints': 826},\n",
    "#         {'stepType': 'avgad', 'numberOfPoints': 207},\n",
    "#         {'stepType': 'avgas', 'numberOfPoints': 207}\n",
    "#     ]\n",
    "    \n",
    "#     all_data = {}\n",
    "    \n",
    "#     for filters in filter_combinations:\n",
    "#         try:\n",
    "#             # Open dataset\n",
    "#             ds = xr.open_dataset(grib_file, \n",
    "#                                engine='cfgrib',\n",
    "#                                backend_kwargs={'filter_by_keys': filters})\n",
    "            \n",
    "#             # Create descriptive name\n",
    "#             step_type = filters['stepType']\n",
    "#             grid_size = filters['numberOfPoints']\n",
    "#             dataset_name = f\"{step_type}_{grid_size}pts\"\n",
    "            \n",
    "#             # Convert to DataFrame\n",
    "#             df = ds.to_dataframe().reset_index()\n",
    "            \n",
    "#             # Add metadata\n",
    "#             df['step_type'] = step_type\n",
    "#             df['grid_points'] = grid_size\n",
    "#             df['data_type'] = 'accumulated' if step_type == 'avgad' else 'instantaneous'\n",
    "            \n",
    "#             all_data[dataset_name] = {\n",
    "#                 'dataset': ds,\n",
    "#                 'dataframe': df,\n",
    "#                 'variables': list(ds.data_vars),\n",
    "#                 'shape': df.shape\n",
    "#             }\n",
    "            \n",
    "#             print(f\"✅ {dataset_name}:\")\n",
    "#             print(f\"   Shape: {df.shape}\")\n",
    "#             print(f\"   Variables: {list(ds.data_vars)}\")\n",
    "#             print(f\"   Sample data types: {df.dtypes.to_dict()}\")\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ Failed {filters}: {e}\")\n",
    "    \n",
    "#     return all_data\n",
    "\n",
    "# # Process all data\n",
    "# all_era5_data = process_all_era5_data(grib_file)\n",
    "\n",
    "# # Save each dataset to CSV\n",
    "# for name, data_info in all_era5_data.items():\n",
    "#     df = data_info['dataframe']\n",
    "#     filename = f'era5_{name}.csv'\n",
    "#     df.to_csv(filename, index=False)\n",
    "#     print(f\"💾 Saved {name} to {filename}\")\n",
    "\n",
    "# # Display summary\n",
    "# print(f\"\\n📋 Summary:\")\n",
    "# for name, data_info in all_era5_data.items():\n",
    "#     print(f\"{name}: {data_info['shape'][0]} rows, Variables: {data_info['variables']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfede07a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826962af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef92cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bce793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5fa512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f956cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open the NetCDF file\n",
    "# ds = xr.open_dataset(\"./data/london_test_2.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee382131",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ab6f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rain = ds['tp'].sel(latitude=51.5, longitude=0.0, method='nearest')\n",
    "rain.plot()\n",
    "plt.title('Daily Precipitation in London')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b3a417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select time series at closest grid point to London\n",
    "rain_series = ds['tp'].sel(latitude=51.5, longitude=0.0, method=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb8bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame\n",
    "df = rain_series.to_dataframe().reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9e02c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RR_mm'] = df['tp'] * 1000\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff307884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_manipulation import transform_data_datetime\n",
    "\n",
    "#london_data = transform_data_datetime(df=df.rename(columns={'valid_time' : 'DATE'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c770df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# london_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d072b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2efb993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74a5979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fab6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999ef925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb0f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific variable\n",
    "grb = grbs.select(name='Total precipitation')[0]\n",
    "data, lats, lons = grb.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134def1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd9ee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "lons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8d3162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read GRIB file\n",
    "ds = xr.open_dataset('./data/london_data.grib', engine='cfgrib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90964df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to open only GRIB2 messages\n",
    "try:\n",
    "    ds = xr.open_dataset('./data/london_data.grib', \n",
    "                        engine='cfgrib',\n",
    "                        backend_kwargs={'filter_by_keys': {'edition': 2}})\n",
    "    print(\"Successfully opened GRIB2 data\")\n",
    "except:\n",
    "    # If that fails, try GRIB1\n",
    "    ds = xr.open_dataset('./data/london_data.grib', \n",
    "                        engine='cfgrib',\n",
    "                        backend_kwargs={'filter_by_keys': {'edition': 1}})\n",
    "    print(\"Successfully opened GRIB1 data\")\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c9e3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset\n",
    "print(\"Variables:\", list(ds.data_vars))\n",
    "print(\"Coordinates:\", list(ds.coords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccf5c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['surface'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be9f753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
